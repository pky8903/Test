Script started on 2025-05-15 08:44:03+09:00 [TERM="screen" TTY="/dev/pts/0" COLUMNS="121" LINES="55"]
[?2004hkyp@kypserver:~/Workspace/00.test/Test/cuda_study/06.gemm_cutlass$ sudo ./run_ncu.sh
[?2004l[sudo] password for kyp: 
Script started, output log file is 'ncu_report.txt'.
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/06.gemm_cutlass# l[Kl
[?2004lbasic_gemm.cu  [0m[01;32mbuild.sh[0m*       DS_definitions.h  DS_timer.h      [01;32mrun_ncu.sh[0m*
[01;34mbuild[0m/         CMakeLists.txt  DS_timer.cpp      ncu_report.txt
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/06.gemm_cutlass# vim run+[K_ncu.sh
[?2004l[?1049h[?1h=[1;55r[23m[24m[0m[H[J[?25l[55;1H"run_ncu.sh" 3L, 106B[2;1H▽[6n[2;1H  [3;1HPzz\[0%m[6n[3;1H           [1;1H[1;1Hscript ncu_report.txt
sudo /usr/local/cuda/bin/ncu [35m--set[0m full [35m--target-processes[0m all ./build/program[2;79H[K[3;1H[33mexit[0m[3;5H[K[4;1H[1m[34m~                                                                                                                        [5;1H~                                                                                                                        [6;1H~                                                                                                                        [7;1H~                                                                                                                        [8;1H~                                                                                                                        [9;1H~                                                                                                                        [10;1H~                                                                                                                        [11;1H~                                                                                                                        [12;1H~                                                                                                                        [13;1H~                                                                                                                        [14;1H~                                                                                                                        [15;1H~                                                                                                                        [16;1H~                                                                                                                        [17;1H~                                                                                                                        [18;1H~                                                                                                                        [19;1H~                                                                                                                        [20;1H~                                                                                                                        [21;1H~                                                                                                                        [22;1H~                                                                                                                        [23;1H~                                                                                                                        [24;1H~                                                                                                                        [25;1H~                                                                                                                        [26;1H~                                                                                                                        [27;1H~                                                                                                                        [28;1H~                                                                [30;1H~                                                                                                                        [31;1H~                                                                                                                        [32;1H~                                                                                                                        [33;1H~                                                                                                                        [34;1H~                                                                                                                        [35;1H~                                                                                                                        [36;1H~                                                                                                                        [37;1H~                                                                                                                        [38;1H~                                                                                                                        [39;1H~                                                                                                                        [40;1H~                                                                                                                        [41;1H~                                                                                                                        [42;1H~                                                                                                                        [43;1H~                                                                                                                        [44;1H~                                                                                                                        [45;1H~                                                                                                                        [46;1H~                                                                                                                        [47;1H~                                                                                                                        [48;1H~                                                                                                                        [49;1H~                                                                                                                        [50;1H~                                                                                                                        [51;1H~                                                                                                                        [52;1H~                                                                                                                        [53;1H~                                                                                                                        [54;1H~                                                                                                                        [0m[55;104H1,1[11CAll[1;1H[34h[?25h[?25l[55;94Hi[1;1H[55;94H [1;1H[55;1H[1m-- INSERT --[0m[55;14H[K[55;104H1,1[11CAll[1;1H[34h[?25h[?25l[34m#script ncu_report.txt[0m[55;106H2[1;2H[34h[?25h[55;1H[K[?25l[55;104H1,1[11CAll[1;1H[34h[?25h[?25l[55;94Hj[1;1H[55;94H [2;1H[55;104H2[2;1H[34h[?25h[?25l[55;94Hj[2;1H[55;94H [3;1H[55;104H3[3;1H[34h[?25h[?25l[55;94Hi[3;1H[55;94H [3;1H[55;1H[1m-- INSERT --[0m[55;104H[K[55;104H3,1[11CAll[3;1H[34h[?25h[?25l[34m#exit[0m[55;106H2[3;2H[34h[?25h[55;1H[K[?25l[55;104H3,1[11CAll[3;1H[34h[?25h[?25l[55;94H:[3;1H[55;94H[K[55;1H:[34h[?25h2![?25l[55;3H[K[55;3H[34h[?25h[?25l[55;2H[K[55;2H[34h[?25hw![?25l"run_ncu.sh" 3L, 108B written[3;1H[55;104H3,1[11CAll[55;104H[K[55;104H3,1[11CAll[3;1H[34h[?25h[?25l[34h[?2[K[55;3H[34h[?25h[?25l[55;2H[K[55;2H[34h[?25hw![?25l"run_ncu.sh" 3L, 108B written[3;1H[55;104H3,1[11CAll[55;104H[K[55;104H3,1[11CAll[3;1H[34h[?25h[?25l[34h[?25h[?25l[55;94H:[3;1H[55;1H[K[55;1H:[34h[?25hq![?25l[55;1H[K[55;1H[?1l>[34h[?25h[?1049l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/06.gemm_cutlass# script run_ncu.[K[K[K[K[K[K[K[K[Kroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/06.gemm_cutlass# script [Kroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/06.gemm_cutlass# script f[K[K[K[K[K[K[K[Kf[Kl
[?2004lbasic_gemm.cu  [0m[01;32mbuild.sh[0m*       DS_definitions.h  DS_timer.h      [01;32mrun_ncu.sh[0m*
[01;34mbuild[0m/         CMakeLists.txt  DS_timer.cpp      ncu_report.txt
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/06.gemm_cutlass# script report.tgxt[K[K[Kxt
[?2004lScript started, output log file is 'report.txt'.
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/06.gemm_cutlass# ./run_ncu.sh
[?2004l==PROF== Connected to process 27148 (/home/kyp/Workspace/00.test/Test/cuda_study/06.gemm_cutlass/build/program)
==PROF== Profiling "InitializeMatrix_kernel" - 0: 0%....50%....100% - 34 passes
==PROF== Profiling "InitializeMatrix_kernel" - 1: 0%....50%....100% - 34 passes
==PROF== Profiling "InitializeMatrix_kernel" - 2: 0%....50%....100% - 34 passes
==PROF== Profiling "InitializeMatrix_kernel" - 3: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 4: 0%....50%....100% - 34 passes
==PROF== Profiling "ReferenceGemm_kernel" - 5: 0%....50%....100% - 34 passes
CUTLASS GEMM duration: 244.199ms 
Reference GEMM duration: 941.569ms 
Passed.
==PROF== Disconnected from process 27148
[27148] program@127.0.0.1
  InitializeMatrix_kernel(float *, int, int, int), 2025-May-15 08:56:06, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.50
    SM Frequency                                                             cycle/nsecond                           1.16
    Elapsed Cycles                                                                   cycle                         15,235
    Memory [%]                                                                           %                          69.82
    DRAM Throughput                                                                      %                          69.82
    Duration                                                                       usecond                          13.12
    L1/TEX Cache Throughput                                                              %                          61.59
    L2 Cache Throughput                                                                  %                          49.87
    SM Active Cycles                                                                 cycle                      11,959.18
    Compute (SM) [%]                                                                     %                          46.17
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           2.25
    Executed Ipc Elapsed                                                        inst/cycle                           1.77
    Issue Slots Busy                                                                     %                          56.76
    Issued Ipc Active                                                           inst/cycle                           2.27
    SM Busy                                                                              %                          56.76
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   ALU is the highest-utilized pipeline (39.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                         217.98
    Mem Busy                                                                             %                          49.87
    Max Bandwidth                                                                        %                          69.82
    L1/TEX Hit Rate                                                                      %                           2.35
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                         100.02
    Mem Pipes Busy                                                                       %                          46.17
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.0 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          57.95
    Issued Warp Per Scheduler                                                                                        0.58
    No Eligible                                                                          %                          42.05
    Active Warps Per Scheduler                                                        warp                           6.10
    Eligible Warps Per Scheduler                                                      warp                           1.07
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          6.10 active warps per scheduler, but only an average of 1.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                          10.52
    Warp Cycles Per Executed Instruction                                             cycle                          10.62
    Avg. Active Threads Per Warp                                                                                       32
    Avg. Not Predicated Off Threads Per Warp                                                                        30.61
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       6,729.14
    Executed Instructions                                                             inst                        753,664
    Avg. Issued Instructions Per Scheduler                                            inst                       6,787.78
    Issued Instructions                                                               inst                        760,231
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       4,096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                           8.19
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                      1,048,576
    Waves Per SM                                                                                                    24.38
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                              8
    Block Limit Warps                                                                block                              6
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          53.34
    Achieved Active Warps Per SM                                                      warp                          25.60
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (53.3%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.09
    Branch Instructions                                                               inst                         65,536
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------

  InitializeMatrix_kernel(float *, int, int, int), 2025-May-15 08:56:06, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.62
    SM Frequency                                                             cycle/nsecond                           1.18
    Elapsed Cycles                                                                   cycle                         15,502
    Memory [%]                                                                           %                          70.20
    DRAM Throughput                                                                      %                          70.20
    Duration                                                                       usecond                          13.12
    L1/TEX Cache Throughput                                                              %                          60.53
    L2 Cache Throughput                                                                  %                          48.93
    SM Active Cycles                                                                 cycle                      11,979.07
    Compute (SM) [%]                                                                     %                          45.37
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           2.25
    Executed Ipc Elapsed                                                        inst/cycle                           1.74
    Issue Slots Busy                                                                     %                          56.66
    Issued Ipc Active                                                           inst/cycle                           2.27
    SM Busy                                                                              %                          56.66
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   ALU is the highest-utilized pipeline (39.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                         223.11
    Mem Busy                                                                             %                          48.93
    Max Bandwidth                                                                        %                          70.20
    L1/TEX Hit Rate                                                                      %                           2.41
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                         100.02
    Mem Pipes Busy                                                                       %                          45.37
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.0 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          59.91
    Issued Warp Per Scheduler                                                                                        0.60
    No Eligible                                                                          %                          40.09
    Active Warps Per Scheduler                                                        warp                           6.19
    Eligible Warps Per Scheduler                                                      warp                           1.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          6.19 active warps per scheduler, but only an average of 1.11 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                          10.32
    Warp Cycles Per Executed Instruction                                             cycle                          10.41
    Avg. Active Threads Per Warp                                                                                       32
    Avg. Not Predicated Off Threads Per Warp                                                                        30.61
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       6,729.14
    Executed Instructions                                                             inst                        753,664
    Avg. Issued Instructions Per Scheduler                                            inst                       6,787.80
    Issued Instructions                                                               inst                        760,234
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Launch Statistics
    ---------------------------------------------------------------------- ---------------tion                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       4,096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                           8.19
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                      1,048,576
    Waves Per SM                                                                                                    24.38
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                              8
    Block Limit Warps                                                                block                              6
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          50.88
    Achieved Active Warps Per SM                                                      warp                          24.42
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (50.9%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.09
    Branch Instructions                                                               inst                         65,536
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------

  InitializeMatrix_kernel(float *, int, int, int), 2025-May-15 08:56:06, Context 1, Stream 7
    Section: GPU Speed Of Light Throughp--------------- ------------------------------

  InitializeMatrix_kernel(float *, int, int, int), 2025-May-15 08:56:06, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.59
    SM Frequency                                                             cycle/nsecond                           1.17
    Elapsed Cycles                                                                   cycle                         15,756
    Memory [%]                                                                           %                          69.31
    DRAM Throughput                                                                      %                          69.31
    Duration                                                                       usecond                          13.41
    L1/TEX Cache Throughput                                                              %                          59.53
    L2 Cache Throughput                                                                  %                          48.23
    SM Active Cycles                                                                 cycle                      11,834.29
    Compute (SM) [%]                                                                     %                          44.63
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           2.27
    Executed Ipc Elapsed                                                        inst/cycle                           1.71
    Issue Slots Busy                                                                     %                          57.36
    Issued Ipc Active                                                           inst/cycle                           2.29
    SM Busy                                                                              %                          57.36
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   ALU is the highest-utilized pipeline (39.6%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                         219.37
    Mem Busy                                                                             %                          48.23
    Max Bandwidth                                                                        %                          69.31
    L1/TEX Hit Rate                                                                      %                           2.24
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                         100.02
    Mem Pipes Busy                                                                       %                          44.63
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.0 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          57.34
    Issued Warp Per Scheduler                                                                                        0.57
    No Eligible                                                                          %                          42.66
    Active Warps Per Scheduler                                                        warp                           5.92
    Eligible Warps Per Scheduler                                                      warp                           1.06
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          5.92 active warps per scheduler, but only an average of 1.06 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                          10.32
    Warp Cycles Per Executed Instruction                                             cycle                          10.41
    Avg. Active Threads Per Warp                                                                                       32
    Avg. Not Predicated Off Threads Per Warp                                                                        30.61
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       6,729.14
    Executed Instructions                                                             inst                        753,664
    Avg. Issued Instructions Per Scheduler                                            inst                       6,787.80
    Issued Instructions                                                               inst                        760,234
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       4,096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                           8.19
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                      1,048,576
    Waves Per SM                                                                                                    24.38
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                              8
    Block Limit Warps                                                                block                              6
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          52.67
    Achieved Active Warps Per SM                                                      warp                          25.28
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (52.7%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.09
    Branch Instructions                                                               inst                         65,536
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------

  InitializeMatrix_kernel(float *, int, int, int), 2025-May-15 08:56:06, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.56
    SM Frequency                                                             cycle/nsecond                           1.17
    Elapsed Cycles                                                                   cycle                         15,419
    Memory [%]                                                                           %                          69.71
    DRAM Throughput                                                                      %                          69.71
    Duration                                                                       usecond                          13.12
    L1/TEX Cache Throughput                                                              %                          60.86
    L2 Cache Throughput                                                                  %                          49.27
    SM Active Cycles                                                                 cycle                      11,569.25
    Compute (SM) [%]                                                                     %                          45.62
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           2.33
    Executed Ipc Elapsed                                                        inst/cycle                           1.75
    Issue Slots Busy                                                                     %                          58.67
    Issued Ipc Active                                                           inst/cycle                           2.35
    SM Busy                                                                              %                          58.67
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   ALU is the highest-utilized pipeline (40.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                         219.39
    Mem Busy                                                                             %                          49.27
    Max Bandwidth                                                                        %                          69.71
    L1/TEX Hit Rate                                                                      %                           2.34
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                         100.02
    Mem Pipes Busy                                                                       %                          45.62
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.0 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          58.52
    Issued Warp Per Scheduler                                                                                        0.59
    No Eligible                                                                          %                          41.48
    Active Warps Per Scheduler                                                        warp                           5.94
    Eligible Warps Per Scheduler                                                      warp                           1.08
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          5.94 active warps per scheduler, but only an average of 1.08 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                          10.15
    Warp Cycles Per Executed Instruction                                             cycle                          10.24
    Avg. Active Threads Per Warp                                                                                       32
    Avg. Not Predicated Off Threads Per Warp                                                                        30.61
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       6,729.14
    Executed Instructions                                                             inst                        753,664
    Avg. Issued Instructions Per Scheduler                                            inst                       6,787.80
    Issued Instructions                                                               inst                        760,234
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       4,096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                           8.19
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                      1,048,576
    Waves Per SM                                                                                                    24.38
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                              8
    Block Limit Warps                                                                block                              6
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          55.09
    Achieved Active Warps Per SM                                                      warp                          26.44
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (55.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.09
    Branch Instructions                                                               inst                         65,536
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------

  void cutlass::Kernel<cutlass::gemm::kernel::Gemm<cutlass::gemm::threadblock::MmaPipelined<cutlass::gemm::GemmShape<(int)128, (int)128, (int)8>, cutlass::transform::threadblock::PredicatedTileIterator<cutlass::MatrixShape<(int)128, (int)8>, float, cutlass::layout::RowMajor, (int)1, cutlass::transform::PitchLinearStripminedThreadMap<cutlass::PitchLinearShape<(int)8, (int)128>, (int)256, (int)1>, (int)1, (bool)0, cutlass::layout::NoPermute>, cutlass::transform::threadblock::RegularTileIterator<cutlass::MatrixShape<(int)128, (int)8>, float, cutlass::layout::ColumnMajor, (int)1, cutlass::transform::TransposePitchLinearThreadMapSimt<cutlass::transform::PitchLinearStripminedThreadMap<cutlass::PitchLinearShape<(int)8, (int)128>, (int)256, (int)1>>, (int)4>, cutlass::transform::threadblock::PredicatedTileIterator<cutlass::MatrixShape<(int)8, (int)128>, float, cutlass::layout::RowMajor, (int)0, cutlass::transform::PitchLinearStripminedThreadMap<cutlass::PitchLinearShape<(int)128, (int)8>, (int)256, (int)1>, (int)1, (bool)0, cutlass::layout::NoPermute>, cutlass::transform::threadblock::RegularTileIterator<cutlass::MatrixShape<(int)8, (int)128>, float, cutlass::layout::RowMajor, (int)0, cutlass::transform::PitchLinearStripminedThreadMap<cutlass::PitchLinearShape<(int)128, (int)8>, (int)256, (int)1>, (int)4>, float, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaSimt<cutlass::gemm::GemmShape<(int)32, (int)64, (int)8>, float, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaSimtPolicy<cutlass::MatrixShape<(int)4, (int)8>, cutlass::layout::RowMajorInterleaved<(int)2>, cutlass::gemm::GemmShape<(int)4, (int)4, (int)1>>, (int)1, (cutlass::ComplexTransform)0, (cutlass::ComplexTransform)0, bool>, cutlass::MatrixShape<(int)4, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, cutlass::NumericArrayConverter<float, float, (int)4, (cutlass::FloatRoundStyle)2, cutlass::transform::thread::UnaryTransform::Identity>, cutlass::NumericArrayConverter<float, float, (int)4, (cutlass::FloatRoundStyle)2, cutlass::transform::thread::UnaryTransform::Identity>, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)8>, cutlass::gemm::warp::MmaSimt<cutlass::gemm::GemmShape<(int)32, (int)64, (int)8>, float, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaSimtPolicy<cutlass::MatrixShape<(int)4, (int)8>, cutlass::layout::RowMajorInterleaved<(int)2>, cutlass::gemm::GemmShape<(int)4, (int)4, (int)1>>, (int)1, (cutlass::ComplexTransform)0, (cutlass::ComplexTransform)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)1, (int)4, (int)4, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)4, (int)2, (int)1, (int)8>, (int)256, (int)1, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorSimt<cutlass::gemm::GemmShape<(int)32, (int)64, (int)8>, cutlass::gemm::thread::Mma<cutlass::gemm::GemmShape<(int)8, (int)8, (int)1>, float, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd, bool>, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaSimtPolicy<cutlass::MatrixShape<(int)4, (int)8>, cutlass::layout::RowMajorInterleaved<(int)2>, cutlass::gemm::GemmShape<(int)4, (int)4, (int)1>>>, cutlass::epilogue::warp::TileIteratorSimt<cutlass::gemm::GemmShape<(int)32, (int)64, (int)8>, cutlass::gemm::thread::Mma<cutlass::gemm::GemmShape<(int)8, (int)8, (int)1>, float, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd, bool>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaSimtPolicy<cutlass::MatrixShape<(int)4, (int)8>, cutlass::layout::RowMajorInterleaved<(int)2>, cutlass::gemm::GemmShape<(int)4, (int)4, (int)1>>>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)1, (int)4, (int)4, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)4, (int)2, (int)1, (int)8>, (int)256, (int)1, (int)32>::CompactedThreadMap, float, (int)4>, cutlass::epilogue::thread::LinearCombination<float, (int)1, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)17>, (int)1, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (bool)0>>(T1::Params), 2025-May-15 08:56:06, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           7.30
    SM Frequency                                                             cycle/nsecond                           1.32
    Elapsed Cycles                                                                   cycle                        651,128
    Memory [%]                                                                           %                          35.71
    DRAM Throughput                                                                      %                          12.18
    Duration                                                                       usecond                         493.12
    L1/TEX Cache Throughput                                                              %                          47.04
    L2 Cache Throughput                                                                  %                          19.92
    SM Active Cycles                                                                 cycle                     494,344.57
    Compute (SM) [%]                                                                     %                          58.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 46%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           3.06
    Executed Ipc Elapsed                                                        inst/cycle                           2.32
    Issue Slots Busy                                                                     %                          76.40
    Issued Ipc Active                                                           inst/cycle                           3.06
    SM Busy                                                                              %                          76.40
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   FMA is the highest-utilized pipeline (62.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. The pipeline is well-utilized, but might become a bottleneck if more work is added. Based on the  
          number of executed instructions, the highest utilized pipeline (62.2%) is FMA. It executes 32-bit floating    
          point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD) operations. Comparing the two, the overall pipeline    
          utilization appears to be caused by frequent, low-latency instructions. See the Kernel Profiling Guide        
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions in this kernel. Check the Warp State Statistics section for which reasons    
          cause warps to stall.                                                                                         

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                          42.65
    Mem Busy                                                                             %                          33.86
    Max Bandwidth                                                                        %                          35.71
    L1/TEX Hit Rate                                                                      %                              0
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          89.07
    Mem Pipes Busy                                                                       %                          35.71
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          76.44
    Issued Warp Per Scheduler                                                                                        0.76
    No Eligible                                                                          %                          23.56
    Active Warps Per Scheduler                                                        warp                           2.00
    Eligible Warps Per Scheduler                                                      warp                           1.27
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           2.62
    Warp Cycles Per Executed Instruction                                             cycle                           2.62
    Avg. Active Threads Per Warp                                                                                       32
    Avg. Not Predicated Off Threads Per Warp                                                                        31.75
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                        377,600
    Executed Instructions                                                             inst                     42,291,200
    Avg. Issued Instructions Per Scheduler                                            inst                     377,657.33
    Issued Instructions                                                               inst                     42,297,621
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          64
    Registers Per Thread                                                   register/thread                            180
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          16.64
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         16,384
    Waves Per SM                                                                                                     2.29
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              1
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                              6
    Theoretical Active Warps per SM                                                   warp                              8
    Theoretical Occupancy                                                                %                          16.67
    Achieved Occupancy                                                                   %                          16.64
    Achieved Active Warps Per SM                                                      warp                           7.99
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (16.7%) is limited by the number of required registers This kernel's      
          theoretical occupancy (16.7%) is limited by the required amount of shared memory See the CUDA Best Practices  
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.00
    Branch Instructions                                                               inst                        135,168
    Branch Efficiency                                                                    %                            100
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------

  ReferenceGemm_kernel(int, int, int, float, const float *, int, const float *, int, float, float *, int), 2025-May-15 08:56:07, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           7.29
    SM Frequency                                                             cycle/nsecond                           1.32
    Elapsed Cycles                                                                   cycle                      4,914,406
    Memory [%]                                                                           %                          97.73
    DRAM Throughput                                                                      %                          21.39
    Duration                                                                       msecond                           3.73
    L1/TEX Cache Throughput                                                              %                          98.23
    L2 Cache Throughput                                                                  %                          19.73
    SM Active Cycles                                                                 cycle                   4,889,633.71
    Compute (SM) [%]                                                                     %                          97.73
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           1.08
    Executed Ipc Elapsed                                                        inst/cycle                           1.08
    Issue Slots Busy                                                                     %                          27.09
    Issued Ipc Active                                                           inst/cycle                           1.08
    SM Busy                                                                              %                          32.77
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                          74.82
    Mem Busy                                                                             %                          73.35
    Max Bandwidth                                                                        %                          97.73
    L1/TEX Hit Rate                                                                      %                          87.81
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          48.00
    Mem Pipes Busy                                                                       %                          97.73
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.0 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 1.4 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          27.09
    Issued Warp Per Scheduler                                                                                        0.27
    No Eligible                                                                          %                          72.91
    Active Warps Per Scheduler                                                        warp                          11.80
    Eligible Warps Per Scheduler                                                      warp                           1.39
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 3.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.80 active warps per scheduler, but only an average of 1.39 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                          43.55
    Warp Cycles Per Executed Instruction                                             cycle                          43.55
    Avg. Active Threads Per Warp                                                                                       32
    Avg. Not Predicated Off Threads Per Warp                                                                        31.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 24.2 cycles being stalled waiting for the local/global            
          instruction queue to be not full. This represents about 55.6% of the total average of 43.5 cycles between     
          issuing two instructions. Typically this stall occurs only when executing local or global memory              
          instructions extremely frequently. If applicable, consider combining multiple lower-width memory operations   
          into fewer wider memory operations and try interleaving memory operations and math instructions.              
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                   1,324,763.43
    Executed Instructions                                                             inst                    148,373,504
    Avg. Issued Instructions Per Scheduler                                            inst                   1,324,833.89
    Issued Instructions                                                               inst                    148,381,396
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       4,096
    Registers Per Thread                                                   register/thread                             40
    Shared Memory Configuration Size                                                 Kbyte                           8.19
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                      1,048,576
    Waves Per SM                                                                                                    24.38
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              6
    Block Limit Shared Mem                                                           block                              8
    Block Limit Warps                                                                block                              6
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          98.32
    Achieved Active Warps Per SM                                                      warp                          47.19
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.02
    Branch Instructions                                                               inst                      2,392,064
    Branch Efficiency                                                                    %                            100
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------

[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/06.gemm_cutlass# exit
[?2004lexit
Script done.
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/06.gemm_cutlass# l
[?2004lbasic_gemm.cu  [0m[01;32mbuild.sh[0m*       DS_definitions.h  DS_timer.h      report.txt
[01;34mbuild[0m/         CMakeLists.txt  DS_timer.cpp      ncu_report.txt  [01;32mrun_ncu.sh[0m*
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/06.gemm_cutlass# vim run[K[K[Kreport.txt 
[?2004l[?1049h[?1h=[1;55r[23m[24m[0m[H[J[?25l[55;1H"report.txt" 784L, 87039B[2;1H▽[6n[2;1H  [3;1HPzz\[0%m[6n[3;1H           [1;1H[1;1HScript started on 2025-05-15 08:55:59+09:00 [TERM="screen" TTY="/dev/pts/6" COLUMNS="121" LINES="55"]
[34m^[[0m[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/06.gemm_cutlass# ./run_ncu.sh[34m^M[0m[2;100H[K[3;1H[34m^[[0m[?2004l[34m^M[0m==PROF== Connected to process 27148 (/home/kyp/Workspace/00.test/Test/cuda_study/06.gemm_cutlass/build/programm[4;1H)[34m^M[0m
==PROF== Profiling "InitializeMatrix_kernel" - 0: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "InitializeMatrix_kernel" - 1: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "InitializeMatrix_kernel" - 2: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "InitializeMatrix_kernel" - 3: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 4: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "ReferenceGemm_kernel" - 5: 0%....50%....100% - 34 passes[34m^M[0m
CUTLASS GEMM duration: 244.199ms [34m^M[0m
Reference GEMM duration: 941.569ms [34m^M[0m
Passed.[34m^M[0m
==PROF== Disconnected from process 27148[34m^M[0m
[27148] program@127.0.0.1[34m^M[0m
  InitializeMatrix_kernel(float *, int, int, int), 2025-May-15 08:56:06, Context 1, Stream 7[34m^M[0m
    Section: GPU Speed Of Light Throughput[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[19;1H[34m^M[0m
    DRAM Frequency[59Ccycle/nsecond[27C6.500[21;1H[34m^M[0m
    SM Frequency[61Ccycle/nsecond[27C1.166[23;1H[34m^M[0m
    Elapsed Cycles[67Ccycle[25C15,2355[25;1H[34m^M[0m
    Memory [%][75C%[26C69.822[27;1H[34m^M[0m
    DRAM Throughput[70C%[26C69.822[29;1H[34m^M[0m
    Duration[71Cusecond[26C13.122[31;1H[34m^M[0m
    L1/TEX Cache Throughput[62C%[26C61.599[33;1H[34m^M[0m
    L2 Cache Throughput[66C%[26C49.877[35;1H[34m^M[0m
    SM Active Cycles[65Ccycle[22C11,959.188[37;1H[34m^M[0m
    Compute (SM) [%][69C%[26C46.177[39;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[41;1H[34m^M[0m
    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    [34m^^[43;1HM[0m[44;11HDRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the[7C[34m^^[45;1HM[0m[46;11Hbytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  [34m^^[47;1HM[0m[48;11Hwhether there are values you can (re)compute.[65C[34m^^[49;1HM
^M[0m
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of [34m^^[52;1HM[0m[53;11Hthis device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide[7C[34m^^[54;1HM[0m[55;104H1,1[11CTop[1;1H[34h[?25h[?25l[55;94Hj[1;1H[55;94H [2;1H[55;104H2[2;1H[34h[?25h[?25l[55;94Hj[2;1H[55;94H [3;1H[55;104H3[3;1H[34h[?25h[?25l[55;94Hj[3;1H[55;94H [5;1H[55;104H4[5;1H[34h[?25h[?25l[55;94Hj[5;1H[55;94H [6;1H[55;104H5[6;1H[34h[?25h[?25l[55;94Hj[6;1H[55;94H [7;1H[55;104H6[7;1H[34h[?25h[?25l[55;94Hj[7;1H[55;94H [8;1H[55;104H7[8;1H[34h[?25h[?25l[55;94Hj[8;1H[55;94H [9;1H[55;104H8[9;1H[34h[?25h[?25l[55;94Hj[9;1H[55;94H [10;1H[55;104H9[10;1H[34h[?25h[?25l[55;94Hj[10;1H[55;94H [11;1H[55;104H10,1[11;1H[34h[?25h[?25l[55;94Hj[11;1H[55;94H [12;1H[55;105H1[12;1H[34h[?25h[?25l[55;94HG[12;1H[55;94H [53;1H[23m[24m[0m[H[J[1;5HDynamic Shared Memory Per Block[45Cbyte/block[30C00[2;1H[34m^M[0m
    Static Shared Memory Per Block[46Cbyte/block[30C00[4;1H[34m^M[0m
    Threads[73Cthread[22C1,048,5766[6;1H[34m^M[0m
    Waves Per SM[100C24.388[8;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[10;1H[34m^M
^M[0m
    Section: Occupancy[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[14;1H[34m^M[0m
    Block Limit SM[67Cblock[29C166[16;1H[34m^M[0m
    Block Limit Registers[60Cblock[30C66[18;1H[34m^M[0m
    Block Limit Shared Mem[59Cblock[30C88[20;1H[34m^M[0m
    Block Limit Warps[64Cblock[30C66[22;1H[34m^M[0m
    Theoretical Active Warps per SM[51Cwarp[29C488[24;1H[34m^M[0m
    Theoretical Occupancy[64C%[28C1000[26;1H[34m^M[0m
    Achieved Occupancy[67C%[26C98.322[28;1H[34m^M[0m
    Achieved Active Warps Per SM[54Cwarp[26C47.199[30;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[32;1H[34m^M[0m
    INF   This kernel's theoretical occupancy is not impacted by any block limit.[39C[34m^^[34;1HM
^M[0m
    Section: Source Counters[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[38;1H[34m^M[0m
    Branch Instructions Ratio[60C%[27C0.022[40;1H[34m^M[0m
    Branch Instructions[63Cinst[22C2,392,0644[42;1H[34m^M[0m
    Branch Efficiency[68C%[28C1000[44;1H[34m^M[0m
    Avg. Divergent Branches[93C00[46;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[48;1H[34m^M
^M
^[[0m[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/06.gemm_cutlass# exit[34m^M
^[[0m[?2004l[34m^M[0mexit[34m^M[0m

Script done on 2025-05-15 08:56:10+09:00 [COMMAND_EXIT_CODE="0"]
[1m[34m~                                                                                                                        [0m[55;104H784,1[9CBot[53;1H[34h[?25h[?25l[55;94Hk[53;1H[55;94H [52;1H[55;106H3,0-1[52;1H[34h[?25h[?25l[55;94Hk[52;1H[55;94H [51;1H[55;106H2,1  [51;1H[34h[?25h[?25l[55;94H1[51;1H[34h[?25h[?25l[55;95HG[51;1H[55;94H  [1;1H[23m[24m[0m[H[J[1;1HScript started on 2025-05-15 08:55:59+09:00 [TERM="screen" TTY="/dev/pts/6" COLUMNS="121" LINES="55"]
[34m^[[0m[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/06.gemm_cutlass# ./run_ncu.sh[34m^M
^[[0m[?2004l[34m^M[0m==PROF== Connected to process 27148 (/home/kyp/Workspace/00.test/Test/cuda_study/06.gemm_cutlass/build/programm[4;1H)[34m^M[0m
==PROF== Profiling "InitializeMatrix_kernel" - 0: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "InitializeMatrix_kernel" - 1: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "InitializeMatrix_kernel" - 2: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "InitializeMatrix_kernel" - 3: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 4: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "ReferenceGemm_kernel" - 5: 0%....50%....100% - 34 passes[34m^M[0m
CUTLASS GEMM duration: 244.199ms [34m^M[0m
Reference GEMM duration: 941.569ms [34m^M[0m
Passed.[34m^M[0m
==PROF== Disconnected from process 27148[34m^M[0m
[27148] program@127.0.0.1[34m^M[0m
  InitializeMatrix_kernel(float *, int, int, int), 2025-May-15 08:56:06, Context 1, Stream 7[34m^M[0m
    Section: GPU Speed Of Light Throughput[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[19;1H[34m^M[0m
    DRAM Frequency[59Ccycle/nsecond[27C6.500[21;1H[34m^M[0m
    SM Frequency[61Ccycle/nsecond[27C1.166[23;1H[34m^M[0m
    Elapsed Cycles[67Ccycle[25C15,2355[25;1H[34m^M[0m
    Memory [%][75C%[26C69.822[27;1H[34m^M[0m
    DRAM Throughput[70C%[26C69.822[29;1H[34m^M[0m
    Duration[71Cusecond[26C13.122[31;1H[34m^M[0m
    L1/TEX Cache Throughput[62C%[26C61.599[33;1H[34m^M[0m
    L2 Cache Throughput[66C%[26C49.877[35;1H[34m^M[0m
    SM Active Cycles[65Ccycle[22C11,959.188[37;1H[34m^M[0m
    Compute (SM) [%][69C%[26C46.177[39;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[41;1H[34m^M[0m
    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    [34m^^[43;1HM[0m[44;11HDRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the[7C[34m^^[45;1HM[0m[46;11Hbytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  [34m^^[47;1HM[0m[48;11Hwhether there are values you can (re)compute.[65C[34m^^[49;1HM
^M[0m
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of [34m^^[52;1HM[0m[53;11Hthis device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide[7C[34m^^[54;1HM[0m[55;104H1,1[11CTop[1;1H[34h[?25h[?25l[55;104H2[2;1H[34h[?25h[?25l[55;94Hj[2;1H[55;94H [3;1H[55;104H3[3;1H[34h[?25h[?25l[55;94HG[3;1H[55;94H [53;1H[23m[24m[0m[H[J[1;5HDynamic Shared Memory Per Block[45Cbyte/block[30C00[2;1H[34m^M[0m
    Static Shared Memory Per Block[46Cbyte/block[30C00[4;1H[34m^M[0m
    Threads[73Cthread[22C1,048,5766[6;1H[34m^M[0m
    Waves Per SM[100C24.388[8;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[10;1H[34m^M
^M[0m
    Section: Occupancy[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[14;1H[34m^M[0m
    Block Limit SM[67Cblock[29C166[16;1H[34m^M[0m
    Block Limit Registers[60Cblock[30C66[18;1H[34m^M[0m
    Block Limit Shared Mem[59Cblock[30C88[20;1H[34m^M[0m
    Block Limit Warps[64Cblock[30C66[22;1H[34m^M[0m
    Theoretical Active Warps per SM[51Cwarp[29C488[24;1H[34m^M[0m
    Theoretical Occupancy[64C%[28C1000[26;1H[34m^M[0m
    Achieved Occupancy[67C%[26C98.322[28;1H[34m^M[0m
    Achieved Active Warps Per SM[54Cwarp[26C47.199[30;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[32;1H[34m^M[0m
    INF   This kernel's theoretical occupancy is not impacted by any block limit.[39C[34m^^[34;1HM
^M[0m
    Section: Source Counters[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[38;1H[34m^M[0m
    Branch Instructions Ratio[60C%[27C0.022[40;1H[34m^M[0m
    Branch Instructions[63Cinst[22C2,392,0644[42;1H[34m^M[0m
    Branch Efficiency[68C%[28C1000[44;1H[34m^M[0m
    Avg. Divergent Branches[93C00[46;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[48;1H[34m^M
^M
^[[0m[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/06.gemm_cutlass# exit[34m^M
^[[0m[?2004l[34m^M[0mexit[34m^M[0m

Script done on 2025-05-15 08:56:10+09:00 [COMMAND_EXIT_CODE="0"]
[1m[34m~                                                                                                                        [0m[55;104H784,1[9CBot[53;1H[34h[?25h[?25l[55;94H/[53;1H[55;94H[K[55;1H/[34h[?25hK[?25l[23m[24m[0m[H[J[1;1HScript started on 2025-05-15 08:55:59+09:00 [TERM="screen" TTY="/dev/pts/6" COLUMNS="121" LINES="55"]
[34m^[[0m[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/06.gemm_cutlass# ./run_ncu.sh[34m^M
^[[0m[?2004l[34m^M[0m==PROF== Connected to process 27148 (/home/kyp/Workspace/00.test/Test/cuda_study/06.gemm_cutlass/build/programm[4;1H)[34m^M[0m
==PROF== Profiling "InitializeMatrix_kernel" - 0: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "InitializeMatrix_kernel" - 1: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "InitializeMatrix_kernel" - 2: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "InitializeMatrix_kernel" - 3: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "[7mK[0mernel" - 4: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "ReferenceGemm_kernel" - 5: 0%....50%....100% - 34 passes[34m^M[0m
CUTLASS GEMM duration: 244.199ms [34m^M[0m
Reference GEMM duration: 941.569ms [34m^M[0m
Passed.[34m^M[0m
==PROF== Disconnected from process 27148[34m^M[0m
[27148] program@127.0.0.1[34m^M[0m
  InitializeMatrix_kernel(float *, int, int, int), 2025-May-15 08:56:06, Context 1, Stream 7[34m^M[0m
    Section: GPU Speed Of Light Throughput[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[19;1H[34m^M[0m
    DRAM Frequency[59Ccycle/nsecond[27C6.500[21;1H[34m^M[0m
    SM Frequency[61Ccycle/nsecond[27C1.166[23;1H[34m^M[0m
    Elapsed Cycles[67Ccycle[25C15,2355[25;1H[34m^M[0m
    Memory [%][75C%[26C69.822[27;1H[34m^M[0m
    DRAM Throughput[70C%[26C69.822[29;1H[34m^M[0m
    Duration[71Cusecond[26C13.122[31;1H[34m^M[0m
    L1/TEX Cache Throughput[62C%[26C61.599[33;1H[34m^M[0m
    L2 Cache Throughput[66C%[26C49.877[35;1H[34m^M[0m
    SM Active Cycles[65Ccycle[22C11,959.188[37;1H[34m^M[0m
    Compute (SM) [%][69C%[26C46.177[39;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[41;1H[34m^M[0m
    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    [34m^^[43;1HM[0m[44;11HDRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the[7C[34m^^[45;1HM[0m[46;11Hbytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  [34m^^[47;1HM[0m[48;11Hwhether there are values you can (re)compute.[65C[34m^^[49;1HM
^M[0m
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of [34m^^[52;1HM[0m[53;11Hthis device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide[7C[34m^^[54;1HM[0m[55;104H8,21[10CTop/K[55;104H[K[55;3H[34h[?25he[?25l[9;22H[7me[0m[55;104H8,21[10CTop[55;104H[K[55;4H[34h[?25hr[?25l[9;23H[7mr[0m[55;104H8,21[10CTop[55;104H[K[55;5H[34h[?25hn[?25l[9;24H[7mn[0m[55;104H8,21[10CTop[55;104H[K[55;6H[34h[?25he[?25l[9;25H[7me[0m[55;104H8,21[10CTop[55;104H[K[55;7H[34h[?25hl[?25l[9;26H[7ml[0m[55;104H8,21[10CTop[55;104H[K[55;8H[34h[?25h[?25l[31msearch hit BOTTOM, continuing at TOP[9;21H[0mKernel[55;104H8,21[10CTop[55;104H[K[55;104H8,21[10CTop[9;21H[34h[?25h[?25l[55;94Hn[9;21H[55;1H/Kernel [55;9H[K[55;1H[48;92H[1;54r[1;1H[5M[1;55r[50;11H(https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      [34m^^[51;1HM[0m[52;11Hanalysis.[101C[34m^^[53;1HM
^M[0m[55;1H[K[55;104H35,92[10C0%[48;92H[34h[?25h[?25l[55;94Hk[48;92H[55;94H [46;92H[55;105H4[46;92H[34h[?25h[?25l[55;94Hk[46;92H[55;94H [45;1H[55;105H3,1 [45;1H[34h[?25h[?25l[55;94Hk[45;1H[55;94H [43;92H[55;105H2,92[43;92H[34h[?25h[?25l[55;94Hk[43;92H[55;94H [41;92H[55;105H1[41;92H[34h[?25h[?25l[55;94Hk[41;92H[55;94H [39;92H[55;105H0[39;92H[34h[?25h[?25l[55;94Hk[39;92H[55;94H [37;92H[55;104H29[37;92H[34h[?25h[?25l[55;94H1[37;92H[34h[?25h[?25l[55;95H/[37;92H[55;94H[K[55;1H/[34h[?25hRef[?25l[23m[24m[0m[H[J[1;5H---------------------------------------------------------------------- --------------- -------------------------------[2;1H[34m^M[0m
    WRN   This kernel's theoretical occupancy (16.7%) is limited by the number of required registers This kernel's      [34m^^[4;1HM[0m[5;11Htheoretical occupancy (16.7%) is limited by the required amount of shared memory See the CUDA Best Practices  [34m^^[6;1HM[0m[7;11HGuide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     [34m^^[8;1HM[0m[9;11Hoptimizing occupancy.[89C[34m^^[10;1HM
^M[0m
    Section: Source Counters[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[14;1H[34m^M[0m
    Branch Instructions Ratio[60C%[27C0.000[16;1H[34m^M[0m
    Branch Instructions[63Cinst[24C135,1688[18;1H[34m^M[0m
    Branch Efficiency[68C%[28C1000[20;1H[34m^M[0m
    Avg. Divergent Branches[93C00[22;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[24;1H[34m^M
^M[0m
  [7mRef[0merenceGemm_kernel(int, int, int, float, const float *, int, const float *, int, float, float *, int), 2025-May-15 088[27;1H:56:07, Context 1, Stream 7[34m^M[0m
    Section: GPU Speed Of Light Throughput[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[30;1H[34m^M[0m
    DRAM Frequency[59Ccycle/nsecond[27C7.299[32;1H[34m^M[0m
    SM Frequency[61Ccycle/nsecond[27C1.322[34;1H[34m^M[0m
    Elapsed Cycles[67Ccycle[22C4,914,4066[36;1H[34m^M[0m
    Memory [%][75C%[26C97.733[38;1H[34m^M[0m
    DRAM Throughput[70C%[26C21.399[40;1H[34m^M[0m
    Duration[71Cmsecond[27C3.733[42;1H[34m^M[0m
    L1/TEX Cache Throughput[62C%[26C98.233[44;1H[34m^M[0m
    L2 Cache Throughput[66C%[26C19.733[46;1H[34m^M[0m
    SM Active Cycles[65Ccycle[19C4,889,633.711[48;1H[34m^M[0m
    Compute (SM) [%][69C%[26C97.733[50;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[52;1H[34m^M[0m
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   [34m^^[54;1HM[0m[55;104H646,3[9C83%/Ref[55;104H[K[55;5H[34h[?25he[?25l[26;6H[7me[0m[114C88[27;1H:[55;104H646,3[9C83%[55;104H[K[55;6H[34h[?25hr[?25l[26;7H[7mr[0m[113C88[27;1H:[55;104H646,3[9C83%[55;104H[K[55;7H[34h[?25he[?25l[26;8H[7me[0m[112C88[27;1H:[55;104H646,3[9C83%[55;104H[K[55;8H[34h[?25hn[?25l[26;9H[7mn[0m[111C88[27;1H:[55;104H646,3[9C83%[55;104H[K[55;9H[34h[?25hc[?25l[26;10H[7mc[0m[110C88[27;1H:[55;104H646,3[9C83%[55;104H[K[55;10H[34h[?25he[?25l[26;11H[7me[0m[109C88[27;1H:[55;104H646,3[9C83%[55;104H[K[55;11H[34h[?25hG[?25l[26;12H[7mG[0m[108C88[27;1H:[55;104H646,3[9C83%[55;104H[K[55;12H[34h[?25he[?25l[26;13H[7me[0m[107C88[27;1H:[55;104H646,3[9C83%[55;104H[K[55;13H[34h[?25hn[?25l[23m[24m[0m[H[J[1;1H==PROF== Profiling "InitializeMatrix_kernel" - 1: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "InitializeMatrix_kernel" - 2: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "InitializeMatrix_kernel" - 3: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 4: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "ReferenceGemm_kernel" - 5: 0%....50%....100% - 34 passes[34m^M[0m
CUTLASS GEMM duration: 244.199ms [34m^M[0m
Reference GEMM duration: 941.569ms [34m^M[0m
Passed.[34m^M[0m
==PROF== Disconnected from process 27148[34m^M[0m
[27148] program@127.0.0.1[34m^M[0m
  InitializeMatrix_kernel(float *, int, int, int), 2025-May-15 08:56:06, Context 1, Stream 7[34m^M[0m
    Section: GPU Speed Of Light Throughput[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[14;1H[34m^M[0m
    DRAM Frequency[59Ccycle/nsecond[27C6.500[16;1H[34m^M[0m
    SM Frequency[61Ccycle/nsecond[27C1.166[18;1H[34m^M[0m
    Elapsed Cycles[67Ccycle[25C15,2355[20;1H[34m^M[0m
    Memory [%][75C%[26C69.822[22;1H[34m^M[0m
    DRAM Throughput[70C%[26C69.822[24;1H[34m^M[0m
    Duration[71Cusecond[26C13.122[26;1H[34m^M[0m
    L1/TEX Cache Throughput[62C%[26C61.599[28;1H[34m^M[0m
    L2 Cache Throughput[66C%[26C49.877[30;1H[34m^M[0m
    SM Active Cycles[65Ccycle[22C11,959.188[32;1H[34m^M[0m
    Compute (SM) [%][69C%[26C46.177[34;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[36;1H[34m^M[0m
    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    [34m^^[38;1HM[0m[39;11HDRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the[7C[34m^^[40;1HM[0m[41;11Hbytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  [34m^^[42;1HM[0m[43;11Hwhether there are values you can (re)compute.[65C[34m^^[44;1HM
^M[0m
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of [34m^^[47;1HM[0m[48;11Hthis device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide[7C[34m^^[49;1HM[0m[50;11H(https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      [34m^^[51;1HM[0m[52;11Hanalysis.[101C[34m^^[53;1HM
^M[0m[55;104H29,92[10C0%/ReferenceGen[55;104H[K[55;14H[34h[?25hn[?25l[89C29,92[10C0%[55;104H[K[55;15H[34h[?25h[?25l[55;14H[K[55;104H29,92[10C0%[55;104H[K[55;14H[34h[?25h[?25l[55;13H[K[23m[24m[0m[H[J[1;5H---------------------------------------------------------------------- --------------- -------------------------------[2;1H[34m^M[0m
    WRN   This kernel's theoretical occupancy (16.7%) is limited by the number of required registers This kernel's      [34m^^[4;1HM[0m[5;11Htheoretical occupancy (16.7%) is limited by the required amount of shared memory See the CUDA Best Practices  [34m^^[6;1HM[0m[7;11HGuide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     [34m^^[8;1HM[0m[9;11Hoptimizing occupancy.[89C[34m^^[10;1HM
^M[0m
    Section: Source Counters[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[14;1H[34m^M[0m
    Branch Instructions Ratio[60C%[27C0.000[16;1H[34m^M[0m
    Branch Instructions[63Cinst[24C135,1688[18;1H[34m^M[0m
    Branch Efficiency[68C%[28C1000[20;1H[34m^M[0m
    Avg. Divergent Branches[93C00[22;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[24;1H[34m^M
^M[0m
  [7mReferenceGe[0mmm_kernel(int, int, int, float, const float *, int, const float *, int, float, float *, int), 2025-May-15 088[27;1H:56:07, Context 1, Stream 7[34m^M[0m
    Section: GPU Speed Of Light Throughput[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[30;1H[34m^M[0m
    DRAM Frequency[59Ccycle/nsecond[27C7.299[32;1H[34m^M[0m
    SM Frequency[61Ccycle/nsecond[27C1.322[34;1H[34m^M[0m
    Elapsed Cycles[67Ccycle[22C4,914,4066[36;1H[34m^M[0m
    Memory [%][75C%[26C97.733[38;1H[34m^M[0m
    DRAM Throughput[70C%[26C21.399[40;1H[34m^M[0m
    Duration[71Cmsecond[27C3.733[42;1H[34m^M[0m
    L1/TEX Cache Throughput[62C%[26C98.233[44;1H[34m^M[0m
    L2 Cache Throughput[66C%[26C19.733[46;1H[34m^M[0m
    SM Active Cycles[65Ccycle[19C4,889,633.711[48;1H[34m^M[0m
    Compute (SM) [%][69C%[26C97.733[50;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[52;1H[34m^M[0m
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   [34m^^[54;1HM[0m[55;104H646,3[9C83%/ReferenceGe[55;104H[K[55;13H[34h[?25hm[?25l[26;14H[7mm[0m[106C88[27;1H:[55;104H646,3[9C83%[55;104H[K[55;14H[34h[?25hm[?25l[26;15H[7mm[0m[105C88[27;1H:[55;104H646,3[9C83%[55;104H[K[55;15H[34h[?25h_[?25l[26;16H[7m_[0m[104C88[27;1H:[55;104H646,3[9C83%[55;104H[K[55;16H[34h[?25hK[?25l[23m[24m[0m[H[J[1;1H==PROF== Profiling "InitializeMatrix_kernel" - 1: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "InitializeMatrix_kernel" - 2: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "InitializeMatrix_kernel" - 3: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 4: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "ReferenceGemm_kernel" - 5: 0%....50%....100% - 34 passes[34m^M[0m
CUTLASS GEMM duration: 244.199ms [34m^M[0m
Reference GEMM duration: 941.569ms [34m^M[0m
Passed.[34m^M[0m
==PROF== Disconnected from process 27148[34m^M[0m
[27148] program@127.0.0.1[34m^M[0m
  InitializeMatrix_kernel(float *, int, int, int), 2025-May-15 08:56:06, Context 1, Stream 7[34m^M[0m
    Section: GPU Speed Of Light Throughput[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[14;1H[34m^M[0m
    DRAM Frequency[59Ccycle/nsecond[27C6.500[16;1H[34m^M[0m
    SM Frequency[61Ccycle/nsecond[27C1.166[18;1H[34m^M[0m
    Elapsed Cycles[67Ccycle[25C15,2355[20;1H[34m^M[0m
    Memory [%][75C%[26C69.822[22;1H[34m^M[0m
    DRAM Throughput[70C%[26C69.822[24;1H[34m^M[0m
    Duration[71Cusecond[26C13.122[26;1H2;1H[34m^M[0m
    Compute (SM) [%][69C%[26C46.177[34;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[36;1H[34m^M[0m
    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    [34m^^[38;1HM[0m[39;11HDRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the[7C[34m^^[40;1HM[0m[41;11Hbytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  [34m^^[42;1HM[0m[43;11Hwhether there are values you can (re)compute.[65C[34m^^[44;1HM
^M[0m
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of [34m^^[47;1HM[0m[48;11Hthis device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide[7C[34m^^[49;1HM[0m[50;11H(https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      [34m^^[51;1HM[0m[52;11Hanalysis.[101C[34m^^[53;1HM
^M[0m[55;104H29,92[10C0%/ReferenceGemm_K[55;104H[K[55;17H[34h[?25h[?25l[55;16H[K[23m[24m[0m[H[J[1;5H---------------------------------------------------------------------- --------------- -------------------------------[2;1H[34m^M[0m
    WRN   This kernel's theoretical occupancy (16.7%) is limited by the number of required registers This kernel's      [34m^^[4;1HM[0m[5;11Htheoretical occupancy (16.7%) is limited by the required amount of shared memory See the CUDA Best Practices  [34m^^[6;1HM[0m[7;11HGuide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     [34m^^[8;1HM[0m[9;11Hoptimizing occupancy.[89C[34m^^[10;1HM
^M[0m
    Section: Source Counters[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[14;1H[34m^M[0m
    Branch Instructions Ratio[60C%[27C0.000[16;1H[34m^M[0m
    Branch Instructions[63Cinst[24C135,1688[18;1H[34m^M[0m
    Branch Efficiency[68C%[28C1000[20;1H[34m^M[0m
    Avg. Divergent Branches[93C00[22;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[24;1H[34m^M
^M[0m
  [7mReferenceGemm_[0mkernel(int, int, int, float, const float *, int, const float *, int, float, float *, int), 2025-May-15 088[27;1H:56:07, Context 1, Stream 7[34m^M[0m
    Section: GPU Speed Of Light Throughput[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[30;1H[34m^M[0m
    DRAM Frequency[59Ccycle/nsecond[27C7.299[32;1H[34m^M[0m
    SM Frequency[61Ccycle/nsecond[27C1.322[34;1H[34m^M[0m
    Elapsed Cycles[67Ccycle[22C4,914,4066[36;1H[34m^M[0m
    Memory [%][75C%[26C97.733[38;1H[34m^M[0m
    DRAM Throughput[70C%[26C21.399[40;1H[34m^M[0m
    Duration[71Cmsecond[27C3.733[42;1H[34m^M[0m
    L1/TEX Cache Throughput[62C%[26C98.233[44;1H[34m^M[0m
    L2 Cache Throughput[66C%[26C19.733[46;1H[34m^M[0m
    SM Active Cycles[65Ccycle[19C4,889,633.711[48;1H[34m^M[0m
    Compute (SM) [%][69C%[26C97.733[50;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[52;1H[34m^M[0m
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   [34m^^[54;1HM[0m[55;104H646,3[9C83%/ReferenceGemm_[55;104H[K[55;16H[34h[?25hk[?25l[26;17H[7mk[0m[103C88[27;1H:[55;104H646,3[9C83%[55;104H[K[55;17H[34h[?25he[?25l[26;18H[7me[0m[102C88[27;1H:[55;104H646,3[9C83%[55;104H[K[55;18H[34h[?25hr[?25l[26;19H[7mr[0m[101C88[27;1H:[55;104H646,3[9C83%[55;104H[K[55;19H[34h[?25hn[?25l[26;20H[7mn[0m[100C88[27;1H:[55;104H646,3[9C83%[55;104H[K[55;18H[34h[?25hr[?25l[26;19H[7mr[0m[101C88[27;1H:[55;104H646,3[9C83%[55;104H[K[55;19H[34h[?25hn[?25l[26;20H[7mn[0m[100C88[27;1H:[55;104H646,3[9C83%[55;104H[K[55;20H[34h[?25he[?25l[26;21H[7me[0m[99C88[27;1H:[55;104H646,3[9C83%[55;104H[K[55;21H[34h[?25hl[?25l[26;22H[7ml[0m[98C88[27;1H:[55;104H646,3[9C83%[55;104H[K[55;22H[34h[?25h[?25l[26;3HReferenceGemm_kernel[98C88[27;1H:[55;104H646,3[9C83%[26;3H[34h[?25h[?25l[55;94HN[26;3H[55;1H?[55;94H[K[55;1H[10;21H[23m[24m[0m[H[J[1;1HScript started on 2025-05-15 08:55:59+09:00 [TERM="screen" TTY="/dev/pts/6" COLUMNS="121" LINES="55"]
[34m^[[0m[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/06.gemm_cutlass# ./run_ncu.sh[34m^M
^[[0m[?2004l[34m^M[0m==PROF== Connected to process 27148 (/home/kyp/Workspace/00.test/Test/cuda_study/06.gemm_cutlass/build/programm[4;1H)[34m^M[0m
==PROF== Profiling "InitializeMatrix_kernel" - 0: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "InitializeMatrix_kernel" - 1: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "InitializeMatrix_kernel" - 2: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "InitializeMatrix_kernel" - 3: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 4: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "ReferenceGemm_kernel" - 5: 0%....50%....100% - 34 passes[34m^M[0m
CUTLASS GEMM duration: 244.199ms [34m^M[0m
Reference GEMM duration: 941.569ms [34m^M[0m
Passed.[34m^M[0m
==PROF== Disconnected from process 27148[34m^M[0m
[27148] program@127.0.0.1[34m^M[0m
  InitializeMatrix_kernel(float *, int, int, int), 2025-May-15 08:56:06, Context 1, Stream 7[34m^M[0m
    Section: GPU Speed Of Light Throughput[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[19;1H[34m^M[0m
    DRAM Frequency[59Ccycle/nsecond[27C6.500[21;1H[34m^M[0m
    SM Frequency[61Ccycle/nsecond[27C1.166[23;1H[34m^M[0m
    Elapsed Cycles[67Ccycle[25C15,2355[25;1H[34m^M[0m
    Memory [%][75C%[26C69.822[27;1H[34m^M[0m
    DRAM Throughput[70C%[26C69.822[29;1H[34m^M[0m
    Duration[71Cusecond[26C13.122[31;1H[34m^M[0m
    L1/TEX Cache Throughput[62C%[26C61.599[33;1H[34m^M[0m
    L2 Cache Throughput[66C%[26C49.877[35;1H[34m^M[0m
    SM Active Cycles[65Ccycle[22C11,959.188[37;1H[34m^M[0m
    Compute (SM) [%][69C%[26C46.177[39;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[41;1H[34m^M[0m
    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    [34m^^[43;1HM[0m[44;11HDRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the[7C[34m^^[45;1HM[0m[46;11Hbytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  [34m^^[47;1HM[0m[48;11Hwhether there are values you can (re)compute.[65C[34m^^[49;1HM
^M[0m
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of [34m^^[52;1HM[0m[53;11Hthis device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide[7C[34m^^[54;1HM[0m[55;104H9,21[10CTop[10;21H[34h[?25h[?25l[55;94Hn[10;21H[55;1H/ReferenceGemm_kernel[55;94H[K[55;1H[26;3H[23m[24m[0m[H[J[1;5H---------------------------------------------------------------------- --------------- -------------------------------[2;1H[34m^M[0m
    WRN   This kernel's theoretical occupancy (16.7%) is limited by the number of required registers This kernel's      [34m^^[4;1HM[0m[5;11Htheoretical occupancy (16.7%) is limited by the required amount of shared memory See the CUDA Best Practices  [34m^^[6;1HM[0m[7;11HGuide (https://docs.nvidia.com/cuda/M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[14;1H[34m^M[0m
    Branch Instructions Ratio[60C%[27C0.000[16;1H[34m^M[0m
    Branch Instructions[63Cinst[24C135,1688[18;1H[34m^M[0m
    Branch Efficiency[68C%[28C1000[20;1H[34m^M[0m
    Avg. Divergent Branches[93C00[22;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[24;1H[34m^M
^M[0m
  ReferenceGemm_kernel(int, int, int, float, const float *, int, const float *, int, float, float *, int), 2025-May-15 088[27;1H:56:07, Context 1, Stream 7[34m^M[0m
    Section: GPU Speed Of Light Throughput[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[30;1H[34m^M[0m
    DRAM Frequency[59Ccycle/nsecond[27C7.299[32;1H[34m^M[0m
    SM Frequency[61Ccycle/nsecond[27C1.322[34;1H[34m^M[0m
    Elapsed Cycles[67Ccycle[22C4,914,4066[36;1H[34m^M[0m
    Memory [%][75C%[26C97.733[38;1H[34m^M[0m
    DRAM Throughput[70C%[26C21.399[40;1H[34m^M[0m
    Duration[71Cmsecond[27C3.733[42;1H[34m^M[0m
    L1/TEX Cache Throughput[62C%[26C98.233[44;1H[34m^M[0m
    L2 Cache Throughput[66C%[26C19.733[46;1H[34m^M[0m
    SM Active Cycles[65Ccycle[19C4,889,633.711[48;1H[34m^M[0m
    Compute (SM) [%][69C%[26C97.733[50;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[52;1H[34m^M[0m
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   [34m^^[54;1HM[0m[55;104H646,3[9C83%[26;3H[34h[?25h[?25l[55;94Hk[26;3H[55;94H [25;1H[55;106H5,1[25;1H[34h[?25h[?25l[55;94Hk[25;1H[55;94H [23;3H[55;106H4,3[23;3H[34h[?25h[?25l[55;94Hk[23;3H[55;94H [21;3H[55;106H3[21;3H[34h[?25h[?25l[55;94Hk[21;3H[55;94H [19;3H[55;106H2[19;3H[34h[?25h[?25l[55;94Hk[19;3H[55;94H [17;3H[55;106H1[17;3H[34h[?25h[?25l[55;94Hk[17;3H[55;94H [15;3H[55;106H0[15;3H[34h[?25h[?25l[55;94Hk[15;3H[55;94H [13;3H[55;105H39[13;3H[34h[?25h[?25l[55;94Hk[13;3H[55;94H [12;3H[55;106H8[12;3H[34h[?25h[?25l[55;94Hk[12;3H[55;94H [11;1H[55;106H7,1[11;1H[34h[?25h[?25l[55;94Hk[11;1H[55;94H [9;3H[55;106H6,3[9;3H[34h[?25h[?25l[55;94Hk[9;3H[55;94H [7;3H[55;106H5[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HAchieved Active Warps Per SM[54Cwarp[27C7.999[2;1H[34m^M[0m[55;104H[K[55;104H634,3[9C83%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HAchieved Occupancy[67C%[26C16.644[2;1H[34m^M[0m[55;104H[K[55;104H633,3[9C83%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HTheoretical Occupancy[64C%[26C16.677[2;1H[34m^M[0m[55;104H[K[55;104H632,3[9C83%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HTheoretical Active Warps per SM[51Cwarp[30C88[2;1H[34m^M[0m[55;104H[K[55;104H631,3[9C83%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HBlock Limit Warps[64Cblock[30C66[2;1H[34m^M[0m[55;104H[K[55;104H630,3[9C82%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HBlock Limit Shared Mem[59Cblock[30C11[2;1H[34m^M[0m[55;104H[K[55;104H629,3[9C82%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HBlock Limit Registers[60Cblock[30C11[2;1H[34m^M[0m[55;104H[K[55;104H628,3[9C82%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HBlock Limit SM[67Cblock[29C166[2;1H[34m^M[0m[55;104H[K[55;104H627,3[9C82%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5H---------------------------------------------------------------------- --------------- -------------------------------[2;1H[34m^M[0m[55;104H[K[55;104H626,3[9C82%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [6;3H[1;54r[1;1H[L[1;55r[1;5HSection: Occupancy[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H625,3[9C82%[6;3H[34h[?25h[?25l[55;94Hk[6;3H[55;94H [7;3H[1;54r[1;1H[3L[1;55r[1;5H---------------------------------------------------------------------- --------------- -------------------------------[2;1H[34m^M
^M[0m[55;104H[K[55;104H624,3[9C82%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HWaves Per SM[101C2.299[2;1H[34m^M[0m[55;104H[K[55;104H623,3[9C81%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [6;3H[55;106H2[6;3H[34h[?25h[?25l[55;94Hk[6;3H[55;94H [7;1H[1;54r[1;1H[2L[1;55r[1;5HThreads[73Cthread[25C16,3844[2;1H[34m^M[0m[55;104H[K[55;104H621,1[9C81%[7;1H[34h[?25h[?25l[55;94Hk[7;1H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HStatic Shared Memory Per Block[46Cbyte/block[30C00[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H620,3[9C81%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HDynamic Shared Memory Per Block[44CKbyte/block[26C16.644[2;1H[34m^M[0m[55;104H[K[55;104H619,3[9C81%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HDriver Shared Memory Per Block[45CKbyte/block[27C1.022[2;1H[34m^M[0m[55;104H[K[55;104H618,3[9C81%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HShared Memory Configuration Size[49CKbyte[26C32.777[2;1H[34m^M[0m[55;104H[K[55;104H617,3[9C81%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HRegisters Per Thread[51Cregister/thread[28C1800[2;1H[34m^M[0m[55;104H[K[55;104H616,3[9C81%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HGrid Size[106C644[2;1H[34m^M[0m[55;104H[K[55;104H615,3[9C80%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HFunction Cache Configuration[66CcudaFuncCachePreferNonee[2;1H[34m^M[0m[55;104H[K[55;104H614,3[9C80%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HBlock Size[104C2566[2;1H[34m^M[0m[55;104H[K[55;104H613,3[9C80%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5H---------------------------------------------------------------------- --------------- -------------------------------[2;1H[34m^M[0m[55;104H[K[55;104H612,3[9C80%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [6;3H[1;54r[1;1H[L[1;55r[1;5HSection: Launch Statistics[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H611,3[9C80%[6;3H[34h[?25h[?25l[55;94Hk[6;3H[55;94H [7;3H[1;54r[1;1H[3L[1;55r[1;5H---------------------------------------------------------------------- --------------- -------------------------------[2;1H[34m^M
^M[0m[55;104H[K[55;104H610,3[9C80%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HIssued Instructions[63Cinst[21C42,297,6211[2;1H[34m^M[0m[55;104H[K[55;104H609,3[9C80%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [6;3H[55;106H8[6;3H[34h[?25h[?25l[55;94Hk[6;3H[55;94H [7;1H[1;54r[1;1H[2L[1;55r[1;5HAvg. Issued Instructions Per Scheduler[44Cinst[21C377,657.333[2;1H[34m^M[0m[55;104H[K[55;104H607,1[9C79%[7;1H[34h[?25h[?25l[55;94Hk[7;1H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HExecuted Instructions[61Cinst[21C42,291,2000[2;1H[34m^M[0m[55;104H[K[55;104H606,3[9C79%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HAvg. Executed Instructions Per Scheduler[42Cinst[24C377,6000[2;1H[34m^M[0m[55;104H[K[55;104H605,3[9C79%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5H---------------------------------------------------------------------- --------------- -------------------------------[2;1H[34m^M[0m[55;104H[K[55;104H604,3[9C79%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [6;3H[1;54r[1;1H[L[1;55r[1;5HSection: Instruction Statistics[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H603,3[9C79%[6;3H[34h[?25h[?25l[55;94Hk[6;3H[55;94H [7;3H[1;54r[1;1H[3L[1;55r[1;5H---------------------------------------------------------------------- --------------- -------------------------------[2;1H[34m^M
^M[0m[55;104H[K[55;104H602,3[9C79%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HAvg. Not Predicated Off Threads Per Warp[72C31.755[2;1H[34m^M[0m[55;104H[K[55;104H601,3[9C79%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [6;3H[55;106H0[6;3H[34h[?25h[?25l[55;94Hk[6;3H[55;94H [7;1H[1;54r[1;1H[2L[1;55r[1;5HAvg. Active Threads Per Warp[87C322[2;1H[34m^M[0m[55;104H[K[55;104H599,1[9C78%[7;1H[34h[?25h[?25l[55;94Hk[7;1H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HWarp Cycles Per Executed Instruction[45Ccycle[27C2.622[2;1H[34m^M[0m[55;104H[K[55;104H598,3[9C78%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HWarp Cycles Per Issued Instruction[47Ccycle[27C2.622[2;1H[34m^M[0m[55;104H[K[55;104H597,3[9C78%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5H---------------------------------------------------------------------- --------------- -------------------------------[2;1H[34m^M[0m[55;104H[K[55;104H596,3[9C78%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [6;3H[1;54r[1;1H[L[1;55r[1;5HSection: Warp State Statistics[34m^M[0m[55;104H[K[55;104H595,3[9C78%[6;3H[34h[?25h[?25l[55;94Hk[6;3H[55;94H [7;3H[1;54r[1;1H[3L[1;55r[1;5H---------------------------------------------------------------------- --------------- -------------------------------[2;1H[34m^M
^M[0m[55;104H[K[55;104H594,3[9C78%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HEligible Warps Per Scheduler[54Cwarp[27C1.277[2;1H[34m^M[0m[55;104H[K[55;104H593,3[9C77%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [6;3H[55;106H2[6;3H[34h[?25h[?25l[55;94Hk[6;3H[55;94H [7;1H[1;54r[1;1H[2L[1;55r[1;5HActive Warps Per Scheduler[56Cwarp[27C2.000[2;1H[34m^M[0m[55;104H[K[55;104H591,1[9C77%[7;1H[34h[?25h[?25l[55;94Hk[7;1H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HNo Eligible[74C%[26C23.566[2;1H[34m^M[0m[55;104H[K[55;104H590,3[9C77%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HIssued Warp Per Scheduler[88C0.766[2;1H[34m^M[0m[55;104H[K[55;104H589,3[9C77%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HOne or More Eligible[65C%[26C76.444[2;1H[34m^M[0m[55;104H[K[55;104H588,3[9C77%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5H---------------------------------------------------------------------- --------------- -------------------------------[2;1H[34m^M[0m[55;104H[K[55;104H587,3[9C77%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [6;3H[1;54r[1;1H[L[1;55r[1;5HSection: Scheduler Statistics[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H586,3[9C77%[6;3H[34h[?25h[?25l[55;94Hk[6;3H[55;94H [7;3H[1;54r[1;1H[3L[1;55r[1;11Hrequest.[102C[34m^^[2;1HM
^M[0m[55;104H[K[55;104H585,3[9C77%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;11Hsection for uncoalesced loads and try to minimize how many cache 0m[55;104H[K[55;104H585,3[9C77%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;11Hsection for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory[9C[34m^^[2;1HM[0m[55;104H[K[55;104H584,3[9C76%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [6;3H[55;106H3[6;3H[34h[?25h[?25l[55;94Hk[6;3H[55;94H [7;1H[1;54r[1;1H[2L[1;55r[1;11Haccesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters    [34m^^[2;1HM[0m[55;104H[K[55;104H582,1[9C76%[7;1H[34h[?25h[?25l[55;94Hk[7;1H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;11HL2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  [34m^^[2;1HM[0m[55;104H[K[55;104H581,3[9C76%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HWRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   [34m^^[2;1HM[0m[55;104H[K[55;104H580,3[9C76%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [6;3H[1;54r[1;1H[L[1;55r[1;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H579,3[9C76%[6;3H[34h[?25h[?25l[55;94Hk[6;3H[55;94H [6;3H[1;54r[1;1H[2L[1;55r[1;5H---------------------------------------------------------------------- --------------- -------------------------------[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H578,3[9C76%[6;3H[34h[?25h[?25l[55;94Hk[6;3H[55;94H [6;3H[1;54r[1;1H[2L[1;55r[1;5HMem Pipes Busy[71C%[26C35.711[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H577,3[9C75%[6;3H[34h[?25h[?25l[55;94Hk[6;3H[55;94H [7;1H[1;54r[1;1H[2L[1;55r[1;5HL2 Hit Rate[74C%[26C89.077[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H576,1[9C75%[7;1H[34h[?25h[?25l[55;94Hk[7;1H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HL2 Compression Ratio[96C00[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H575,3[9C75%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HL2 Compression Success Rate[58C%[30C00[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H574,3[9C75%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HL1/TEX Hit Rate[70C%[30C00[2;1H[34m^M[0m[55;104H[K[55;104H573,3[9C75%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HMax Bandwidth[72C%[26C35.711[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H572,3[9C75%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HMem Busy[77C%[26C33.866[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H571,3[9C75%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HMemory Throughput[57CGbyte/second[26C42.655[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H570,3[9C74%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5H---------------------------------------------------------------------- --------------- -------------------------------[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H569,3[9C74%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [6;3H[1;54r[1;1H[L[1;55r[1;5HSection: Memory Workload Analysis[34m^M[0m[55;104H[K[55;104H568,3[9C74%[6;3H[34h[?25h[?25l[55;94Hk[6;3H[55;94H [7;3H[1;54r[1;1H[3L[1;55r[1;11Hcause warps to stall.[89C[34m^^[2;1HM
^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H567,3[9C74%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;11Hthe mix of executed instructions in this kernel. Check the Warp State Statistics section for which reasons    [34m^^[2;1HM[0m[55;104H[K[55;104H566,3[9C74%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [6;3H[55;106H5[6;3H[34h[?25h[?25l[55;94Hk[6;3H[55;94H [7;1H[1;54r[1;1H[2L[1;55r[1;11Hpipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  [34m^^[2;1HM[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H564,1[9C74%[7;1H[34h[?25h[?25l[55;94Hk[7;1H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;11H(https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the[10C[34m^^[2;1HM[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H563,3[9C74%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;11Hutilization appears to be caused by frequent, low-latency instructions. See the Kernel Profiling Guide[8C[34m^^[2;1HM[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H562,3[9C73%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;11Hpoint (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD) operations. Comparing the two, the overall pipeline    [34m^^[2;1HM[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H561,3[9C73%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;11Hnumber of executed instructions, the highest utilized pipeline (62.2%) is FMA. It executes 32-bit floating    [34m^^[2;1HM[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H560,3[9C73%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;11Hoperations. The pipeline is well-utilized, but might become a bottleneck if more work is added. Based on the  [34m^^[2;1HM[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H559,3[9C73%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;11Hdifferent instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    [34m^^[2;1HM[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H558,3[9C73%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HWRN   FMA is the highest-utilized pipeline (62.3%) based on active cycles, taking into account the rates of its     [34m^^[2;1HM[0m[55;104H[K[55;104H557,3[9C73%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5H---------------------------------------------------------------------- --------------- -------------------------------[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H556,3[9C73%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HSM Busy[78C%[26C76.400[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H555,3[9C72%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HIssued Ipc Active[59Cinst/cycle[27C3.066[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H554,3[9C72%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HIssue Slots Busy[69C%[26C76.400[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H553,3[9C72%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HExecuted Ipc Elapsed[56Cinst/cycle[27C2.322[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H552,3[9C72%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HExecuted Ipc Active[57Cinst/cycle[27C3.066[2;1H[34m^M[0m[55;104H[K[55;104H551,3[9C72%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5H---------------------------------------------------------------------- --------------- -------------------------------[2;1H[34m^M[0m[55;104H[K[55;104H550,3[9C72%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [6;3H[1;54r[1;1H[L[1;55r[1;5HSection: Compute Workload Analysis[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H549,3[9C72%[6;3H[34h[?25h[?25l[55;94Hk[6;3H[55;94H [7;3H[1;54r[1;1H[3L[1;55r[1;11Hanalysis.[101C[34m^^[2;1HM
^M[0m[55;104H[K[55;104H548,3[9C71%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;11H(https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      [34m^^[2;1HM[0m[55;104H[K[55;104H547,3[9C71%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [6;3H[55;106H6[6;3H[34h[?25h[?25l[55;94Hk[6;3H[55;94H [7;1H[1;54r[1;1H[2L[1;55r[1;11Hof this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    [34m^^[2;1HM[0m[55;104H[K[55;104H545,1[9C71%[7;1H[34h[?25h[?25l[55;94Hk[7;1H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HINF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 46%   [34m^^[2;1HM[0m[55;104H[K[55;104H544,3[9C71%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [6;3H[1;54r[1;1H[L[1;55r[1;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H543,3[9C71%[6;3H[34h[?25h[?25l[55;94Hk[6;3H[55;94H [6;3H[1;54r[1;1H[2L[1;55r[1;11Hlatency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.[17C[34m^^[2;1HM[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H542,3[9C71%[6;3H[34h[?25h[?25l[55;94Hk[6;3H[55;94H [6;3H[1;54r[1;1H[2L[1;55r[1;11Hof this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    [34m^^[2;1HM[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H541,3[9C71%[6;3H[34h[?25h[?25l[55;94Hk[6;3H[55;94H [7;1H[1;54r[1;1H[2L[1;55r[1;5HWRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance [34m^^[2;1HM[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H540,1[9C70%[7;1H[34h[?25h[?25l[55;94Hk[7;1H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5H---------------------------------------------------------------------- --------------- -------------------------------[2;1H[34m^M[0m[55;104H[K[55;104H539,3[9C70%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HCompute (SM) [%][69C%[26C58.000[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H538,3[9C70%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HSM Active Cycles[65Ccycle[21C494,344.577[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H537,3[9C70%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HL2 Cache Throughput[66C%[26C19.922[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H536,3[9C70%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HL1/TEX Cache Throughput[62C%[26C47.044[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H535,3[9C70%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HDuration[71Cusecond[25C493.122[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H534,3[9C70%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HDRAM Throughput[70C%[26C12.188[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H533,3[9C69%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HMemory [%][75C%[26C35.711[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H532,3[9C69%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HElapsed Cycles[67Ccycle[24C651,1288[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H531,3[9C69%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HSM Frequency[61Ccycle/nsecond[27C1.322[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H530,3[9C69%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5HDRAM Frequency[59Ccycle/nsecond[27C7.300[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H529,3[9C69%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [7;3H[1;54r[1;1H[2L[1;55r[1;5H---------------------------------------------------------------------- --------------- -------------------------------[2;1H[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H528,3[9C69%[7;3H[34h[?25h[?25l[55;94Hk[7;3H[55;94H [6;3H[1;54r[1;1H[L[1;55r[1;5HSection: GPU Speed Of Light Throughput[34m^M[0m[55;104H[K[55;104H527,3[9C69%[6;3H[34h[?25h[?25l[55;94Hk[6;3H[55;94H [44;3H[1;54r[1;1H[40L[1;55r[1;3Hvoid cutlass::Kernel<cutlass::gemm::kernel::Gemm<cutlass::gemm::threadblock::MmaPipelined<cutlass::gemm::GemmShape<(intt[2;1H)128, (int)128, (int)8>, cutlass::transform::threadblock::PredicatedTileIterator<cutlass::MatrixShape<(int)128, (int)8>,  [3;1Hfloat, cutlass::layout::RowMajor, (int)1, cutlass::transform::PitchLinearStripminedThreadMap<cutlass::PitchLinearShape<(ii[4;1Hnt)8, (int)128>, (int)256, (int)1>, (int)1, (bool)0, cutlass::layout::NoPermute>, cutlass::transform::threadblock::Regulaa[5;1HrTileIterator<cutlass::MatrixShape<(int)128, (int)8>, float, cutlass::layout::ColumnMajor, (int)1, cutlass::transform::Trr[6;1HansposePitchLinearThreadMapSimt<cutlass::transform::PitchLinearStripminedThreadMap<cutlass::PitchLinearShape<(int)8, (intt[7;1H)128>, (int)256, (int)1>>, (int)4>, cutlass::transform::threadblock::PredicatedTileIterator<cutlass::MatrixShape<(int)8,  [8;1H(int)128>, float, cutlass::layout::RowMajor, (int)0, cutlass::transform::PitchLinearStripminedThreadMap<cutlass::PitchLinn[9;1HearShape<(int)128, (int)8>, (int)256, (int)1>, (int)1, (bool)0, cutlass::layout::NoPermute>, cutlass::transform::threadbll[10;1Hock::RegularTileIterator<cutlass::MatrixShape<(int)8, (int)128>, float, cutlass::layout::RowMajor, (int)0, cutlass::transs[11;1Hform::PitchLinearStripminedThreadMap<cutlass::PitchLinearShape<(int)128, (int)8>, (int)256, (int)1>, (int)4>, float, cutll[12;1Hass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaSimt<cutlass::gemm::GemmShape<(int)33[13;1H2, (int)64, (int)8>, float, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, float, cutlass::layout::RowMaa[14;1Hjor, cutlass::gemm::warp::MmaSimtPolicy<cutlass::MatrixShape<(int)4, (int)8>, cutlass::layout::RowMajorInterleaved<(int)22[15;1H>, cutlass::gemm::GemmShape<(int)4, (int)4, (int)1>>, (int)1, (cutlass::ComplexTransform)0, (cutlass::ComplexTransform)0,,[16;1H bool>, cutlass::MatrixShape<(int)4, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, cutlass::NumericArrayConvertt[17;1Her<float, float, (int)4, (cutlass::FloatRoundStyle)2, cutlass::transform::thread::UnaryTransform::Identity>, cutlass::Numm[18;1HericArrayConverter<float, float, (int)4, (cutlass::FloatRoundStyle)2, cutlass::transform::thread::UnaryTransform::Identitt[19;1Hy>, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)8>, cutlass::gemm:::[20;1Hwarp::MmaSimt<cutlass::gemm::GemmShape<(int)32, (int)64, (int)8>, float, cutlass::layout::ColumnMajor, float, cutlass::laa[21;1Hyout::RowMajor, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaSimtPolicy<cutlass::MatrixShape<(int)4, (int)8>>[22;1H, cutlass::layout::RowMajorInterleaved<(int)2>, cutlass::gemm::GemmShape<(int)4, (int)4, (int)1>>, (int)1, (cutlass::Compp[23;1HlexTransform)0, (cutlass::ComplexTransform)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlaa[24;1Hss::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)1, (([25;1Hint)4, (int)4, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)4, (int)2, (int)1, (int)8>, (int)2566[26;1H, (int)1, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorSimt<cc[27;1Hutlass::gemm::GemmShape<(int)32, (int)64, (int)8>, cutlass::gemm::thread::Mma<cutlass::gemm::GemmShape<(int)8, (int)8, (ii[28;1Hnt)1>, float, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, float, cutlass::layout::RowMajor, cutlass:::[29;1utlass::gemm::GemmShape<(int)4, (int)4, (int)1>>>, cutlass::epiloguu[31;1He::warp::TileIteratorSimt<cutlass::gemm::GemmShape<(int)32, (int)64, (int)8>, cutlass::gemm::thread::Mma<cutlass::gemm::GG[32;1HemmShape<(int)8, (int)8, (int)1>, float, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, float, cutlass:::[33;1Hlayout::RowMajor, cutlass::arch::OpMultiplyAdd, bool>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaSimtPolii[34;1Hcy<cutlass::MatrixShape<(int)4, (int)8>, cutlass::layout::RowMajorInterleaved<(int)2>, cutlass::gemm::GemmShape<(int)4, (([35;1Hint)4, (int)1>>>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThrr[36;1HeadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)1, (int)4, (int)4, (int)1>, cutlass::epilogue::three[37;1Hadblock::OutputTileShape<(int)1, (int)4, (int)2, (int)1, (int)8>, (int)256, (int)1, (int)32>::CompactedThreadMap, float,  [38;1H(int)4>, cutlass::epilogue::thread::LinearCombination<float, (int)1, float, float, (cutlass::epilogue::thread::ScaleType::[39;1H:Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)17>, (int)1, (int)1>, cutlass::gemm::three[40;1Hadblock::GemmIdentityThreadblockSwizzle<(int)1>, (bool)0>>(T1::Params), 2025-May-15 08:56:06, Context 1, Stream 7[34m^M[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H526,3[9C67%[44;3H[34h[?25h[?25l[55;94Hk[44;3H[55;94H [42;3H[55;106H5[42;3H[34h[?25h[?25l[55;94Hj[42;3H[55;94H [44;3H[55;106H6[44;3H[34h[?25h[?25l[55;94Hj[44;3H[55;94H [46;3H[55;106H7[46;3H[34h[?25h[?25l[55;94Hj[46;3H[55;94H [48;3H[55;106H8[48;3H[34h[?25h[?25l[55;94Hj[48;3H[55;94H [10;3H[1;54r[1;1H[40M[1;55r[14;1H    Duration                                                                       usecond                         493.122[15;1H[34m^M[0m
    L1/TEX Cache Throughput[62C%[26C47.044[17;1H[34m^M[0m
    L2 Cache Throughput[66C%[26C19.922[19;1H[34m^M[0m
    SM Active Cycles[65Ccycle[21C494,344.577[21;1H[34m^M[0m
    Compute (SM) [%][69C%[26C58.000[23;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[25;1H[34m^M[0m
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance [34m^^[27;1HM[0m[28;11Hof this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    [34m^^[29;1HM[0m[30;11Hlatency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.[17C[34m^^[31;1HM
^M[0m
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 46%   [34m^^[34;1HM[0m[35;11Hof this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    [34m^^[36;1HM[0m[37;11H(https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      [34m^^[38;1HM[0m[39;11Hanalysis.[101C[34m^^[40;1HM
^M[0m
    Section: Compute Workload Analysis[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[44;1H[34m^M[0m
    Executed Ipc Active[57Cinst/cycle[27C3.066[46;1H[34m^M[0m
    Executed Ipc Elapsed[56Cinst/cycle[27C2.322[48;1H[34m^M[0m
    Issue Slots Busy[69C%[26C76.400[50;1H[34m^M[0m
    Issued Ipc Active[59Cinst/cycle[27C3.066[52;1H[34m^M[0m
    SM Busy[78C%[26C76.400[54;1H[34m^M[0m[55;104H[K[55;104H529,3[9C69%[10;3H[34h[?25h[?25l[55;94Hj[10;3H[55;94H [12;3H[55;105H30[12;3H[34h[?25h[?25l[55;94Hj[12;3H[55;94H [14;3H[55;106H1[14;3H[34h[?25h[?25l[55;94Hj[14;3H[55;94H [16;3H[55;106H2[16;3H[34h[?25h[?25l[55;94Hj[16;3H[55;94H [18;3H[55;106H3[18;3H[34h[;94Hj[12;3H[55;94H [14;3H[55;106H1[14;3H[34h[?25h[?25l[55;94Hj[14;3H[55;94H [16;3H[55;106H2[16;3H[34h[?25h[?25l[55;94Hj[16;3H[55;94H [18;3H[55;106H3[18;3H[34h[?25h[?25l[55;94Hj[18;3H[55;94H [20;3H[55;106H4[20;3H[34h[?25h[?25l[55;94Hj[20;3H[55;94H [22;3H[55;106H5[22;3H[34h[?25h[?25l[55;94Hj[22;3H[55;94H [24;3H[55;106H6[24;3H[34h[?25h[?25l[55;94Hj[24;3H[55;94H [26;3H[55;106H7[26;3H[34h[?25h[?25l[55;94Hj[26;3H[55;94H [28;3H[55;106H8[28;3H[34h[?25h[?25l[55;94Hj[28;3H[55;94H [30;3H[55;106H9[30;3H[34h[?25h[?25l[55;94Hj[30;3H[55;94H [32;1H[55;105H40,1[32;1H[34h[?25h[?25l[55;94Hj[32;1H[55;94H [33;3H[55;106H1,3[33;3H[34h[?25h[?25l[55;94Hj[33;3H[55;94H [35;3H[55;106H2[35;3H[34h[?25h[?25l[55;94Hj[35;3H[55;94H [37;3H[55;106H3[37;3H[34h[?25h[?25l[55;94Hj[37;3H[55;94H [39;3H[55;106H4[39;3H[34h[?25h[?25l[55;94Hj[39;3H[55;94H [41;1H[55;106H5,1[41;1H[34h[?25h[?25l[55;94Hj[41;1H[55;94H [42;3H[55;106H6,3[42;3H[34h[?25h[?25l[55;94Hj[42;3H[55;94H [43;3H[55;106H7[43;3H[34h[?25h[?25l[55;94Hj[43;3H[55;94H [45;3H[55;106H8[45;3H[34h[?25h[?25l[55;94Hj[45;3H[55;94H [47;3H[55;106H9[47;3H[34h[?25h[?25l[55;94Hj[47;3H[55;94H [46;3H[1;54r[1;1H[3M[1;55r[52;5H---------------------------------------------------------------------- --------------- -------------------------------[53;1H[34m^M[0m
    WRN   FMA is the highest-utilized pipeline (62.3%) based on active cycles, taking into account the rates of its     [34m^[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H550,3[9C69%[46;3H[34h[?25h[?25l[55;94Hj[46;3H[55;94H [48;3H[55;106H1[48;3H[34h[?25h[?25l[55;94Hj[48;3H[55;94H [46;3H[1;54r[1;1H[4M[1;55r[50;1H    WRN   FMA is the highest-utilized pipeline (62.3%) based on active cycles, taking into account the rates of its     [34m^^[51;1HM[0m[52;11Hdifferent instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    [34m^^[53;1HM[0m[54;11Hoperations. The pipeline is well-utilized, but might become a bottleneck if more work is added. Based on the  [34m^[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H552,3[9C69%[46;3H[34h[?25h[?25l[55;94Hj[46;3H[55;94H [48;3H[55;106H3[48;3H[34h[?25h[?25l[55;94Hj[48;3H[55;94H [46;3H[1;54r[1;1H[4M[1;55r[50;1H          operations. The pipeline is well-utilized, but might become a bottleneck if more work is added. Based on the  [34m^^[51;1HM[0m[52;11Hnumber of executed instructions, the highest utilized pipeline (62.2%) is FMA. It executes 32-bit floating    [34m^^[53;1HM[0m[54;11Hpoint (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD) operations. Comparing the two, the overall pipeline    [34m^[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H554,3[9C69%[46;3H[34h[?25h[?25l[55;94Hj[46;3H[55;94H [48;3H[55;106H5[48;3H[34h[?25h[?25l[55;94Hj[48;3H[55;94H [46;3H[1;54r[1;1H[4M[1;55r[50;1H          point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD) operations. Comparing the two, the overall pipeline    [34m^^[51;1HM[0m[52;11Hutilization appears to be caused by frequent, low-latency instructions. See the Kernel Profiling Guide[8C[34m^^[53;1HM[0m[54;11H(https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the[10C[34m^[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H556,3[9C70%[46;3H[34h[?25h[?25l[55;94Hj[46;3H[55;94H [48;3H[55;106H7[48;3H[34h[?25h[?25l[55;94Hj[48;3H[55;94H [46;3H[1;54r[1;1H[4M[1;55r[workloads handled by each pipeline. The Instruction Statistics section shows  [34m^^[53;1HM[0m[54;11Hthe mix of executed instructions in this kernel. Check the Warp State Statistics section for which reasons    [34m^[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H558,3[9C70%[46;3H[34h[?25h[?25l[55;94Hj[46;3H[55;94H [48;3H[55;106H9[48;3H[34h[?25h[?25l[55;94Hj[48;3H[55;94H [46;3H[1;54r[1;1H[4M[1;55r[50;1H          the mix of executed instructions in this kernel. Check the Warp State Statistics section for which reasons    [34m^^[51;1HM[0m[52;11Hcause warps to stall.[89C[34m^^[53;1HM
^M[0m[55;104H[K[55;104H560,3[9C70%[46;3H[34h[?25h[?25l[55;94Hj[46;3H[55;94H [48;3H[55;106H1[48;3H[34h[?25h[?25l[55;94Hk[48;3H[55;94H [46;3H[55;106H0[46;3H[34h[?25h[?25l[55;94H:[46;3H[55;94H[K[55;1H:[34h[?25hq![?25l[55;1H[K[55;1H[?1l>[34h[?25h[?1049l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/06.gemm_cutlass# l
[?2004lbasic_gemm.cu  [0m[01;32mbuild.sh[0m*       DS_definitions.h  DS_timer.h      report.txt
[01;34mbuild[0m/         CMakeLists.txt  DS_timer.cpp      ncu_report.txt  [01;32mrun_ncu.sh[0m*
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/06.gemm_cutlass# cd ..
[?2004l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study# l
[?2004l[0m[01;34m01.hello_cuda[0m/   [01;34m05.1.gemm_naive[0m/            [01;34m05.5.gemm_shared_mem_coalesced[0m/  [01;34m08.convolution[0m/
[01;34m02.gpu_adder[0m/    [01;34m05.2.gemm_occupancy_ctrl[0m/   [01;34m05.6.gemm_wmma[0m/                  [01;34mcompute_ai[0m/
[01;34m03.gpu_thread[0m/   [01;34m05.3.gemm_shared_mem[0m/       [01;34m06.gemm_cutlass[0m/                 [01;34mcompute_ai_batch[0m/
[01;34m04.device_info[0m/  [01;34m05.4.gemm_shared_mem_bank[0m/  [01;34m07.conv2d_cutlss[0m/                [01;31mcompute_ai.tar.gz[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study# cd 0u7[K[K8.convolution/
[?2004l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# l
[?2004l[0m[01;34m00.cufft_1d_c2c[0m/  [01;34m02.cufft_1d_r2c_runtime[0m/    [01;34m03.cufft_1d_conv[0m/  [01;34m05.cufftDX_1d_c2c[0m/
[01;34m01.cufft_1d_r2c[0m/  [01;34m02.cufft_1d_r2c_runtime_2[0m/  [01;34m04.cufft_2d_conv[0m/  [01;34m08.cufftDX_2d_conv[0m/
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# cd ..
[?2004l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study# l
[?2004l[0m[01;34m01.hello_cuda[0m/   [01;34m05.1.gemm_naive[0m/            [01;34m05.5.gemm_shared_mem_coalesced[0m/  [01;34m08.convolution[0m/
[01;34m02.gpu_adder[0m/    [01;34m05.2.gemm_occupancy_ctrl[0m/   [01;34m05.6.gemm_wmma[0m/                  [01;34mcompute_ai[0m/
[01;34m03.gpu_thread[0m/   [01;34m05.3.gemm_shared_mem[0m/       [01;34m06.gemm_cutlass[0m/                 [01;34mcompute_ai_batch[0m/
[01;34m04.device_info[0m/  [01;34m05.4.gemm_shared_mem_bank[0m/  [01;34m07.conv2d_cutlss[0m/                [01;31mcompute_ai.tar.gz[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study# cd 07.conv2d_cutlss/
[?2004l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# l
[?2004lampere_tensorop_conv2dfprop.cu  [0m[01;34mbuild[0m/  [01;32mbuild.sh[0m*  CMakeLists.txt  DS_definitions.h  DS_timer.cpp  DS_timer.h  [01;32mrun.sh[0m*
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# l
[?2004lampere_tensorop_conv2dfprop.cu  [0m[01;34mbuild[0m/  [01;32mbuild.sh[0m*  CMakeLists.txt  DS_definitions.h  DS_timer.cpp  DS_timer.h  [01;32mrun.sh[0m*
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# vim ~/.my[K[Kbashrc
[?2004l[?1049h[?1h=[1;55r[23m[24m[0m[H[J[?25l[55;1n.sh[0m*
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# vim ~/.my[K[Kbashrc
[?2004l[?1049h[?1h=[1;55r[23m[24m[0m[H[J[?25l[55;1H"~/.bashrc" 99L, 3106B[2;1H▽[6n[2;1H  [3;1HPzz\[0%m[6n[3;1H           [1;1H[1;1H[34m# ~/.bashrc: executed by bash(1) for non-login shells.
# see /usr/share/doc/bash/examples/startup-files (in the package bash-doc)[0m[2;75H[K[3;1H[34m# for examples[0m[3;15H[K[5;1H[34m# If not running interactively, don't do anything[0m
[33m[[0m [33m-z[0m [33m"[0m[35m$PS1[0m[33m"[0m [33m][0m && [33mreturn[0m

[34m# don't put duplicate lines in the history. See bash(1) for more options
# ... or force ignoredups and ignorespace[0m
[36mHISTCONTROL[0m=ignoredups:ignorespace

[34m# append to the history file, don't overwrite it[0m
shopt [35m-s[0m histappend

[34m# for setting history length see HISTSIZE and HISTFILESIZE in bash(1)[0m
[36mHISTSIZE[0m=[31m1000[0m
[36mHISTFILESIZE[0m=[31m2000[0m

[34m# check the window size after each command and, if necessary,
# update the values of LINES and COLUMNS.[0m
shopt [35m-s[0m checkwinsize

[34m# make less more friendly for non-text input files, see lesspipe(1)[0m
[33m[[0m [33m-x[0m /usr/bin/lesspipe [33m][0m && [33meval[0m [33m"[0m[35m$(SHELL[0m[33m=[0m[35m/bin/sh lesspipe[0m[35m)[0m[33m"[0m

[34m# set variable identifying the chroot you work in (used in the prompt below)[0m
[33mif [[0m [33m-z[0m [33m"[0m[35m$debian_chroot[0m[33m"[0m [33m][0m [33m&&[0m [33m[[0m [33m-r[0m /etc/debian_chroot [33m];[0m [33mthen[0m
    [36mdebian_chroot[0m=[35m$([0m[33mcat[0m[35m /etc/debian_chroot[0m[35m)[0m
[33mfi[0m

[34m# set a fancy prompt (non-color, unless we know we "want" color)[0m
[33mcase[0m [33m"[0m[35m$TERM[0m[33m"[0m [33min[0m
    xterm-color[33m)[0m [36mcolor_prompt[0m=yes[33m;;
esac[0m

[34m# uncomment for a colored prompt, if the terminal has the capability; turned
# off by default to not distract the user: the focus in a terminal window
# should be on the output of commands, not on the prompt
#force_color_prompt=yes[0m

[33mif [[0m [33m-n[0m [33m"[0m[35m$force_color_prompt[0m[33m"[0m [33m];[0m [33mthen
[0m    [33mif [[0m [33m-x[0m /usr/bin/tput [33m][0m [33m&&[0m [33mtput[0m setaf [31m1[0m [33m>&[0m/dev/null[33m;[0m [33mthen[0m[43;9H[34m# We have color support; assume it's compliant with Ecma-48[44;9H# (ISO/IEC-6429). (Lack of such support is extremely rare, and such[45;9H# a case would tend to support setf rather than setaf.)[0m[46;9H[36mcolor_prompt[0m=yes
    [33melse[0m[48;9H[36mcolor_prompt[0m=
    [33mfi
fi

if [[0m [33m"[0m[35m$color_prompt[0m[33m"[0m [33m=[0m [31myes[0m [33m];[0m [33mthen[0m
    [35mPS1[0m[33m='[0m[31m${debian_chroot:+($debian_chroot)}\[\033[01;32m\]\u@\h\[\033[00m\]:\[\033[01;34m\]\w\[\033[00m\]\$ [0m[33m'
else[0m[55;104H1,1[11CTop[1;1H[34h[?25h[?25l[55;94Hj[1;1H[55;94H [2;1H[55;104H2[2;1H[34h[?25h[?25l[55;94Hj[2;1H[55;94H [3;1H[55;104H3[3;1H[34h[?25h[?25l[55;94Hj[3;1H[55;94H [4;1H[55;104H4,0-1[4;1H[34h[?25h[?25l[55;94HG[4;1H[55;94H [54;1H[1;54r[1;1H[45M[1;55r[10;5H[35mPS1[0m[33m='[0m[31m${debian_chroot:+($debian_chroot)}\u@\h:\w\$ [0m[33m'
fi
unset[0m[36m color_prompt force_color_prompt[0m

[34m# If this is an xterm set the title to user@host:dir[0m
[33mcase[0m [33m"[0m[35m$TERM[0m[33m"[0m [33min[0m
xterm*|rxvt*[33m)[0m
    [35mPS1[0m[33m="[0m[31m\[[0m[35m\e[0m[31m]0;[0m[35m${debian_chroot[0m[33m:+[0m([35m$debian_chroot[0m)[35m}[0m[31m\u@\h: \w[0m[35m\a[0m[31m\][0m[35m$PS1[0m[33m"
[0m    [33m;;[0m
*[33m)
[0m    [33m;;
esac[0m

[34m# enable color support of ls and also add handy aliases[0m
[33mif [[0m [33m-x[0m /usr/bin/dircolors [33m];[0m [33mthen
[0m    [33mtest[0m [33m-r[0m ~/.dircolors [33m&&[0m [33meval[0m [33m"[[0m[33m='[0m[31mls --color=auto[0m[33m'[0m
    [34m#alias dir='dir --color=auto'
[0m    [34m#alias vdir='vdir --color=auto'[0m[30;5H[33malias [0m[36mgrep[0m[33m='[0m[31mgrep --color=auto[0m[33m'
[0m    [33malias [0m[36mfgrep[0m[33m='[0m[31mfgrep --color=auto[0m[33m'
[0m    [33malias [0m[36megrep[0m[33m='[0m[31megrep --color=auto[0m[33m'
fi[0m

[34m# some more ls aliases[0m
[33malias [0m[36mll[0m[33m='[0m[31mls -alF[0m[33m'
alias [0m[36mla[0m[33m='[0m[31mls -A[0m[33m'
alias[0m [36ml[0m=[33m'[0m[31mls -CF[0m[33m'[0m

[34m# Alias definitions.
# You may want to put all your additions into a separate file like
# ~/.bash_aliases, instead of adding them here directly.
# See /usr/share/doc/bash-doc/examples in the bash-doc package.[0m

[33mif [[0m [33m-f[0m ~/.bash_aliases [33m];[0m [33mthen
[0m   [33m . [0m~/.bash_aliases
[33mfi[0m

[34m# enable programmable completion features (you don't need to enable
# this, if it's already enabled in /etc/bash.bashrc and /etc/profile
# sources /etc/bash.bashrc).
#if [ -f /etc/bash_completion ] && ! shopt -oq posix; then
#    . /etc/bash_completion
#fi[0m[55;1H[K[55;104H99,1[10CBot[54;1H[34h[?25h[?25l[55;94Hk[54;1H[55;94H [53;1H[55;105H8[53;1H[34h[?25h[?25l[55;94Hk[53;1H[55;94H [52;1H[55;105H7[52;1H[34h[?25h[?25l[55;94Hk[52;1H[55;94H [51;1H[55;105H6[51;1H[34h[?25h[?25l[55;94Hk[51;1H[55;94H [50;1H[55;105H5[50;1H[34h[?25h[?25l[55;94Hk[50;1H[55;94H [49;1H[55;105H4[49;1H[34h[?25h[?25l[55;94H:[49;1H[55;94H[K[55;1H:[34h[?25hq![?25l[55;1H[K[55;1H[?1l>[34h[?25h[?1049l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# ll
[?2004ltotal 88
drwxrwxr-x  3 kyp kyp  4096  9월  9  2024 [0m[01;34m.[0m/
drwxrwxr-x 17 kyp kyp  4096  4월 19 15:11 [01;34m..[0m/
-rw-rw-r--  1 kyp kyp 28072  9월  9  2024 ampere_tensorop_conv2dfprop.cu
-rw-r--r--  1 kyp kyp 16384  9월  9  2024 .ampere_tensorop_conv2dfprop.cu.swp
drwxrwxr-x  3 kyp kyp  4096  9월  9  2024 [01;34mbuild[0m/
-rwxrwxr-x  1 kyp kyp   141  9월  9  2024 [01;32mbuild.sh[0m*
-rw-rw-r--  1 kyp kyp  2362  9월  9  2024 CMakeLists.txt
-rw-rw-r--  1 kyp kyp  5084  9월  9  2024 DS_definitions.h
-rw-rw-r--  1 kyp kyp  7940  9월  9  2024 DS_timer.cpp
-rw-rw-r--  1 kyp kyp  2015  9월  9  2024 DS_timer.h
-rwxrwxrwx  1 kyp kyp    64  9월  9  2024 [01;32mrun.sh[0m*
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# l
[?2004lampere_tensorop_conv2dfprop.cu  [0m[01;34mbuild[0m/  [01;32mbuild.sh[0m*  CMakeLists.txt  DS_definitions.h  DS_timer.cpp  DS_timer.h  [01;32mrun.sh[0m*
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# l
[?2004lampere_tensorop_conv2dfprop.cu  [0m[01;34mbuild[0m/  [01;32mbuild.sh[0m*  CMakeLists.txt  DS_definitions.h  DS_timer.cpp  DS_timer.h  [01;32mrun.sh[0m*
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# ll
[?2004ltotal 88
drwxrwxr-x  3 kyp kyp  4096  9월  9  2024 [0m[01;34m.[0m/
drwxrwxr-x 17 kyp kyp  4096  4월 19 15:11 [01;34m..[0m/
-rw-rw-r--  1 kyp kyp 28072  9월  9  2024 ampere_tensorop_conv2dfprop.cu
-rw-r--r--  1 kyp kyp 16384  9월  9  2024 .ampere_tensorop_conv2dfprop.cu.swp
drwxrwxr-x  3 kyp kyp  4096  9월  9  2024 [01;34mbuild[0m/
-rwxrwxr-x  1 kyp kyp   141  9월  9  2024 [01;32mbuild.sh[0m*
-rw-rw-r--  1 kyp kyp  2362  9월  9  2024 CMakeLists.txt
-rw-rw-r--  1 kyp kyp  5084  9월  9  2024 DS_definitions.h
-rw-rw-r--  1 kyp kyp  7940  9월  9  2024 DS_timer.cpp
-rw-rw-r--  1 kyp kyp  2015  9월  9  2024 DS_timer.h
-rwxrwxrwx  1 kyp kyp    64  9월  9  2024 [01;32mrun.sh[0m*
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# d[Ks[Ks[K./build.sh
[?2004lmkdir: cannot create directory ‘build’: File exists
-- Configuring done
-- Generating done
-- Build files have been written to: /home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss/build
[35m[1mConsolidate compiler generated dependencies of target program[0m
[ 33%] [32mBuilding CUDA object CMakeFiles/program.dir/ampere_tensorop_conv2dfprop.cu.o[0m
[ 66%] [32mBuilding CXX object CMakeFiles/program.dir/DS_timer.cpp.o[0m
[100%] [32m[1mLinking CXX executable program[0m
[100%] Built target program
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# ./build/program
[?2004lLayer,N,H,W,C,K,R,S,Runtime,GFLOPs
conv_1,1,32,32,32,32,3,3,0.0256512,735.808
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# l
[?2004lampere_tensorop_conv2dfprop.cu  [0m[01;34mbuild[0m/  [01;32mbuild.sh[0m*  CMakeLists.txt  DS_definitions.h  DS_timer.cpp  DS_timer.h  [01;32mrun.sh[0m*
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# ll
[?2004ltotal 88
drwxrwxr-x  3 kyp kyp  4096  9월  9  2024 [0m[01;34m.[0m/
drwxrwxr-x 17 kyp kyp  4096  4월 19 15:11 [01;34m..[0m/
-rw-rw-r--  1 kyp kyp 28072  9월  9  2024 ampere_tensorop_conv2dfprop.cu
-rw-r--r--  1 kyp kyp 16384  9월  9  2024 .ampere_tensorop_conv2dfprop.cu.swp
drwxrwxr-x  3 kyp kyp  4096  5월 15 08:58 [01;34mbuild[0m/
-rwxrwxr-x  1 kyp kyp   141  9월  9  2024 [01;32mbuild.sh[0m*
-rw-rw-r--  1 kyp kyp  2362  9월  9  2024 CMakeLists.txt
-rw-rw-r--  1 kyp kyp  5084  9월  9  2024 DS_definitions.h
-rw-rw-r--  1 kyp kyp  7940  9월  9  2024 DS_timer.cpp
-rw-rw-r--  1 kyp kyp  2015  9월  9  2024 DS_timer.h
-rwxrwxrwx  1 kyp kyp    64  9월  9  2024 [01;32mrun.sh[0m*
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# vim ~/,[K.[Kbas[K[K[K.bashrc
[?2004l[?1049h[?1h=[1;55r[23m[24m[0m[H[J[?25l[55;1H"~/.bashrc" 99L, 3106B[2;1H▽[6n[2;1H  [3;1HPzz\[0%m[6n[3;1H           [1;1H[1;9H[36mcolor_prompt[0m=yes
    [33melse[0m[2;9H[K[3;1H        [36mcolor_prompt[0m=[3;22H[K[4;5H[33mfi
fi

if [[0m [33m"[0m[35m$color_prompt[0m[33m"[0m [33m=[0m [31myes[0m [33m];[0m [33mthen[0m
    [35mPS1[0m[33m='[0m[31m${debian_chroot:+($debian_chroot)}\[\033[01;32m\]\u@\h\[\033[00m\]:\[\033[01;34m\]\w\[\033[00m\]\$ [0m[33m'
else[0m
    [35mPS1[0m[33m='[0m[31m${debian_chroot:+($debian_chroot)}\u@\h:\w\$ [0m[33m'
fi
unset[0m[36m color_prompt force_color_prompt[0m

[34m# If this is an xterm set the title to user@host:dir[0m
[33mcase[0m [33m"[0m[35m$TERM[0m[33m"[0m [33min[0m
xterm*|rxvt*[33m)[0m
    [35mPS1[0m[33m="[0m[31m\[[0m[35m\e[0m[31m]0;[0m[35m${debian_chroot[0m[33m:+[0m([35m$debian_chroot[0m)[35m}[0m[31m\u@\h: \w[0m[35m\a[0m[31m\][0m[35m$PS1[0m[33m"
[0m    [33m;;[0m
*[33m)
[0m    [33m;;
esac[0m

[34m# enable color support of ls and also add handy aliases[0m
[33mif [[0m [33m-x[0m /usr/bin/dircolors [33m];[0m [33mthen
[0m    [33mtest[0m [33m-r[0m ~/.dircolors [33m&&[0m [33meval[0m [33m"[0m[35m$([0m[35mdircolors -b ~/.dircolors[0m[35m)[0m[33m"[0m [33m||[0m [33meval[0m [33m"[0m[35m$([0m[35mdircolors -b[0m[35m)[0m[33m"
[0m    [33malias [0m[36mls[0m[33m='[0m[31mls --color=auto[0m[33m'[0m
    [34m#alias dir='dir --color=auto'
[0m    [34m#alias vdir='vdir --color=auto'[0m[30;5H[33malias [0m[36mgrep[0m[33m='[0m[31mgrep --color=auto[0m[33m'
[0m    [33malias [0m[36mfgrep[0m[33m='[0m[31mfgrep --color=auto[0m[33m'
[0m    [33malias [0m[36megrep[0m[33m='[0m[31megrep --color=auto[0m[33m'
fi[0m

[34m# some more ls aliases[0m
[33malias [0m[36mll[0m[33m='[0m[31mls -alF[0m[33m'
alias [0m[36mla[0m[33m='[0m[31mls -A[0m[33m'
alias[0m [36ml[0m=[33m'[0m[31mls -CF[0m[33m'[0m

[34m# Alias definitions.
# You may want to put all your additions into a separate file like
# ~/.bash_aliases, instead of adding them here directly.
# See /usr/share/doc/bash-doc/examples in the bash-doc package.[0m

[33mif [[0m [33additions into a separate file like
# ~/.bash_aliases, instead of adding them here directly.
# See /usr/share/doc/bash-doc/examples in the bash-doc package.[0m

[33mif [[0m [33m-f[0m ~/.bash_aliases [33m];[0m [33mthen
[0m   [33m . [0m~/.bash_aliases
[33mfi[0m

[34m# enable programmable completion features (you don't need to enable
# this, if it's already enabled in /etc/bash.bashrc and /etc/profile
# sources /etc/bash.bashrc).
#if [ -f /etc/bash_completion ] && ! shopt -oq posix; then
#    . /etc/bash_completion
#fi[0m[55;104H94,1[10CBot[49;1H[34h[?25h[?25l[55;94Hj[49;1H[55;94H [50;1H[55;105H5[50;1H[34h[?25h[?25l[55;94Hj[50;1H[55;94H [51;1H[55;105H6[51;1H[34h[?25h[?25l[55;94Hj[51;1H[55;94H [52;1H[55;105H7[52;1H[34h[?25h[?25l[55;94Hj[52;1H[55;94H [53;1H[55;105H8[53;1H[34h[?25h[?25l[55;94Hj[53;1H[55;94H [54;1H[55;105H9[54;1H[34h[?25h[?25l[55;94Hk[54;1H[55;94H [53;1H[55;105H8[53;1H[34h[?25h[?25l[55;94Hk[53;1H[55;94H [52;1H[55;105H7[52;1H[34h[?25h[?25l[55;94Hk[52;1H[55;94H [51;1H[55;105H6[51;1H[34h[?25h[?25l[55;94Hk[51;1H[55;94H [50;1H[55;105H5[50;1H[34h[?25h[?25l[55;94Hk[50;1H[55;94H [49;1H[55;105H4[49;1H[34h[?25h[?25l[55;94Hk[49;1H[55;94H [48;1H[55;105H3,0-1[48;1H[34h[?25h[?25l[55;94Hk[48;1H[55;94H [47;1H[55;105H2,1  [47;1H[34h[?25h[?25l[55;94Hk[47;1H[55;94H [46;1H[55;105H1[46;1H[34h[?25h[?25l[55;94Hk[46;1H[55;94H [45;1H[55;105H0[45;1H[34h[?25h[?25l[55;94Hk[45;1H[55;94H [44;1H[55;104H89,0-1[44;1H[34h[?25h[?25l[55;94Hk[44;1H[55;94H [43;1H[55;105H8,1  [43;1H[34h[?25h[?25l[55;94Hk[43;1H[55;94H [42;1H[55;105H7[42;1H[34h[?25h[?25l[55;94Hk[42;1H[55;94H [41;1H[55;105H6[41;1H[34h[?25h[?25l[55;94Hk[41;1H[55;94H [40;1H[55;105H5[40;1H[34h[?25h[?25l[55;94Hk[40;1H[55;94H [39;1H[55;105H4,0-1[39;1H[34h[?25h[?25l[55;94Hk[39;1H[55;94H [38;1H[55;105H3,1  [38;1H[34h[?25h[?25l[55;94H:[38;1H[55;1H[K[55;1H:[34h[?25hq![?25l[55;1H[K[55;1H[?1l>[34h[?25h[?1049l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# ls -lf[K[Ksf
[?2004l.ampere_tensorop_conv2dfprop.cu.swp  ampere_tensorop_conv2dfprop.cu  build     run.sh        DS_timer.h      .
DS_definitions.h                     ..                              build.sh  DS_timer.cpp  CMakeLists.txt
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# ls
[?2004lampere_tensorop_conv2dfprop.cu  [0m[01;34mbuild[0m  [01;32mbuild.sh[0m  CMakeLists.txt  DS_definitions.h  DS_timer.cpp  DS_timer.h  [01;32mrun.sh[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# ls -l
[?2004ltotal 64
-rw-rw-r-- 1 kyp kyp 28072  9월  9  2024 ampere_tensorop_conv2dfprop.cu
drwxrwxr-x 3 kyp kyp  4096  5월 15 08:58 [0m[01;34mbuild[0m
-rwxrwxr-x 1 kyp kyp   141  9월  9  2024 [01;32mbuild.sh[0m
-rw-rw-r-- 1 kyp kyp  2362  9월  9  2024 CMakeLists.txt
-rw-rw-r-- 1 kyp kyp  5084  9월  9  2024 DS_definitions.h
-rw-rw-r-- 1 kyp kyp  7940  9월  9  2024 DS_timer.cpp
-rw-rw-r-- 1 kyp kyp  2015  9월  9  2024 DS_timer.h
-rwxrwxrwx 1 kyp kyp    64  9월  9  2024 [01;32mrun.sh[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# vim [K[K[K[Kvim ~/.m[Kbashrc
[?2004l[?1049h[?1h=[1;55r[23m[24m[0m[H[J[?25l[55;1H"~/.bashrc" 99L, 3106B[2;1H▽[6n[2;1H  [3;1HPzz\[0%m[6n[3;1H           [1;1H[2;1H[34m# uncomment for a colored prompt, if the terminal has the capability; turned[0m[2;77H[K[3;1H[34m# off by default to not distract the user: the focus in a terminal window[0m[3;74H[K[4;1H[34m# should be on the output of commands, not on the prompt
#force_color_prompt=yes[0m

[33mif [[0m [33m-n[0m [33m"[0m[35m$force_color_prompt[0m[33m"[0m [33m];[0m [33mthen
[0m    [33mif [[0m [33m-x[0m /usr/bin/tput [33m][0m [33m&&[0m [33mtput[0m setaf [31m1[0m [33m>&[0m/dev/null[uld tend to support setf rather than setaf.)[0m[12;9H[36mcolor_prompt[0m=yes
    [33melse[0m[14;9H[36mcolor_prompt[0m=
    [33mfi
fi

if [[0m [33m"[0m[35m$color_prompt[0m[33m"[0m [33m=[0m [31myes[0m [33m];[0m [33mthen[0m
    [35mPS1[0m[33m='[0m[31m${debian_chroot:+($debian_chroot)}\[\033[01;32m\]\u@\h\[\033[00m\]:\[\033[01;34m\]\w\[\033[00m\]\$ [0m[33m'
else[0m
    [35mPS1[0m[33m='[0m[31m${debian_chroot:+($debian_chroot)}\u@\h:\w\$ [0m[33m'
fi
unset[0m[36m color_prompt force_color_prompt[0m

[34m# If this is an xterm set the title to user@host:dir[0m
[33mcase[0m [33m"[0m[35m$TERM[0m[33m"[0m [33min[0m
xterm*|rxvt*[33m)[0m
    [35mPS1[0m[33m="[0m[31m\[[0m[35m\e[0m[31m]0;[0m[35m${debian_chroot[0m[33m:+[0m([35m$debian_chroot[0m)[35m}[0m[31m\u@\h: \w[0m[35m\a[0m[31m\][0m[35m$PS1[0m[33m"
[0m    [33m;;[0m
*[33m)
[0m    [33m;;
esac[0m

[34m# enable color support of ls and also add handy aliases[0m
[33mif [[0m [33m-x[0m /usr/bin/dircolors [33m];[0m [33mthen
[0m    [33mtest[0m [33m-r[0m ~/.dircolors [33m&&[0m [33meval[0m [33m"[0m[35m$([0m[35mdircolors -b ~/.dircolors[0m[35m)[0m[33m"[0m [33m||[0m [33meval[0m [33m"[0m[35m$([0m[35mdircolors -b[0m[35m)[0m[33m"
[0m    [33malias [0m[36mls[0m[33m='[0m[31mls --color=auto[0m[33m'[0m
    [34m#alias dir='dir --color=auto'
[0m    [34m#alias vdir='vdir --color=auto'[0m[41;5H[33malias [0m[36mgrep[0m[33m='[0m[31mgrep --color=auto[0m[33m'
[0m    [33malias [0m[36mfgrep[0m[33m='[0m[31mfgrep --color=auto[0m[33m'
[0m    [33malias [0m[36megrep[0m[33m='[0m[31megrep --color=auto[0m[33m'
fi[0m

[34m# some more ls aliases[0m
[33malias [0m[36mll[0m[33m='[0m[31mls -alF[0m[33m'
alias [0m[36mla[0m[33m='[0m[31mls -A[0m[33m'
alias[0m [36ml[0m=[33m'[0m[31mls -CF[0m[33m'[0m

[34m# Alias definitions.
# You may want to put all your additions into a separate file like
# ~/.bash_aliases, instead of adding them here directly.
# See /usr/share/doc/bash-doc/examples in the bash-doc package.[0m[55;104H83,1[10C75%[49;1H[34h[?25h[?25l[55;94Hj[49;1H[55;94H [49;1H[1;54r[54;1H
[1;55r[55;1H[K[55;104H84,0-1[8C77%[49;1H[34h[?25h[?25l[55;94Hk[49;1H[55;94H [48;1H[55;105H3,1  [48;1H[34h[?25h[?25l[55;94Hl[48;1H[55;94H [48;2H[55;107H2[48;2H[34h[?25h[?25l[55;94Hl[48;2H[55;94H [48;3H[55;107H3[48;3H[34h[?25h[?25l[55;94Hl[48;3H[55;94H [48;4H[55;107H4[48;4H[34h[?25h[?25l[55;94Hl[48;4H[55;94H [48;5H[55;107H5[48;5H[34h[?25h[?25l[55;94Hl[48;5H[55;94H [48;6H[55;107H6[48;6H[34h[?25h[?25l[55;94Hl[48;6H[55;94H [48;7H[55;107H7[48;7H[34h[?25h[?25l[55;94Hl[48;7H[55;94H [48;8H[55;107H8[48;8H[34h[?25h[?25l[55;94Hl[48;8H[55;94H [48;9H[55;107H9[48;9H[34h[?25h[?25l[55;94Hl[48;9H[55;94H [48;10H[55;107H10[48;10H[34h[?25h[?25l[55;94Hl[48;10H[55;94H [48;11H[55;108H1[48;11H[34h[?25h[?25l[55;94Hl[48;11H[55;94H [48;12H[55;108H2[48;12H[34h[?25h[?25l[55;94Hl[48;12H[55;94H [48;13H[55;108H3[48;13H[34h[?25h[?25l[55;94Hl[48;13H[55;94H [48;14H[55;108H4[48;14H[34h[?25h[?25l[55;94Hx[48;14H[55;94H [48;14H[55;94Hdl[48;14H[55;94H  [48;14H[31mF[0m[33m'[0m[48;16H[K[48;14H[34h[?25h[?25l[55;94Hx[48;14H[55;94H [48;14H[55;94Hdl[48;14H[55;94H  [48;14H[33m'[0m[48;15H[K[48;14H[34h[?25h[?25l[55;94Hi[48;14H[55;94H [48;14H[55;1H[1m-- INSERT --[0m[55;104H[K[55;104H83,14[9C77%[48;14H[34h[?25h[?25l[31ml[0m[33m'[0m[55;108H5[48;15H[34h[?25h[55;1H[K[?25l[55;104H83,14[9C77%[48;14H[34h[?25h[?25l[55;94H:[48;14H[55;94H[K[55;1H:[34h[?25hwQ[?25l[55;3H[K[55;3H[34h[?25hq![?25l"~/.bashrc" 99L, 3105B written
[?1l>[34h[?25h[?1049l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# source ~/.bashrc
[?2004l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# l
[?2004ltotal 64
-rw-rw-r-- 1 kyp kyp 28072  9월  9  2024 ampere_tensorop_conv2dfprop.cu
drwxrwxr-x 3 kyp kyp  4096  5월 15 08:58 [0m[01;34mbuild[0m
-rwxrwxr-x 1 kyp kyp   141  9월  9  2024 [01;32mbuild.sh[0m
-rw-rw-r-- 1 kyp kyp  2362  9월  9  2024 CMakeLists.txt
-rw-rw-r-- 1 kyp kyp  5084  9월  9  2024 DS_definitions.h
-rw-rw-r-- 1 kyp kyp  7940  9월  9  2024 DS_timer.cpp
-rw-rw-r-- 1 kyp kyp  2015  9월  9  2024 DS_timer.h
-rwxrwxrwx 1 kyp kyp    64  9월  9  2024 [01;32mrun.sh[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# i[K./run.sh
[?2004lLayer,N,H,W,C,K,R,S,Runtime,GFLOPs
conv_1,1,1024,1024,8,8,3,3,7.22662,167.154
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# l
[?2004ltotal 64
-rw-rw-r-- 1 kyp kyp 28072  9월  9  2024 ampere_tensorop_conv2dfprop.cu
drwxrwxr-x 3 kyp kyp  4096  5월 15 08:58 [0m[01;34mbuild[0m
-rwxrwxr-x 1 kyp kyp   141  9월  9  2024 [01;32mbuild.sh[0m
-rw-rw-r-- 1 kyp kyp  2362  9월  9  2024 CMakeLists.txt
-rw-rw-r-- 1 kyp kyp  5084  9월  9  2024 DS_definitions.h
-rw-rw-r-- 1 kyp kyp  7940  9월  9  2024 DS_timer.cpp
-rw-rw-r-- 1 kyp kyp  2015  9월  9  2024 DS_timer.h
-rwxrwxrwx 1 kyp kyp    64  9월  9  2024 [01;32mrun.sh[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# cp -r ../
01.hello_cuda/                  05.3.gemm_shared_mem/           08.convolution/
02.gpu_adder/                   05.4.gemm_shared_mem_bank/      compute_ai/
03.gpu_thread/                  05.5.gemm_shared_mem_coalesced/ compute_ai_batch/
04.device_info/                 05.6.gemm_wmma/                 compute_ai.tar.gz
05.1.gemm_naive/                06.gemm_cutlass/                
05.2.gemm_occupancy_ctrl/       07.conv2d_cutlss/               
root@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# cp -r ../06.gemm_cutlass/run_ncu.sh .
[?2004l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# l
[?2004ltotal 68
-rw-rw-r-- 1 kyp  kyp  28072  9월  9  2024 ampere_tensorop_conv2dfprop.cu
drwxrwxr-x 3 kyp  kyp   4096  5월 15 08:58 [0m[01;34mbuild[0m
-rwxrwxr-x 1 kyp  kyp    141  9월  9  2024 [01;32mbuild.sh[0m
-rw-rw-r-- 1 kyp  kyp   2362  9월  9  2024 CMakeLists.txt
-rw-rw-r-- 1 kyp  kyp   5084  9월  9  2024 DS_definitions.h
-rw-rw-r-- 1 kyp  kyp   7940  9월  9  2024 DS_timer.cpp
-rw-rw-r-- 1 kyp  kyp   2015  9월  9  2024 DS_timer.h
-rwxr-xr-x 1 root root   108  5월 15 09:00 [01;32mrun_ncu.sh[0m
-rwxrwxrwx 1 kyp  kyp     64  9월  9  2024 [01;32mrun.sh[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# ./ru [Kn_ncu.sh
[?2004l==PROF== Connected to process 27282 (/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss/build/program)
==PROF== Profiling "Kernel" - 0: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 1: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 2: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 3: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 4: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 5: j0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 6: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 7: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 8: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 9: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 10: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 11: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 12: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 13: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 14: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 15: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 16: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 17: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 18: 0%....50%.4 passes
==PROF== Profiling "Kernel" - 16: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 17: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 18: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 19: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 20: 0%....50%....100% - 34 passes
Layer,N,H,W,C,K,R,S,Runtime,GFLOPs
conv_1,1,32,32,32,32,3,3,90.0037,0.209707
==PROF== Disconnected from process 27282
[27282] program@127.0.0.1
  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:07, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           7.04
    SM Frequency                                                             cycle/nsecond                           1.27
    Elapsed Cycles                                                                   cycle                         46,240
    Memory [%]                                                                           %                           9.06
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.29
    L1/TEX Cache Throughput                                                              %                          32.88
    L2 Cache Throughput                                                                  %                           1.53
    SM Active Cycles                                                                 cycle                      12,731.79
    Compute (SM) [%]                                                                     %                          11.39
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.43
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          10.92
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.36
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.4%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           9.06
    Max Bandwidth                                                                        %                           3.63
    L1/TEX Hit Rate                                                                      %                          57.24
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.20
    Mem Pipes Busy                                                                       %                           3.63
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          10.93
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          89.07
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.15
    Warp Cycles Per Executed Instruction                                             cycle                           9.25
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 61.8% of the total average of 9.2 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.83
    Issued Instructions                                                               inst                        155,661
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.32
    Achieved Active Warps Per SM                                                      warp                           3.99
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:07, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.96
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,784
    Memory [%]                                                                           %                           7.77
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.35
    L1/TEX Cache Throughput                                                              %                          28.29
    L2 Cache Throughput                                                                  %                           1.55
    SM Active Cycles                                                                 cycle                      12,578.89
    Compute (SM) [%]                                                                     %                          11.50
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.05
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.87
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.77
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.16
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.38
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, but only an average of 0.11 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.05
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.5% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.96
    Issued Instructions                                                               inst                        155,675
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.34
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:08, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.96
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,746
    Memory [%]                                                                           %                           7.80
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.32
    L1/TEX Cache Throughput                                                              %                          28.34
    L2 Cache Throughput                                                                  %                           1.55
    SM Active Cycles                                                                 cycle                      12,583.71
    Compute (SM) [%]                                                                     %                          11.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.04
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.85
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.80
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.13
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.38
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.06
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.94
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, but only an average of 0.11 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.05
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.5% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.85
    Issued Instructions                                                               inst                        155,663
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.32
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:08, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.96
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,804
    Memory [%]                                                                           %                           7.78
    DRAM Throughput                                                                      %                           0.71
    Duration                                                                       usecond                          36.38
    L1/TEX Cache Throughput                                                              %                          28.33
    L2 Cache Throughput                                                                  %                           1.55
    SM Active Cycles                                                                 cycle                      12,583.18
    Compute (SM) [%]                                                                     %                          11.50
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.05
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.85
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.37
    Mem Busy                                                                             %                           7.78
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.21
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.44
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligibl           warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.04
    Warp Cycles Per Executed Instruction                                             cycle                           9.13
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.6% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.96
    Issued Instructions                                                               inst                        155,676
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.n-------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::M, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:08, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.95
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,783
    Memory [%]                                                                           %                           7.79
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.38
    L1/TEX Cache Throughput                                                              %                          28.36
    L2 Cache Throughput                                                                  %                           1.55
    SM Active Cycles                                                                 cycle                      12,581.14
    Compute (SM) [%]                                                                     %                          11.50
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The keraunch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.05
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.86
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.31
    Mem Busy                                                                             %                           7.79
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.20
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.41
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                             er                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.1 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.05
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.5% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.88
    Issued Instructions                                                               inst                        155,667
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
      ------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)s::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:08, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.95
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,751
    Memory [%]                                                                           %                           7.78
    DRAM Throughput                                                                      %                           0.71
    Duration                                                                       usecond                          36.35
    L1/TEX Cache Throughput                                                              %                          28.27
    L2 Cache Throughput                                                                  %                           1.55
    SM Active Cycles                                                                 cycle                      12,589.61
    Compute (SM) [%]                                                                     %                          11.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp    
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.04
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.83
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.8%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.37
    Mem Busy                                                                             %                           7.78
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.16
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.46
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.06
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.94
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, but only an average of 0.11 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.05
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.5% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.90
    Issued Instructions                                                               inst                        155,669
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.32
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:08, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.96
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,756
    Memory [%]                                                                           %                           7.78
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.32
    L1/TEX Cache Throughput                                                              %                          28.30
    L2 Cache Throughput                                                                  %                           1.55
    SM Active Cycles                                                                 cycle                      12,573.36
    Compute (SM) [%]                                                                     %                          11.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.05
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.88
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.78
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.18
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.37
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.04
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.96
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.1 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.05
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.5% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.97
    Issued Instructions                                                               inst                        155,677
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.34
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:08, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.97
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,807
    Memory [%]                                                                           %                           7.77
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.32
    L1/TEX Cache Throughput                                                              %                          28.30
    L2 Cache Throughput                                                                  %                           1.55
    SM Active Cycles                                                                 cycle                      12,579.71
    Compute (SM) [%]                                                                     %                          11.50
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.05
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.86
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.77
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.11
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.41
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.05
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.5% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.87
    Issued Instructions                                                               inst                        155,665
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:08, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.96
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,754
    Memory [%]                                                                           %                           7.79
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.32
    L1/TEX Cache Throughput                                                              %                          28.33
    L2 Cache Throughput                                                                  %                           1.55
    SM Active Cycles                                                                 cycle                      12,583.96
    Compute (SM) [%]                                                                     %                          11.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.04
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.85
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.8%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.79
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.16
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.36
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.06
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.94
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, but only an average of 0.11 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.04
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.6% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.87
    Issued Instructions                                                               inst                        155,665
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:08, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.96
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,772
    Memory [%]                                                                           %                           7.79
    DRAM Throughput                                                                      %                           0.71
    Duration                                                                       usecond                          36.32
    L1/TEX Cache Throughput                                                              %                          28.35
    L2 Cache Throughput                                                                  %                           1.55
    SM Active Cycles                                                                 cycle                      12,580.86
    Compute (SM) [%]                                                                     %                          11.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.05
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.86
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.38
    Mem Busy                                                                             %                           7.79
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.18
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.41
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.1 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.04
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.6% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.92
    Issued Instructions                                                               inst                        155,671
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:08, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.96
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,777
    Memory [%]                                                                           %                           7.78
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.35
    L1/TEX Cache Throughput                                                              %                          28.28
    L2 Cache Throughput                                                                  %                           1.57
    SM Active Cycles                                                                 cycle                      12,583.43
    Compute (SM) [%]                                                                     %                          11.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.05
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.85
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.31
    Mem Busy                                                                             %                           7.78
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.12
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          85.00
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.04
    Warp Cycles Per Executed Instruction                                             cycle                           9.13
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.6% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.89
    Issued Instructions                                                               inst                        155,668
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:08, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.94
    SM Frequency                                                             cycle/nsecond                           1.25
    Elapsed Cycles                                                                   cycle                         45,778
    Memory [%]                                                                           %                           7.82
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.58
    L1/TEX Cache Throughput                                                              %                          28.43
    L2 Cache Throughput                                                                  %                           1.54
    SM Active Cycles                                                                 cycle                      12,552.18
    Compute (SM) [%]                                                                     %                          11.54
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.07
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (42.0%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.30
    Mem Busy                                                                             %                           7.82
    Max Bandwidth                                                                        %                           3.68
    L1/TEX Hit Rate                                                                      %                          57.15
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.41
    Mem Pipes Busy                                                                       %                           3.68
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.07
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.93
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, but only an average of 0.11 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.04
    Warp Cycles Per Executed Instruction                                             cycle                           9.13
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.6% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.84
    Issued Instructions                                                               inst                        155,662
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:08, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.96
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,752
    Memory [%]                                                                           %                           7.80
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.32
    L1/TEX Cache Throughput                                                              %                          28.33
    L2 Cache Throughput                                                                  %                           1.55
    SM Active Cycles                                                                 cycle                      12,589.29
    Compute (SM) [%]                                                                     %                          11.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.04
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.83
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.8%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.80
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.20
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.27
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.06
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.94
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, but only an average of 0.11 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.04
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.6% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.98
    Issued Instructions                                                               inst                        155,678
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTilt)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:09, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.97
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,778
    Memory [%]                                                                           %                           7.81
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.29
    L1/TEX Cache Throughput                                                              %                          28.40
    L2 Cache Throughput                                                                  %                           1.55
    SM Active Cycles                                                                 cycle                      12,580.54
    Compute (SM) [%]                                                                     %                          11.50
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.05
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.86
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    --------------------       Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.81
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.16
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.41
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.06
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.94
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, but only an average of 0.11 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.05
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.5% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.89
    Issued Instructions                                                               inst                        155,668
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:09, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.96
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,748
    Memory [%]                                                                           %                           7.78
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.35
    L1/TEX Cache Throughput                                                              %                          28.31
    L2 Cache Throughput                                                                  %                           1.55
    SM Active Cycles                                                                 cycle                      12,572.75
    Compute (SM) [%]                                                                     %                          11.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.05
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.78
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.20
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.97
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.1 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.05
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.5% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.90
    Issued Instructions                                                               inst                        155,669
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.34
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:09, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.96
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,740
    Memory [%]                                                                           %                           7.78
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.35
    L1/TEX Cache Throughput                                                              %                          28.25
    L2 Cache Throughput                                                                  %                           1.57
    SM Active Cycles                                                                 cycle                      12,595.21
    Compute (SM) [%]                                                                     %                          11.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.04
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.8%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.78
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.15
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          85.40
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.04
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.6% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,390.01
    Issued Instructions                                                               inst                        155,681
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:09, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.95
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,763
    Memory [%]                                                                           %                           7.78
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.38
    L1/TEX Cache Throughput                                                              %                          28.29
    L2 Cache Throughput                                                                  %                           1.55
    SM Active Cycles                                                                 cycle                      12,578.18
    Compute (SM) [%]                                                                     %                          11.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.05
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.87
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.31
    Mem Busy                                                                             %                           7.78
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.17
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.47
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.05
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.5% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.96
    Issued Instructions                                                               inst                        155,676
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.34
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:09, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.96
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,788
    Memory [%]                                                                           %                           7.77
    DRAM Throughput                                                                      %                           0.74
    Duration                                                                       usecond                          36.35
    L1/TEX Cache Throughput                                                              %                          28.28
    L2 Cache Throughput                                                                  %                           1.55
    SM Active Cycles                                                                 cycle                      12,581.11
    Compute (SM) [%]                                                                     %                          11.50
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.05
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.86
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.49
    Mem Busy                                                                             %                           7.77
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.15
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.44
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.06
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.94
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, but only an average of 0.11 warps were eligible per cycle. Eligible warps    
          are the subset of acticrease the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.04
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.5% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.97
    Issued Instructions                                                               inst                        155,677
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                          : Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.34
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warpMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:09, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.96
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,810
    Memory [%]                                                                           %                           7.80
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.35
    L1/TEX Cache Throughput                                                              %                          28.41
    L2 Cache Throughput                                                                  %                           1.55
    SM Active Cycles                                                                 cycle                      12,580.61
    Compute (SM) [%]                                                                     %                          11.50
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.05
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.86
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.31
    Mem Busy                                                                             %                           7.80
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.17
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.79
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.06
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.94
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, but only an average of 0.11 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.05
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.5% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.84
    Issued Instructions                                                               inst                        155,662
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to th- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:09, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.97
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,791
    Memory [%]                                                                           %                           7.81
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.29
    L1/TEX Cache Throughput                                                              %                          28.40
    L2 Cache Throughput                                                                  %                           1.55
    SM Active Cycles                                                                 cycle                      12,583.18
    Compute (SM) [%]                                                                     %                          11.50
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.05
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.85
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.81
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.19
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.53
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, but only an average of 0.11 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.06
    Warp Cycles Per Executed Instruction                                             cycle                           9.15
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.4% of the total average of 9.1 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.90
    Issued Instructions                                                               inst                        155,669
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:09, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.96
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,781
    Memory [%]                                                                           %                           7.79
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.32
    L1/TEX Cache Throughput                                                              %                          28.34
    L2 Cache Throughput                                                                  %                           1.55
    SM Active Cycles                                                                 cycle                      12,581.89
    Compute (SM) [%]                                                                     %                          11.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.05
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.86
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.79
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.16
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.38
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.04
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.6% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.84
    Issued Instructions                                                               inst                        155,662
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                                                             %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# ./run_ncu.shl[Kcp -r ../06.gemm_cutlass/run_ncu.sh .l[K./run.shl[Ksource ~/.bashrc[3Pvim[C[C[C[C[C[C[C[C[C[C[8Pls -l[K -sfvim ~/.bashrcll[K[K./build/program[5P.shll[K[Klvim ~/.bashrcl[Kcd 07.conv2d_cutlss/l[Kcd ..l[Kcd 08.convolution/l[Kcd ..l[Kvim report.txt [Kl[K[Kscript report.txt[Kl[K[Kvim run_ncu.sh[K[K[K[K[K[K[K[K[K[K[K[K[K[Kscript report.txt
[?2004lScript started, output log file is 'report.txt'.
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# exit./run_ncu.sh
[?2004l==PROF== Connected to process 27316 (/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss/build/program)
==PROF== Profiling "Kernel" - 0: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 1: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 2: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 3: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 4: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 5: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 6: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 7: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 8: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 9: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 10: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 11: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 12: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 13: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 14: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 15: 0%...% - 34 passes
==PROF== Profiling "Kernel" - 13: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 14: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 15: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 16: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 17: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 18: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 19: 0%....50%....100% - 34 passes
==PROF== Profiling "Kernel" - 20: 0%....50%....100% - 34 passes
Layer,N,H,W,C,K,R,S,Runtime,GFLOPs
conv_1,1,32,32,32,32,3,3,90.5378,0.208469
==PROF== Disconnected from process 27316
[27316] program@127.0.0.1
  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:25, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           7.05
    SM Frequency                                                             cycle/nsecond                           1.27
    Elapsed Cycles                                                                   cycle                         46,271
    Memory [%]                                                                           %                           9.05
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.29
    L1/TEX Cache Throughput                                                              %                          32.92
    L2 Cache Throughput                                                                  %                           1.52
    SM Active Cycles                                                                 cycle                      12,724.64
    Compute (SM) [%]                                                                     %                          11.38
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.43
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          10.92
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.39
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.4%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           9.05
    Max Bandwidth                                                                        %                           3.63
    L1/TEX Hit Rate                                                                      %                          57.24
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.19
    Mem Pipes Busy                                                                       %                           3.63
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          10.92
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          89.08
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.14
    Warp Cycles Per Executed Instruction                                             cycle                           9.24
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 61.9% of the total average of 9.1 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.96
    Issued Instructions                                                               inst                        155,676
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:25, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.96
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,774
    Memory [%]                                                                           %                           7.78
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.35
    L1/TEX Cache Throughput                                                              %                          28.28
    L2 Cache Throughput                                                                  %                           1.54
    SM Active Cycles                                                                 cycle                      12,589.96
    Compute (SM) [%]                                                                     %                          11.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.04
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.83
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.8%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.78
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.18
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.23
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.1 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warp Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.06
    Warp Cycles Per Executed Instruction                                             cycle                           9.15
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.5% of the total average of 9.1 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.94
    Issued Instructions                                                               inst                        155,673
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    -------------------------------   current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:25, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.95
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,751
    Memory [%]                                                                           %                           7.80
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.35
    L1/TEX Cache Throughput                                                              %                          28.33
    L2 Cache Throughput                                                                  %                           1.54
    SM Active Cycles                                                                 cycle                      12,585.86
    Compute (SM) [%]                                                                     %                          11.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsig   

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.04
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.84
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.8%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.80
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.13
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.31
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of i              0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.04
    Warp Cycles Per Executed Instruction                                             cycle                           9.13
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.6% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.87
    Issued Instructions                                                               inst                        155,665
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:25, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.96
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,784
    Memory [%]                                                                           %                           7.82
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.35
    L1/TEX Cache Throughput                                                              %                          28.44
    L2 Cache Throughput                                                                  %                           1.59
    SM Active Cycles                                                                 cycle                      12,591.96
    Compute (SM) [%]                                                                     %                          11.50
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.04
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.82
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.8%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.82
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.21
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          83.26
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.1 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.05
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.5% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.89
    Issued Instructions                                                               inst                        155,668
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equival instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.32
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                       ounters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:26, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.96
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,773
    Memory [%]                                                                           %                           7.79
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.32
    L1/TEX Cache Throughput                                                              %                          28.33
    L2 Cache Throughput                                                                  %                           1.54
    SM Active Cycles                                                                 cycle                      12,586.89
    Compute (SM) [%]                                                                     %                          11.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.04
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.84
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.8%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.79
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.19
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.26
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.1 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.05
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.5% of the total average of 9.1 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.90
    Issued Instructions                                                               inst                        155,669
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), age to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    B                      

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:26, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.98
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,771
    Memory [%]                                                                           %                           7.81
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.26
    L1/TEX Cache Throughput                                                              %                          28.41
    L2 Cache Throughput                                                                  %                           1.54
    SM Active Cycles                                                                 cycle                      12,585.39
    Compute (SM) [%]                                                                     %                          11.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.04
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.84
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.8%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.81
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.11
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.38
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.06
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.94
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, but only an average of 0.11 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.05
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.5% of the total average of 9.1 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.93
    Issued Instructions                                                               inst                        155,672
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::ajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:26, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.96
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,755
    Memory [%]                                                                           %                           7.79
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.32
    L1/TEX Cache Throughput                                                              %                          28.34
    L2 Cache Throughput                                                                  %                           1.54
    SM Active Cycles                                                                 cycle                      12,578.93
    Compute (SM) [%]                                                                     %                          11.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the a 11.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.05
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.87
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.79
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.18
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.18
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.1 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.05
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.5% of the total average of 9.1 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.83
    Issued Instructions                                                               inst                        155,661
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlassyout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:26, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.97
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,811
    Memory [%]                                                                           %                           7.78
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.32
    L1/TEX Cache Throughput                                                              %                          28.34
    L2 Cache Throughput                                                                  %                           1.54
    SM Active Cycles                                                                 cycle                      12,581.14
    Compute (SM) [%]                                                                     %                          11.50
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.05
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.86
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.78
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.21
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.23
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.1 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, but only an average of 0.11 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.05
    Warp Cycles Per Executed Instruction                                             cycle                           9.15
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.5% of the total average of 9.1 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    --------------------------------- details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.81
    Issued Instructions                                                               inst                        155,659
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:26, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.97
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,770
    Memory [%]                                                                           %                           7.81
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.29
    L1/TEX Cache Throughput                                                              %                          28.40
    L2 Cache Throughput                                                                  %                           1.58
    SM Active Cycles                                                                 cycle                      12,583.79
    Compute (SM) [%]                                                                     %                          11.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.04
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.85
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.8%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.81
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.21
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          83.91
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.1 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, but only an average of 0.11 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.05
    Warp Cycles Per Executed Instruction                                             cycle                           9.15
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.5% of the total average of 9.1 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hid-------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.77
    Issued Instructions                                                               inst                        155,654
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- -------------                                                    

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:26, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.96
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,769
    Memory [%]                                                                           %                           7.79
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.32
    L1/TEX Cache Throughput                                                              %                          28.33
    L2 Cache Throughput                                                                  %                           1.54
    SM Active Cycles                                                                 cycle                      12,583.54
    Compute (SM) [%]                                                                     %                          11.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.05
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.85
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.79
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.20
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.70
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.04
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.96
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.1 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.05
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.5% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Tr                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.92
    Issued Instructions                                                               inst                        155,671
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ----------------------------------------------------- for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:26, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.97
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,728
    Memory [%]                                                                           %                           7.78
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.29
    L1/TEX Cache Throughput                                                              %                          28.28
    L2 Cache Throughput                                                                  %                           1.58
    SM Active Cycles                                                                 cycle                      12,579.57
    Compute (SM) [%]                                                                     %                          11.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.05
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.86
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.78
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.13
    L2 Compression Success Rate                                                          %                              0
    L2 Compression          %                          84.15
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, but only an average of 0.11 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.05
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.781
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.5% of the total average of 9.1 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.81
    Issued Instructions                                                               inst                        155,659
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.34
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:26, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.97
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,777
    Memory [%]                                                                           %                           7.80
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.29
    L1/TEX Cache Throughput                                                              %                          28.39
    L2 Cache Throughput                                                                  %                           1.54
    SM Active Cycles                                                                 cycle                      12,582.39
    Compute (SM) [%]                                                                     %                          11.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.05
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.85
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                                                             %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.19
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.18
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.04
    Warp Cycles Per Executed Instruction                                             cycle                           9.13
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN                                               29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.6% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.77
    Issued Instructions                                                               inst                        155,654
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.32
    Achieved Active Warps Per SM                                                      warp                           3.99
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:26, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.95
    SM Frequency                                                             cycle/nsecond                           1.25
    Elapsed Cycles                                                                   cycle                         45,770
    Memory [%]                                                                           %                           7.81
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.45
    L1/TEX Cache Throughput                                                              %                          28.38
    L2 Cache Throughput                                                                  %                           1.54
    SM Active Cycles                                                                 cycle                      12,569.54
    Compute (SM) [%]                                                                     %                          11.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.06
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.90
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.31
    Mem Busy                                                                             %                           7.81
    Max Bandwidth                                                                        %                           3.68
    L1/TEX Hit Rate                                                                      %                          57.18
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.20
    Mem Pipes Busy                                                                       %                           3.68
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.07
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.93
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.03
    Warp Cycles Per Executed Instruction                                             cycle                           9.13
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.6% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.90
    Issued Instructions                                                               inst                        155,669
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:26, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.97
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,760
    Memory [%]                                                                           %                           7.79
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.29
    L1/TEX Cache Throughput                                                              %                          28.33
    L2 Cache Throughput                                                                  %                           1.54
    SM Active Cycles                                                                 cycle                      12,583.93
    Compute (SM) [%]                                                                     %                          11.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.05
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.85
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.8%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.79
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.17
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.37
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.04
    Warp Cycles Per Executed Instruction                                             cycle                           9.13
    Avg. Active Threads Per Warp                                                                     ----------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.6% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.90
    Issued Instructions                                                               inst                        155,669
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multipr------------ ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:26, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.96
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,745
    Memory [%]                                                                           %                           7.78
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.35
    L1/TEX Cache Throughput                                                              %                          28.27
    L2 Cache Throughput                                                                  %                           1.54
    SM Active Cycles                                                                 cycle                      12,586.75
    Compute (SM) [%]                                                                     %                          11.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.04
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.84
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.8%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- ------                                                                      %                           7.78
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.13
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.35
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.1 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.04
    Warp Cycles Per Executed Instruction                                             cycle                           9.13
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                                                                                     31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.6% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.81
    Issued Instructions                                                               inst                        155,659
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:27, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.96
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,741
    Memory [%]                                                                           %                           7.80
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.29
    L1/TEX Cache Throughput                                                              %                          28.34
    L2 Cache Throughput                                                                  %                           1.56
    SM Active Cycles                                                                 cycle                      12,584.75
    Compute (SM) [%]                                                                     %                          11.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.04
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.85
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.8%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ------------------------------                      2.32
    Mem Busy                                                                             %                           7.80
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.19
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          85.34
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, but only an average of 0.11 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    --------------------------------------------------------------------                                                                        

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.05
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.5% of the total average of 9.1 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.79
    Issued Instructions                                                               inst                        155,656
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:27, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.96
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,786
    Memory [%]                                                                           %                           7.77
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.35
    L1/TEX Cache Throughput                                                              %                          28.25
    L2 Cache Throughput                                                                  %                           1.54
    SM Active Cycles                                                                 cycle                      12,585.64
    Compute (SM) [%]                                                                     %                          11.50
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.04
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.84
    -------------------------nto account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.77
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.19
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.28
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------         occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.04
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.6% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.96
    Issued Instructions                                                               inst                        155,676
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:27, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.96
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,794
    Memory [%]                                                                           %                           7.78
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.32
    L1/TEX Cache Throughput                                                              %                          28.35
    L2 Cache Throughput                                                                  %                           1.54
    SM Active Cycles                                                                 cycle                      12,571.86
    Compute (SM) [%]                                                                     %                          11.50
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.06
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.78
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.19
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.29
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.04
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.6% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.94
    Issued Instructions                                                               inst                        155,673
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.34
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:27, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.95
    SM Frequency                                                             cycle/nsecond                           1.25
    Elapsed Cycles                                                                   cycle                         45,794
    Memory [%]                                                                           %                           7.78
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.48
    L1/TEX Cache Throughput                                                              %                          28.27
    L2 Cache Throughput                                                                  %                           1.53
    SM Active Cycles                                                                 cycle                      12,580.89
    Compute (SM) [%]                                                                     %                          11.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.05
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.86
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.31
    Mem Busy                                                                             %                           7.78
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.20
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.13
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.06
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.94
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.03
    Warp Cycles Per Executed Instruction                                             cycle                           9.13
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.6% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.92
    Issued Instructions                                                               inst                        155,671
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.32
    Achieved Active Warps Per SM                                                      warp                           3.99
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:27, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.95
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,711
    Memory [%]                                                                           %                           7.82
    DRAM Throughput                                                                      %                           0.76
    Duration                                                                       usecond                          36.38
    L1/TEX Cache Throughput                                                              %                          28.41
    L2 Cache Throughput                                                                  %                           1.54
    SM Active Cycles                                                                 cycle                      12,584.25
    Compute (SM) [%]                                                                     %                          11.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.05
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.85
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.8%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.54
    Mem Busy                                                                             %                           7.82
    Max Bandwidth                                                                        %                           3.68
    L1/TEX Hit Rate                                                                      %                          57.21
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.25
    Mem Pipes Busy                                                                       %                           3.68
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.04
    Warp Cycles Per Executed Instruction                                             cycle                           9.14
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.5% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.94
    Issued Instructions                                                               inst                        155,673
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.33
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlass::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutlass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:27, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           6.96
    SM Frequency                                                             cycle/nsecond                           1.26
    Elapsed Cycles                                                                   cycle                         45,798
    Memory [%]                                                                           %                           7.77
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       usecond                          36.35
    L1/TEX Cache Throughput                                                              %                          28.28
    L2 Cache Throughput                                                                  %                           1.54
    SM Active Cycles                                                                 cycle                      12,582.11
    Compute (SM) [%]                                                                     %                          11.50
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved       
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.44
    Executed Ipc Elapsed                                                        inst/cycle                           0.12
    Issue Slots Busy                                                                     %                          11.05
    Issued Ipc Active                                                           inst/cycle                           0.44
    SM Busy                                                                              %                          41.86
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Tensor is the highest-utilized pipeline (41.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                           2.32
    Mem Busy                                                                             %                           7.77
    Max Bandwidth                                                                        %                           3.67
    L1/TEX Hit Rate                                                                      %                          57.18
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                          86.26
    Mem Pipes Busy                                                                       %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          11.05
    Issued Warp Per Scheduler                                                                                        0.11
    No Eligible                                                                          %                          88.95
    Active Warps Per Scheduler                                                        warp                           1.00
    Eligible Warps Per Scheduler                                                      warp                           0.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                           9.04
    Warp Cycles Per Executed Instruction                                             cycle                           9.13
    Avg. Active Threads Per Warp                                                                                    31.93
    Avg. Not Predicated Off Threads Per Warp                                                                        29.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to 
          be available. This represents about 62.6% of the total average of 9.0 cycles between issuing two              
          instructions. This stall occurs when all active warps execute their next instruction on a specific,           
          oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try  
          changing the instruction mix to utilize all available pipelines in a more balanced way.                       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Source Counters section for the top stall locations in your source based on sampling data. The      
          Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides   
          more details on each stall reason.                                                                            

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       1,375.71
    Executed Instructions                                                             inst                        154,080
    Avg. Issued Instructions Per Scheduler                                            inst                       1,389.94
    Issued Instructions                                                               inst                        155,673
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel executes 0 fused and 4096 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           8
    Registers Per Thread                                                   register/thread                            254
    Shared Memory Configuration Size                                                 Kbyte                         102.40
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                          98.30
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 28              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              1
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                              4
    Theoretical Occupancy                                                                %                           8.33
    Achieved Occupancy                                                                   %                           8.34
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.01
    Branch Instructions                                                               inst                          1,216
    Branch Efficiency                                                                    %                          92.86
    Avg. Divergent Branches                                                                                          0.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      
          total 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# exit
[?2004lexit
Script done.
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# l
[?2004ltotal 496
-rw-rw-r-- 1 kyp  kyp   28072  9월  9  2024 ampere_tensorop_conv2dfprop.cu
drwxrwxr-x 3 kyp  kyp    4096  5월 15 08:58 [0m[01;34mbuild[0m
-rwxrwxr-x 1 kyp  kyp     141  9월  9  2024 [01;32mbuild.sh[0m
-rw-rw-r-- 1 kyp  kyp    2362  9월  9  2024 CMakeLists.txt
-rw-rw-r-- 1 kyp  kyp    5084  9월  9  2024 DS_definitions.h
-rw-rw-r-- 1 kyp  kyp    7940  9월  9  2024 DS_timer.cpp
-rw-rw-r-- 1 kyp  kyp    2015  9월  9  2024 DS_timer.h
-rw-r--r-- 1 root root 436002  5월 15 09:00 report.txt
-rwxr-xr-x 1 root root    108  5월 15 09:00 [01;32mrun_ncu.sh[0m
-rwxrwxrwx 1 kyp  kyp      64  9월  9  2024 [01;32mrun.sh[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# vim repr[Kort.txt 
[?2004l[?1049h[?1h=[1;55r[23m[24m[0m[H[J[?25l[55;1H"report.txt" 3122L, 436002B[2;1H▽[6n[2;1H  [3;1HPzz\[0%m[6n[3;1H           [1;1H[1;1HScript started on 2025-05-15 09:00:22+09:00 [TERM="screen" TTY="/dev/pts/6" COLUMNS="121" LINES="55"]
[34m^[[0m[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# exit[34m^H^H^H^H[0m./run_ncu.sh[34m^M[0m[2;113H[K[3;1H[34m^[[0m[?2004l[34m^M[0m==PROF== Connected to process 27316 (/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss/build/prograa[4;1Hm)[34m^M[0m
==PROF== Profiling "Kernel" - 0: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 1: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 2: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 3: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 4: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 5: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 6: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 7: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 8: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 9: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 10: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 11: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 12: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 13: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 14: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 15: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 16: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 17: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 18: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 19: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 20: 0%....50%....100% - 34 passes[34m^M[0m
Layer,N,H,W,C,K,R,S,Runtime,GFLOPs[34m^M[0m
conv_1,1,32,32,32,32,3,3,90.5378,0.208469[34m^M[0m
==PROF== Disconnected from process 27316[34m^M[0m
[27316] program@127.0.0.1[34m^M[0m
  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<<[31;1Hcutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratoo[32<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (intt[34;1H)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccess[35;1HsIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswiss[36;1He<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)122[37;1H8>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlaa[38;1Hss::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::hall[39;1Hf_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (ii[40;1Hnt)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (inn[41;1Ht)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutll[42;1Hass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::Pitt[43;1HchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (intt[44;1H)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warpp[45;1H::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultt[46;1HiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16,  [47;1H(int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::Gemmm[48;1HShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::CC[49;1HolumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int))[50;1H1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cuu[51;1Htlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTenn[52;1HsorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandd[53;1HCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>>[54;1H, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(i[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H1,1[11CTop[1;1H[34h[?25h[?25l[55;94H/[1;1H[55;1H[K[55;1H/[34h[?25hM[?25l[1;49H[7mM[0m[54;1H, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(i[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H1,49[10CTop[55;104H[K[55;3H[34h[?25hD[?25l[1;49HM[54;1H, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(i[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H1,1[11CTop[55;104H[K[55;4H[34h[?25hM[?25l[54;1H, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(i[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H1,1[11CTop[55;104H[K[55;5H[34h[?25h[?25l[55;4H[K[54;1H, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTen                                            [0m[55;104H1,1[11CTop[55;104H[K[55;5H[34h[?25h[?25l[55;4H[K[54;1H, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(i[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H1,1[11CTop[55;104H[K[55;4H[34h[?25h[?25l[55;3H[K[1;49H[7mM[0m[54;1H, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(i[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H1,49[10CTop[55;104H[K[55;3H[34h[?25hM[?25l[23m[24m[0m[H[J[1;5HBlock Limit SM[67Cblock[29C166[2;1H[34m^M[0m
    Block Limit Registers[60Cblock[30C22[4;1H[34m^M[0m
    Block Limit Shared Mem[59Cblock[30C11[6;1H[34m^M[0m
    Block Limit Warps[64Cblock[29C122[8;1H[34m^M[0m
    Theoretical Active Warps per SM[51Cwarp[30C44[10;1H[34m^M[0m
    Theoretical Occupancy[64C%[27C8.333[12;1H[34m^M[0m
    Achieved Occupancy[67C%[27C8.344[14;1H[34m^M[0m
    Achieved Active Warps Per SM[54Cwarp[27C4.000[16;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[18;1H[34m^M[0m
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    [34m^^[20;1HM[0m[21;11HBest Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      [34m^^[22;1HM[0m[23;11Hmore details on optimizing occupancy.[73C[34m^^[24;1HM
^M[0m
    Section: Source Counters[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[28;1H[34m^M[0m
    Branch Instructions Ratio[60C%[27C0.011[30;1H[34m^M[0m
    Branch Instructions[63Cinst[26C1,2166[32;1H[34m^M[0m
    Branch Efficiency[68C%[26C92.866[34;1H[34m^M[0m
    Avg. Divergent Branches[90C0.577[36;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[38;1H[34m^M[0m
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      [34m^^[40;1HM[0m[41;11Htotal 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     [34m^^[42;1HM[0m[43;11HThe CUDA Best Practices Guide[81C[34m^^[44;1HM[0m[45;12H(https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c[34m^^[46;1HM[0m[47;11H-aa) has an example on optimizing shared memory accesses.[53C[34m^^[48;1HM
^M
^[[0m[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# exit[34m^M
^[[0m[?2004l[34m^M[0mexit[34m^M[0m

Script done on 2025-05-15 09:00:29+09:00 [CO[7mMM[0mAND_EXIT_CODE="0"]
[1m[34m~                                                                                                                        [0m[55;104H3122,45[7CBot/MM[55;104H[K[55;4H[34h[?25hM[?25l[23m[24m[0m[H[J[1;1HScript started on 2025-05-15 09:00:22+09:00 [TERM="screen" TTY="/dev/pts/6" COLUMNS="121" LINES="55"]
[34m^[[0m[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# exit[34m^H^H^H^H[0m./run_ncu.sh[34m^M
^[[0m[?2004l[34m^M[0m==PROF== Connected to process 27316 (/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss/build/prograa[4;1Hm)[34m^M[0m
==PROF== Profiling "Kernel" - 0: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 1: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 2: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 3: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 4: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 5: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 6: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 7: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 8: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 9: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 10: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 11: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 12: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 13: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 14: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 15: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 16: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 17: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 18: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 19: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 20: 0%....50%....100% - 34 passes[34m^M[0m
Layer,N,H,W,C,K,R,S,Runtime,GFLOPs[34m^M[0m
conv_1,1,32,32,32,32,3,3,90.5378,0.208469[34m^M[0m
==PROF== Disconnected from process 27316[34m^M[0m
[27316] program@127.0.0.1[34m^M[0m
  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<<[31;1Hcutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratoo[32;1HrOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::Pitt[33;1HchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (intt[34;1H)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccess[35;1HsIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswiss[36;1He<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)122[37;1H8>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlaa[38;1Hss::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::hall[39;1Hf_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (ii[40;1Hnt)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (inn[41;1Ht)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutll[42;1Hass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::Pitt[43;1HchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (intt[44;1H)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warpp[45;1H::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultt[46;1HiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16,  [47;1H(int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::Gemmm[48;1HShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::CC[49;1HolumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int))[50;1H1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cuu[51;1Htlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTenn[52;1HsorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandd[53;1HCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>>[54;1H, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(i[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H1,1[11CTop/MMM[55;104H[K[55;5H[34h[?25h[?25l[55;4H[K[23m[24m[0m[H[J[1;5HBlock Limit SM[67Cblock[29C166[2;1H[34m^M[0m
    Block Limit Registers[60Cblock[30C22[4;1H[34m^M[0m
    Block Limit Shared Mem[59Cblock[30C11[6;1H[34m^M[0m
    Block Limit Warps[64Cblock[29C122[8;1H[34m^M[0m
    Theoretical Active Warps per SM[51Cwarp[30C44[10;1H[34m^M[0m
    Theoretical Occupancy[64C%[27C8.333[12;1H[34m^M[0m
    Achieved Occupancy[67C%[27C8.344[14;1H[34m^M[0m
    Achieved Active Warps Per SM[54Cwarp[27C4.000[16;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[18;1H[34m^M[0m
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    [34m^^[20;1HM[0m[21;11HBest Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      [34m^^[22;1HM[0m[23;11Hmore details on optimizing occupancy.[73C[34m^^[24;1HM
^M[0m
    Section: Source Counters[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[28;1H[34m^M[0m
    Branch Instructions Ratio[60C%[27C0.011[30;1H[34m^M[0m
    Branch Instructions[63Cinst[26C1,2166[32;1H[34m^M[0m
    Branch Efficiency[68C%[26C92.866[34;1H[34m^M[0m
    Avg. Divergent Branches[90C0.577[36;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[38;1H[34m^M[0m
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      [34m^^[40;1HM[0m[41;11Htotal 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     [34m^^[42;1HM[0m[43;11HThe CUDA Best Practices Guide[81C[34m^^[44;1HM[0m[45;12H(https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c[34m^^[46;1HM[0m[47;11H-aa) has an example on optimizing shared memory accesses.[53C[34m^^[48;1HM
^M
^[[0m[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# exit[34m^M
^[[0m[?2004l[34m^M[0mexit[34m^M[0m

Script done on 2025-05-15 09:00:29+09:00 [CO[7mMM[0mAND_EXIT_CODE="0"]
[1m[34m~                                                                                                                        [0m[55;104H3122,45[7CBot/MM[55;104H[K[55;4H[34h[?25hA[?25l[53;47H[7mA[0m[55;104H3122,45[7CBot[55;104H[K[55;5H[34h[?25h[?25l[53;45HMMA[55;104H3122,45[7CBot[53;45H[34h[?25h[?25l[55;94H/[53;45H[55;1H[K[55;1H/[34h[?25hH[?25l[23m[24m[0m[H[J[1;1H==PROF== Profiling "Kernel" - 12: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 13: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 14: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 15: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 16: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 17: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 18: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 19: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 20: 0%....50%....100% - 34 passes[34m^M[0m
Layer,N,[7mH[0m,W,C,K,R,S,Runtime,GFLOPs[34m^M[0m
conv_1,1,32,32,32,32,3,3,90.5378,0.208469[34m^M[0m
==PROF== Disconnected from process 27316[34m^M[0m
[27316] program@127.0.0.1[34m^M[0m
  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<<[15;1Hcutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratoo[16;1HrOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::TensorNHWC, cutlass::transform::Pitt[17;1HchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (intt[18;1H)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccess[19;1HsIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswiss[20;1He<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)122[21;1H8>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlaa[22;1Hss::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::hall[23;1Hf_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (ii[24;1Hnt)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (inn[25;1Ht)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutll[26;1Hass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::Pitt[27;1HchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (intt[28;1H)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warpp[29;1H::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultt[30;1HiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16,  [31;1H(int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::Gemmm[32;1HShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::CC[33;1HolumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int))[34;1H1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cuu[35;1Htlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTenn[36;1HsorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandd[37;1HCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>>[38;1H, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(ii[39;1Hnt)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajj[40;1Hor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (booll[41;1H)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimm[42;1HalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::[43;rp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int))[45;1H64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::[46;1H:layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cuu[47;1Htlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::Sharr[48;1HedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShapee[49;1H<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int))[50;1H1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombinationn[51;1H<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlasss[52;1H::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutll[53;1Hass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:25,,[54;1H Context 1, Stream 7[34m^M[0m[55;104H25,9[11C0%/H[55;104H[K[55;3H[34h[?25hM[?25l[23m[24m[0m[H[J[1;5HBlock Limit SM[67Cblock[29C166[2;1H[34m^M[0m
    Block Limit Registers[60Cblock[30C22[4;1H[34m^M[0m
    Block Limit Shared Mem[59Cblock[30C11[6;1H[34m^M[0m
    Block Limit Warps[64Cblock[29C122[8;1H[34m^M[0m
    Theoretical Active Warps per SM[51Cwarp[30C44[10;1H[34m^M[0m
    Theoretical Occupancy[64C%[27C8.333[12;1H[34m^M[0m
    Achieved Occupancy[67C%[27C8.344[14;1H[34m^M[0m
    Achieved Active Warps Per SM[54Cwarp[27C4.000[16;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[18;1H[34m^M[0m
    WRN   This kernel's theoretical occupancy (8.3%) is limited by the required amount of shared memory See the CUDA    [34m^^[20;1HM[0m[21;11HBest Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      [34m^^[22;1HM[0m[23;11Hmore details on optimizing occupancy.[73C[34m^^[24;1HM
^M[0m
    Section: Source Counters[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[28;1H[34m^M[0m
    Branch Instructions Ratio[60C%[27C0.011[30;1H[34m^M[0m
    Branch Instructions[63Cinst[26C1,2166[32;1H[34m^M[0m
    Branch Efficiency[68C%[26C92.866[34;1H[34m^M[0m
    Avg. Divergent Branches[90C0.577[36;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[38;1H[34m^M[0m
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 1152 excessive wavefronts (2% of the      [34m^^[40;1HM[0m[41;11Htotal 71424 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     [34m^^[42;1HM[0m[43;11HThe CUDA Best Practices Guide[81C[34m^^[44;1HM[0m[45;12H(https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c[34m^^[46;1HM[0m[47;11H-aa) has an example on optimizing shared memory accesses.[53C[34m^^[48;1HM
^M
^[[0m[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# exit[34m^M
^[[0m[?2004l[34m^M[0mexit[34m^M[0m

Script done on 2025-05-15 09:00:29+09:00 [COMMAND_EXIT_CODE="0"]
[1m[34m~                                                                                                                        [0m[55;104H3122,45[7CBot/HM[55;104H[K[55;4H[34h[?25hM[?25l[99C3122,45[7CBot[55;104H[K[55;5H[34h[?25hA[?25l[98C3122,45[7CBot[55;104H[K[55;6H[34h[?25h[?25l[31msearch hit BOTTOM, continuing at TOP[0m[1m[37m[41mE486: Pattern not found: HMMA[0m[55;30HK[55;5H[34h[?25hA[?25l[98C3122,45[7CBot[55;104H[K[55;6H[34h[?25h[?25l[31msearch hit BOTTOM, continuing at TOP[0m[1m[37m[41mE486: Pattern not found: HMMA[0m[55;30H[K[53;45H[55;104H3122,45[7CBot[53;45H[34h[?25h[?25l[55;94H~@k[53;45H[55;94H   [53;44H[55;110H4[53;44H[34h[?25h[?25l[55;94H~@k[53;44H[55;94H   [53;43H[55;110H3[53;43H[34h[?25h[?25l[55;94H~@k[53;43H[55;94H   [53;42H[46m[[21C][0m[55;110H2[53;42H[34h[?25h[?25l[55;94H~@k[53;42H[55;94H   [53;41H [[21C][55;110H1[53;41H[34h[?25h[?25l[55;94H~@k[53;41H[55;94H   [53;40H[55;110H0[53;40H[34h[?25h[?25l[55;94H~@k[53;40H[55;94H   [53;39H[55;109H39[53;39H[34h[?25h[?25l[55;94Hk[53;39H[55;94H [52;1H[55;107H1,0-1[52;1H[34h[?25h[?25l[55;94Hk[52;1H[55;94H [51;16H[55;107H0,14-16[51;16H[34h[?25h[?25l[55;94Hk[51;16H[55;94H [50;39H[55;106H19,38-39[50;39H[34h[?25h[?25l[55;94Hk[50;39H[55;94H [49;1H[55;107H8,1    [49;1H[34h[?25h[?25l[55;94Hk[49;1H[55;94H [47;39H[55;107H7,39[47;39H[34h[?25h[?25l[55;94Hk[47;39H[55;94H [45;39H[55;107H6[45;39H[34h[?25h[?25l[55;94Hk[45;39H[55;94H [43;39H[55;107H5[43;39H[34h[?25h[?25l[55;94Hk[43;39H[55;94H [41;39H[55;107H4[41;39H[34h[?25h[?25l[55;94Hk[41;39H[55;94H [39;39H[55;107H3[39;39H[34h[?25h[?25l[55;94Hk[39;39H[55;94H [37;39H[55;107H2[37;39H[34h[?25h[?25l[55;94Hk[37;39H[55;94H [35;39H[55;107H1[35;39H[34h[?25h[?25l[55;94Hk[35;39H[55;94H [33;39H[55;107H0[33;39H[34h[?25h[?25l[55;94H/[33;39H[55;1H[K[55;1H/[34h[?25hT[?25l[39;11H[7mT[0m[109C[34m^^[40;1HM[0m[55;104H3113,11[7CBot[55;104H[K[55;3H[34h[?25he[?25l[39;11HT[109C[34m^^[40;1HM[0m[50;53H[7mTe[0m[55;104H3119,52-53    Bot[55;104H[K[55;4H[34h[?25hn[?25l[23m[24m[0m[H[J[1;1H==PROF== Profiling "Kernel" - 19: 0%....50%....100% - 34 passes[34m^M[0m
==PROF== Profiling "Kernel" - 20: 0%....50%....100% - 34 passes[34m^M[0m
Layer,N,H,W,C,K,R,S,Runtime,GFLOPs[34m^M[0m
conv_1,1,32,32,32,32,3,3,90.5378,0.208469[34m^M[0m
==PROF== Disconnected from process 27316[34m^M[0m
[27316] program@127.0.0.1[34m^M[0m
  void cutlass::Kernel<cutlass::conv::kernel::ImplicitGemmConvolution<cutlass::conv::threadblock::ImplicitGemmMultistage<<[8;1Hcutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratoo[9;1HrOptimized<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::[7mTen[0msorNHWC, cutlass::transform::Pitt[10;1HchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (intt[11;1H)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (int)16>>, cutlass::transform::threadblock::RegularTileAccess[12;1HsIterator<cutlass::MatrixShape<(int)128, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswiss[13;1He<(int)16, (int)64>, (int)0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)122[14;1H8>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)0, cutlaa[15;1Hss::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<cutlass::MatrixShape<(int)64, (int)128>, cutlass::hall[16;1Hf_t, cutlass::layout::TensorNHWC, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (ii[17;1Hnt)128>, (int)128, cutlass::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass::AlignedArray<cutlass::half_t, (int)8, (inn[18;1Ht)16>, (bool)0>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<(int)64, (int)128>, cutll[19;1Hass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>, (int)1, cutlass::transform::Pitt[20;1HchLinearWarpRakedThreadMap<cutlass::PitchLinearShape<(int)64, (int)128>, (int)128, cutlass::PitchLinearShape<(int)8, (intt[21;1H)4>, (int)8>, (int)16>, (cutlass::arch::CacheOperation::Kind)1, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warpp[22;1H::MmaTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultt[23;1HiplicandCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16,  [24;1H(int)64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::Gemmm[25;1HShape<(int)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::CC[26;1HolumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int))[27;1H1, (bool)0, bool>, cutlass::MatrixShape<(int)0, (int)0>, cutlass::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cuu[28;1Htlass::epilogue::threadblock::Epilogue<cutlass::gemm::GemmShape<(int)128, (int)128, (int)64>, cutlass::gemm::warp::MmaTenn[29;1HsorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandd[30;1HCrosswise<(int)16, (int)64>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<(int)16, (int)64>>[31;1H, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<(ii[32;1Hnt)16, (int)8, (int)16>, (int)32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajj[33;1Hor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<(int)1, (int)1>>, (int)1, (booll[34;1H)0, bool>, (int)1, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimm[35;1HalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::[36;1H:threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0, cutlass[37;1Hs::layout::NoPermute, (bool)0>, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int))[38;1H64, (int)64>, cutlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::Array<float, (int)4, (bool)1>, cutlass::[39;1H:layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<(int)64, (int)64, (int)64>, cuu[40;1Htlass::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::Sharr[41;1HedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShapee[42;1H<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int))[43;1H1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass::epilogue::thread::LinearCombinationn[44;1H<float, (int)4, float, float, (cutlass::epilogue::thread::ScaleType::Kind)0, (cutlass::FloatRoundStyle)2, float>, cutlasss[45;1H::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)1>, (cutll[46;1Hass::conv::Operator)0, cutlass::conv::Conv2dProblemSize, (cutlass::conv::GroupMode)0>>(T1::Params), 2025-May-15 09:00:25,,[47;1H Context 1, Stream 7[34m^M[0m
    Section: GPU Speed Of Light Throughput[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[50;1H[34m^M[0m
    DRAM Frequency[59Ccycle/nsecond[27C7.055[52;1H[34m^M[0m
    SM Frequency[61Ccycle/nsecond[27C1.277[54;1H[34m^M[0m[55;104H29,329[9C0%/Ten[55;104H[K[55;5H[34h[?25hs[?25l[9;90H[7ms[0m[30Ctt[10;1Hc[119Ctt[11;1H)[119Css[12;1Hs[119Css[13;1He[119C22[14;1H8[119Caa[15;1Hs[119Cll[16;1Hf[119Cii[17;1Hn[119Cnn[18;1Ht[119Cll[19;1Ha[119Ctt[20;1Hc[119Ctt[21;1H)[119Cpp[22;1H:[119Ctt[23;1Hi[119C  [24;1H([119Cmm[25;1HS[119CCC[26;1Ho[119C))[27;1H1[119Cuu[28;1Ht[119Cnn[29;1Hs[119Cdd[30;1HC[119C>>[31;1H,[119Cii[32;1Hn[119Cjj[33;1Ho[119Cll[34;1H)[119Cmm[35;1Ha[119C::[36;1H:[119Css[37;1Hs[119C))[38;1H6[119C::[39;1H:[119Cuu[40;1Ht[119Crr[41;1He[119Cee[42;1H<[119C))[43;1H1[119Cnn[44;1H<[119Css[45;1H:[119Cll[46;1Ha[119C,,[47;1H [55;104H29,329[9C0%[55;104H[K[55;6H[34h[?25ho[?25l[9;91H[7mo[0m[29Ctt[10;1Hc[119Ctt[11;1H)[119Css[12;1Hs[119Css[13;1He[119C22[14;1H8[119Caa[15;1Hs[119Cll[16;1Hf[119Cii[17;1Hn[119Cnn[18;1Ht[119Cll[19;1Ha[119Ctt[20;1Hc[119Ctt[21;1H)[119Cpp[22;1H:[119Ctt[23;1Hi[119C  [24;1H([119Cmm[25;1HS[119CCC[26;1Ho[119C))[27;1H1[119Cuu[28;1Ht[119Cnn[29;1Hs[119Cdd[30;1HC[119C>>[31;1H,[119Cii[32;1Hn[119Cjj[33;1Ho[119Cll[34;1H)[119Cmm[35;1Ha[119C::[36;1H:[119Css[37;1Hs[119C))[38;1H6[119C::[39;1H:[119Cuu[40;1Ht[119Crr[41;1He[119Cee[42;1H<[119C))[43;1H1[119Cnn[44;1H<[119Css[45;1H:[119Cll[46;1Ha[119C,,[47;1H [55;104H29,329[9C0%[55;104H[K[55;7H[34h[?25hr[?25l[9;92H[7mr[0m[28Ctt[10;1Hc[119Ctt[11;1H)[119Css[12;1Hs[119Css[13;1He[119C22[14;1H8[119Caa[15;1Hs[119Cll[16;1Hf[119Cii[17;1Hn[119Cnn[18;1Ht[119Cll[19;1Ha[119Ctt[20;1Hc[119Ctt[21;1H)[119Cpp[22;1H:[119Ctt[23;1Hi[119C  [24;1H([119Cmm[25;1HS[119CCC[26;1Ho[119C))[27;1H1[119Cuu[28;1Ht[119Cnn[29;1Hs[119Cdd[30;1HC[119C>>[31;1H,[119Cii[32;1Hn[119Cjj[33;1Ho[119Cll[34;1H)[119Cmm[35;1Ha[119C::[36;1H:[119Css[37;1Hs[119C))[38;1H6[119C::[39;1H:[119Cuu[40;1Ht[119Crr[41;1He[119Cee[42;1H<[119C))[43;1H1[119Cnn[44;1H<[119Css[45;1H:[119Cll[46;1Ha[119C,,[47;1H [55;104H29,329[9C0%[55;104H[K[55;8H[34h[?25h[?25l[31msearch hit BOTTOM, continuing at TOP[9;87H[0mTensor[28Ctt[10;1Hc[119Ctt[11;1H)[119Css[12;1Hs[119Css[13;1He[119C22[14;1H8[119Caa[15;1Hs[119Cll[16;1Hf[119Cii[17;1Hn[119Cnn[18;1Ht[119Cll[19;1Ha[119Ctt[20;1Hc[119Ctt[21;1H)[119Cpp[22;1H:[119Ctt[23;1Hi[119C  [24;1H([119Cmm[25;1HS[119CCC[26;1Ho[119C))[27;1H1[119Cuu[28;1Ht[119Cnn[29;1Hs[119Cdd[30;1HC[119C>>[31;1H,[119Cii[32;1Hn[119Cjj[33;1Ho[119Cll[34;1H)[119Cmm[35;1Ha[119C::[36;1H:[119Css[37;1Hs[119C))[38;1H6[119C::[39;1H:[119Cuu[40;1Ht[119Crr[41;1He[119Cee[42;1H<[119C))[43;1H1[119Cnn[44;1H<[119Css[45;1H:[119Cll[46;1Ha[119C,,[47;1H [55;104H29,329[9C0%[55;104H[K[55;104H29,329[9C0%[9;87H[34h[?25h[?25l[55;94H^M[9;87H[55;94H  [48;5H[55;104H30,5  [48;5H[34h[?25h[?25l[55;94H/[48;5H[55;1H[K[55;1H/[34h[?25ht[?25l[48;8H[7mt[0m[55;104H30,8[11C0%[55;104H[K[55;3H[34h[?25he[?25l[1;54r[1;1H[47M[1;55r[1;8Ht[8;5HElapsed Cycles[67Ccycle[25C46,2711[9;1H[34m^M[0m
    Memory [%][75C%[27C9.055[11;1H[34m^M[0m
    DRAM Throughput[70C%[27C0.699[13;1H[34m^M[0m
    Duration[71Cusecond[26C36.299[15;1H[34m^M[0m
    L1/TEX Cache Throughput[62C%[26C32.922[17;1H[34m^M[0m
    L2 Cache Throughput[66C%[27C1.522[19;1H[34m^M[0m
    SM Active Cycles[65Ccycle[22C12,724.644[21;1H[34m^M[0m
    Compu[7mte[0m (SM) [%][69C%[26C11.388[23;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[25;1H[34m^M[0m
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      [34m^^[27;1HM[0m[28;11Hwaves across all SMs. Look at Launch Statistics for more details.[45C[34m^^[29;1HM
^M[0m
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved[7C[34m^^[32;1HM[0m[33;11Hclose to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel[8C[34m^^[34;1HM[0m[35;11HProfiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  [34m^^[36;1HM[0m[37;11Hon roofline analysis.[89C[34m^^[38;1HM
^M[0m
    Section: Compute Workload Analysis[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------[69C%[26C10.922[48;1H[34m^M[0m
    Issued Ipc Active[59Cinst/cycle[27C0.444[50;1H[34m^M[0m
    SM Busy[78C%[26C41.399[52;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[54;1H[34m^M[0m[55;1H[K[55;104H41,10[10C0%/te[55;104H[K[55;4H[34h[?25hn[?25l[1;54r[1;1H[30M[1;55r[25;5HINF   Tensor is the highest-utilized pipeline (41.4%) based on active cycles, taking into account the rates of its  [34m^^[26;1HM[0m[27;11Hdifferent instructions. It is the logical aggregation of individual [7mten[0msor pipelines. It's dominated by its   [34m^^[28;1HM[0m[29;11HTensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.[32C[34m^^[30;1HM
^M[0m
    Section: Memory Workload Analysis[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[34;1H[34m^M[0m
    Memory Throughput[57CGbyte/second[27C2.322[36;1H[34m^M[0m
    Mem Busy[77C%[27C9.055[38;1H[34m^M[0m
    Max Bandwidth[72C%[27C3.633[40;1H[34m^M[0m
    L1/TEX Hit Rate[70C%[26C57.244[42;1H[34m^M[0m
    L2 Compression Success Rate[58C%[30C00[44;1H[34m^M[0m
    L2 Compression Ratio[96C00[46;1H[34m^M[0m
    L2 Hit Rate[74C%[26C86.199[48;1H[34m^M[0m
    Mem Pipes Busy[71C%[27C3.633[50;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[52;1H[34m^M
^M[0m
    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   [34m^[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;1H[K[55;104H60,79[10C1%/ten[55;104H[K[55;5H[34h[?25hs[?25l[27;82H[7ms[0m[38C[34m^^[28;1HM[0m[54;1H    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   [34m^[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H60,79[10C1%[55;104H[K[55;6H[34h[?25ho[?25l[27;83H[7mo[0m[37C[34m^^[28;1HM[0m[54;1H    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   [34m^[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H60,79[10C1%[55;104H[K[55;7H[34h[?25hr[?25l[27;84H[7mr[0m[36C[34m^^[28;1HM[0m[54;1H    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   [34m^[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H60,79[10C1%[55;104H[K[55;8H[34h[?25h[?25l[27;79Htensor[36C[34m^^[28;1HM[0m[54;1H    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   [34m^[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H60,79[10C1%[27;79H[34h[?25h[?25l[55;94H~@k[27;79H[55;94H   [27;78H[55;108H8[27;78H[34h[?25h[?25l[55;94H~@k[27;78H[55;94H   [27;77H[55;108H7[27;77H[34h[?25h[?25l[55;94H~@k[27;77H[55;94H   [27;76H[55;108H6[27;76H[34h[?25h[?25l[55;94H~@k[27;76H[55;94H   [27;75H[55;108H5[27;75H[34h[?25h[?25l[55;94H~@k[27;75H[55;94H   [27;74H[55;108H4[27;74H[34h[?25h[?25l[55;94H~@k[27;74H[55;94H   [27;73H[55;108H3[27;73H[34h[?25h[?25l[55;94H~@k[27;73H[55;94H   [27;72H[55;108H2[27;72H[34h[?25h[?25l[55;94H~@k[27;72H[55;94H   [27;71H[55;108H1[27;71H[34h[?25h[?25l[55;94H~@k[27;71H[55;94H   [27;70H[55;108H0[27;70H[34h[?25h[?25l[55;94H~@k[27;70H[55;94H   [27;69H[55;107H69[27;69H[34h[?25h[?25l[55;94H~@k[27;69H[55;94H   [27;68H[55;108H8[27;68H[34h[?25h[?25l[55;94H~@k[27;68H[55;94H   [27;67H[55;108H7[27;67H[34h[?25h[?25l[55;94H~@k[27;67H[55;94H   [27;66H[55;108H6[27;66H[34h[?25h[?25l[55;94Hk[27;66H[55;94H [25;66H[55;104H59[25;66H[34h[?25h[?25l[55;94Hk[25;66H[55;94H [23;66H[55;105H8[23;66H[34h[?25h[?25l[55;94Hk[23;66H[55;94H [21;66H[55;105H7[21;66H[34h[?25h[?25l[55;94Hk[21;66H[55;94H [19;66H[55;105H6[19;66H[34h[?25h[?25l[55;94Hk[19;66H[55;94H [17;66H[55;105H5[17;66H[34h[?25h[?25l[55;94Hj[17;66H[55;94H [19;66H[55;105H6[19;66H[34h[?25h[?25l[55;94Hj[19;66H[55;94H [21;66H[55;105H7[21;66H[34h[?25h[?25l[55;94Hj[21;66H[55;94H [23;66H[55;105H8[23;66H[34h[?25h[?25l[55;94Hj[23;66H[55;94H [25;66H[55;105H9[25;66H[34h[?25h[?25l[55;94Hk[25;66H[55;94H [23;66H[55;105H8[23;66H[34h[?25h[?25l[55;94Hj[23;66H[55;94H [25;66H[55;105H9[25;66H[34h[?25h[?25l[55;94Hl[25;66H[55;94H [25;67H[55;108H7[25;67H[34h[?25h[?25l[55;94Hl[25;67H[55;94H [25;68H[55;108H8[25;68H[34h[?25h[?25l[55;94Hl[25;68H[55;94H [25;69H[55;108H9[25;69H[34h[?25h[?25l[55;94Hl[25;69H[55;94H [25;70H[55;107H70[25;70H[34h[?25h[?25l[55;94Hl[25;70H[55;94H [25;71H[55;108H1[25;71H[34h[?25h[?25l[55;94Hl[25;71H[55;94H [25;72H[55;108H2[25;72H[34h[?25h[?25l[55;94Hl[25;72H[55;94H [25;73H[55;108H3[25;73H[34h[?25h[?25l[55;94Hl[25;73H[55;94H [25;74H[55;108H4[25;74H[34h[?25h[?25l[55;94Hj[25;74H[55;94H [27;74H[55;104H60[27;74H[34h[?25h[?25l[55;94Hh[27;74H[55;94H [27;73H[55;108H3[27;73H[34h[?25h[?25l[55;94Hh[27;73H[55;94H [27;72H[55;108H2[27;72H[34h[?25h[?25l[55;94Hh[27;72H[55;94H [27;71H[55;108H1[27;71H[34h[?25h[?25l[55;94Hh[27;71H[55;94H [27;70H[55;108H0[27;70H[34h[?25h[?25l[55;94Hh[27;70H[55;94H [27;69H[55;107H69[27;69H[34h[?25h[?25l[55;94Hh[27;69H[55;94H [27;68H[55;108H8[27;68H[34h[?25h[?25l[55;94Hh[27;68H[55;94H [27;67H[55;108H7[27;67H[34h[?25h[?25l[55;94Hh[27;67H[55;94H [27;66H[55;108H6[27;66H[34h[?25h[?25l[55;94Hh[27;66H[55;94H [27;65H[55;108H5[27;65H[34h[?25h[?25l[55;94Hh[27;65H[55;94H [27;64H[55;108H4[27;64H[34h[?25h[?25l[55;94Hh[27;64H[55;94H [27;63H[55;108H3[27;63H[34h[?25h[?25l[55;94Hh[27;63H[55;94H [27;62H[55;108H2[27;62H[34h[?25h[?25l[55;94Hh[27;62H[55;94H [27;61H[55;108H1[27;61H[34h[?25h[?25l[55;94Hh[27;61H[55;94H [27;60H[55;108H0[27;60H[34h[?25h[?25l[55;94Hh[27;60H[55;94H [27;59H[55;107H59[27;59H[34h[?25h[?25l[55;94Hh[27;59H[55;94H [27;58H[55;108H8[27;58H[34h[?25h[?25l[55;94Hh[27;58H[55;94H [27;57H[55;108H7[27;57H[34h[?25h[?25l[55;94Hh[27;57H[55;94H [27;56H[55;108H6[27;56H[34h[?25h[?25l[55;94Hh[27;56H[55;94H [27;55H[55;108H5[27;55H[34h[?25h[?25l[55;94Hh[27;55H[55;94H [27;54H[55;108H4[27;54H[34h[?25h[?25l[55;94Hh[27;54H[55;94H [27;53H[55;108H3[27;53H[34h[?25h[?25l[55;94Hh[27;53H[55;94H [27;52H[55;108H2[27;52H[34h[?25h[?25l[55;94Hh[27;52H[55;94H [27;51H[55;108H1[27;51H[34h[?25h[?25l[55;94Hh[27;51H[55;94H [27;50H[55;108H0[27;50H[34h[?25h[?25l[55;94Hh[27;50H[55;94H [27;49H[55;107H49[27;49H[34h[?25h[?25l[55;94Hl[27;49H[55;94H [27;50H[55;107H50[27;50H[34h[?25h[?25l[55;108H1[27;51H[34h[?25h[?25l[55;94Hl[27;51H[55;94H [27;52H[55;108H2[27;52H[34h[?25h[?25l[55;94Hl[27;52H[55;94H [27;53H[55;108H3[27;53H[34h[?25h[?25l[55;94Hl[27;53H[55;94H [27;54H[55;108H4[27;54H[34h[?25h[?25l[55;94Hl[27;54H[55;94H [27;55H[55;108H5[27;55H[34h[?25h[?25l[55;94Hl[27;55H[55;94H [27;56H[55;108H6[27;56H[34h[?25h[?25l[55;94Hl[27;56H[55;94H [27;57H[55;108H7[27;57H[34h[?25h[?25l[55;94Hl[27;57H[55;94H [27;58H[55;108H8[27;58H[34h[?25h[?25l[55;94Hl[27;58H[55;94H [27;59H[55;108H9[27;59H[34h[?25h[?25l[55;94Hl[27;59H[55;94H [27;60H[55;107H60[27;60H[34h[?25h[?25l[55;94Hl[27;60H[55;94H [27;61H[55;108H1[27;61H[34h[?25h[?25l[55;94Hl[27;61H[55;94H [27;62H[55;108H2[27;62H[34h[?25h[?25l[55;94Hl[27;62H[55;94H [27;63H[55;108H3[27;63H[34h[?25h[?25l[55;94Hl[27;63H[55;94H [27;64H[55;108H4[27;64H[34h[?25h[?25l[55;94Hl[27;64H[55;94H [27;65H[55;108H5[27;65H[34h[?25h[?25l[55;94Hl[27;65H[55;94H [27;66H[55;108H6[27;66H[34h[?25h[?25l[55;94Hl[27;66H[55;94H [27;67H[55;108H7[27;67H[34h[?25h[?25l[55;94Hl[27;67H[55;94H [27;68H[55;108H8[27;68H[34h[?25h[?25l[55;94Hl[27;68H[55;94H [27;69H[55;108H9[27;69H[34h[?25h[?25l[55;94Hl[27;69H[55;94H [27;70H[55;107H70[27;70H[34h[?25h[?25l[55;94Hl[27;70H[55;94H [27;71H[55;108H1[27;71H[34h[?25h[?25l[55;94Hl[27;71H[55;94H [27;72H[55;108H2[27;72H[34h[?25h[?25l[55;94Hl[27;72H[55;94H [27;73H[55;108H3[27;73H[34h[?25h[?25l[55;94Hl[27;73H[55;94H [27;74H[55;108H4[27;74H[34h[?25h[?25l[55;94Hl[27;74H[55;94H [27;75H[55;108H5[27;75H[34h[?25h[?25l[55;94Hl[27;75H[55;94H [27;76H[55;108H6[27;76H[34h[?25h[?25l[55;94Hl[27;76H[55;94H [27;77H[55;108H7[27;77H[34h[?25h[?25l[55;94Hl[27;77H[55;94H [27;78H[55;108H8[27;78H[34h[?25h[?25l[55;94Hl[27;78H[55;94H [27;79H[55;108H9[27;79H[34h[?25h[?25l[55;94Hl[27;79H[55;94H [27;80H[55;107H80[27;80H[34h[?25h[?25l[55;94Hl[27;80H[55;94H [27;81H[55;108H1[27;81H[34h[?25h[?25l[55;94Hl[27;81H[55;94H [27;82H[55;108H2[27;82H[34h[?25h[?25l[55;94Hl[27;82H[55;94H [27;83H[55;108H3[27;83H[34h[?25h[?25l[55;94Hl[27;83H[55;94H [27;84H[55;108H4[27;84H[34h[?25h[?25l[55;94Hl[27;84H[55;94H [27;85H[55;108H5[27;85H[34h[?25h[?25l[55;94Hl[27;85H[55;94H [27;86H[55;108H6[27;86H[34h[?25h[?25l[55;94Hl[27;86H[55;94H [27;87H[55;108H7[27;87H[34h[?25h[?25l[55;94Hl[27;87H[55;94H [27;88H[55;108H8[27;88H[34h[?25h[?25l[55;94Hl[27;88H[55;94H [27;89H[55;108H9[27;89H[34h[?25h[?25l[55;94Hl[27;89H[55;94H [27;90H[55;107H90[27;90H[34h[?25h[?25l[55;94Hl[27;90H[55;94H [27;91H[55;108H1[27;91H[34h[?25h[?25l[55;94Hl[27;91H[55;94H [27;92H[55;108H2[27;92H[34h[?25h[?25l[55;94Hl[27;92H[55;94H [27;93H[55;108H3[27;93H[34h[?25h[?25l[55;94Hl[27;93H[55;94H [27;94H[55;108H4[27;94H[34h[?25h[?25l[55;94Hl[27;94H[55;94H [27;95H[55;108H5[27;95H[34h[?25h[?25l[55;94Hl[27;95H[55;94H [27;96H[55;108H6[27;96H[34h[?25h[?25l[55;94Hl[27;96H[55;94H [27;97H[55;108H7[27;97H[34h[?25h[?25l[55;94Hl[27;97H[55;94H [27;98H[55;108H8[27;98H[34h[?25h[?25l[55;94Hl[27;98H[55;94H [27;99H[55;108H9[27;99H[34h[?25h[?25l[55;94Hl[27;99H[55;94H [27;100H[55;107H100[27;100H[34h[?25h[?25l[55;94Hl[27;100H[55;94H [27;101H[55;109H1[27;101H[34h[?25h[?25l[55;94Hl[27;101H[55;94H [27;102H[55;109H2[27;102H[34h[?25h[?25l[55;94Hl[27;102H[55;94H [27;103H[55;109H3[27;103H[34h[?25h[?25l[55;94Hl[27;103H[55;94H [27;104H[55;109H4[27;104H[34h[?25h[?25l[55;94Hl[27;104H[55;94H [27;105H[55;109H5[27;105H[34h[?25h[?25l[55;94Hj[27;105H[55;94H [29;105H[55;105H1[29;105H[34h[?25h[?25l[55;94Hj[29;105H[55;94H [31;1H[55;105H2,1  [31;1H[34h[?25h[?25l[55;94Hk[31;1H[55;94H [29;105H[55;105H1,105[29;105H[34h[?25h[?25l[55;94Hj[29;105H[55;94H [31;1H[55;105H2,1  [31;1H[34h[?25h[?25l[55;94Hj[31;1H[55;94H [32;38H[55;105H3,38[32;38H[34h[?25h[?25l[55;94Hj[32;38H[55;94H [33;105H[55;105H4,105[33;105H[34h[?25h[?25l[55;94Hj[33;105H[55;94H [35;105H[55;105H5[35;105H[34h[?25h[?25l[55;94Hj[35;105H[55;94H [37;105H[55;105H6[37;105H[34h[?25h[?25l[55;94Hj[37;105H[55;94H [39;105H[55;105H7[39;105H[34h[?25h[?25l[55;94Hj[39;105H[55;94H [41;105H[55;105H8[41;105H[34h[?25h[?25l[55;94Hj[41;105H[55;94H [43;105H[55;105H9[43;105H[34h[?25h[?25l[55;94Hj[43;105H[55;94H [45;105H[55;104H70[45;105H[34h[?25h[?25l[55;94Hj[45;105H[55;94H [47;105H[55;105H1[47;105H[34h[?25h[?25l[55;94Hj[47;105H[5H[34h[?25h[?25l[55;94Hj[43;105H[55;94H [45;105H[55;104H70[45;105H[34h[?25h[?25l[55;94Hj[45;105H[55;94H [47;105H[55;105H1[47;105H[34h[?25h[?25l[55;94Hj[47;105H[55;94H [47;105H[1;54r[1;1H[2M[1;55r[52;1H    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   [34m^^[53;1HM[0m[54;11HL2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  [34m^[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;1H[K[55;104H72,105[9C1%[47;105H[34h[?25h[?25l[55;94Hj[47;105H[55;94H [47;105H[1;54r[1;1H[2M[1;55r[52;1H          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  [34m^^[53;1HM[0m[54;11Haccesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    [34m^[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H73,105[9C1%[47;105H[34h[?25h[?25l[55;94Hj[47;105H[55;94H [49;1H[55;105H4,1  [49;1H[34h[?25h[?25l[55;94Hj[49;1H[55;94H [46;105H[1;54r[1;1H[4M[1;55r[50;1H          accesses an average of 2.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    [34m^^[51;1HM[0m[52;11Hsection for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory[9C[34m^^[53;1HM[0m[54;11Hrequest.[102C[34m^[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H75,105[9C1%[46;105H[34h[?25h[?25l[55;94Hj[46;105H[55;94H [48;105H[55;105H6[48;105H[34h[?25h[?25l[55;94Hj[48;105H[55;94H [48;105H[1;54r[1;1H[2M[1;55r[52;1H          request.                                                                                                      [34m^^[53;1HM
^M[0m[55;104H[K[55;104H77,105[9C1%[48;105H[34h[?25h[?25l[55;94Hj[48;105H[55;94H [46;105H[1;54r[1;1H[4M[1;55r[51;5HSection: Scheduler Statistics[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[53;1H[34m^M[0m
    One or More Eligible[65C%[26C10.92[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H78,105[9C1%[46;105H[34h[?25h[?25l[55;94Hj[46;105H[55;94H [48;105H[55;105H9[48;105H[34h[?25h[?25l[55;94Hj[48;105H[55;94H [48;1H[1;54r[1;1H[2M[1;55r[52;1H    One or More Eligible                                                                 %                          10.922[53;1H[34m^M[0m
    Issued Warp Per Scheduler[88C0.11[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H80,1[11C1%[48;1H[34h[?25h[?25l[55;94Hj[48;1H[55;94H [49;34H[55;105H1,34[49;34H[34h[?25h[?25l[55;94Hj[49;34H[55;94H [46;105H[1;54r[1;1H[4M[1;55r[50;1H    Issued Warp Per Scheduler                                                                                        0.111[51;1H[34m^M[0m
    No Eligible[74C%[26C89.088[53;1H[34m^M[0m
    Active Warps Per Scheduler[56Cwarp[27C1.00[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H82,105[9C1%[46;105H[34h[?25h[?25l[55;94Hj[46;105H[55;94H [48;105H[55;105H3[48;105H[34h[?25h[?25l[55;94Hj[48;105H[55;94H [46;105H[1;54r[1;1H[4M[1;55r[50;1H    Active Warps Per Scheduler                                                        warp                           1.000[51;1H[34m^M[0m
    Eligible Warps Per Scheduler[54Cwarp[27C0.111[53;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- ------------------------------[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H84,105[9C1%[46;105H[34h[?25h[?25l[55;94Hj[46;105H[55;94H [48;105H[55;105H5[48;105H[34h[?25h[?25l[55;94Hj[48;105H[55;94H [46;105H[1;54r[1;1H[4M[1;55r[50;1H    ---------------------------------------------------------------------- --------------- -------------------------------[51;1H[34m^M[0m
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      [34m^^[53;1HM[0m[54;11Hissues an instruction every 9.2 cycles. This might leave hardware resources underutilized and may lead to     [34m^[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H86,105[9C1%[46;105H[34h[?25h[?25l[55;94Hj[46;105H[55;94H [48;105H[55;105H7[48;105H[34h[?25h[?25l[55;94Hj[48;105H[55;94H [47;105H[1;54r[1;1H[3M[1;55r[51;1H          issues an instruction every 9.2 cycles. This might leave hardware resources underutilized and may lead to     [34m^^[52;1HM[0m[53;11Hless optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   [34m^^[54;1HM[0m[55;104H[K[55;104H88,105[9C2%[47;105H[34h[?25h[?25l[55;94Hj[47;105H[55;94H [46;105H[1;54r[1;1H[3M[1;55r[52;11H1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      [34m^^[53;1HM[0m
    ----- --------------------------------------------------------------------------------------------------------------[34m^[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H89,105[9C2%[46;105H[34h[?25h[?25l[55;94Hj[46;105H[55;94H [48;105H[55;104H90[48;105H[34h[?25h[?25l[55;94Hj[48;105H[55;94H [46;105H[1;54r[1;1H[4M[1;55r[50;1H    ----- --------------------------------------------------------------------------------------------------------------[34m^^[51;1HM[0m
    WRN   The 1.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the[7C[34m^^[53;1HM[0m[54;11Hhardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical[11C[34m^[0m[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H91,105[9C2%[46;105H[34h[?25h[?25l[55;94Hj[46;105H[55;94H [48;105H[55;105H2[48;105H[34h[?25h[?25l[55;94Hj[48;105H[55;94H [46;105H[1;54r[1;1H[4M[1;55r[50;1H          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           [34m^^[51;1HM[0m[52;11Hoccupancy.[100C[34m^^[53;1HM
^M[0m[55;104H[K[55;104H93,105[9C2%[46;105H[34h[?25h[?25l[55;94Hj[46;105H[55;94H [48;105H[55;105H4[48;105H[34h[?25h[?25l[55;94Hj[48;105H[55;94H [46;105H[1;54r[1;1H[4M[1;55r[51;5HSection: Warp State Statistics[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[53;1H[34m^M[0m
    Warp Cycles Per Issued Instruction[47Ccycle[27C9.14[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H95,105[9C2%[46;105H[34h[?25h[?25l[55;94Hj[46;105H[55;94H [48;105H[55;105H6[48;105H[34h[?25h[?25l[55;94Hj[48;105H[55;94H [48;1H[1;54r[1;1H[2M[1;55r[52;1H    Warp Cycles Per Issued Instruction                                               cycle                           9.144[53;1H[34m^M[0m
    Warp Cycles Per Executed Instruction[45Ccycle[27C9.24[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H97,1[11C2%[48;1H[34h[?25h[?25l[55;94Hj[48;1H[55;94H [49;35H[55;105H8,35[49;35H[34h[?25h[?25l[55;94Hj[49;35H[55;94H [46;105H[1;54r[1;1H[4M[1;55r[50;1H    Warp Cycles Per Executed Instruction                                             cycle                           9.244[51;1H[34m^M[0m
    Avg. Active Threads Per Warp[84C31.933[53;1H[34m^M[0m
    Avg. Not Predicated Off Threads Per Warp[72C29.81[54;1H[1m[34m@@@                                                                                                                      [0m[55;104H[K[55;104H99,105[9C2%[46;105H[34h[?25h[?25l[55;94Hj[46;105H[55;94H [48;105H[55;104H100,105[48;105H[34h[?25h[?25l[55;94Hj[48;105H[55;94H [47;105H[1;54r[1;1H[3M[1;55r[51;1H    Avg. Not Predicated Off Threads Per Warp                                                                        29.811[52;1H[34m^M[0m
    ---------------------------------------------------------------------- --------------- -------------------------------[54;1H[34m^M[0m[55;104H[K[55;104H101,105[8C2%[47;105H[34h[?25h[?25l[55;94Hj[47;105H[55;94H [47;105H[1;54r[1;1H[2M[1;55r[53;5HWRN   On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a math execution pipeline to [34m^^[54;1HM[0m[55;104H[K[55;104H102,105[8C2%[47;105H[34h[?25h[?25l[55;94Hj[47;105H[55;94H [47;105H[1;54r[1;1H[2M[1;55r[53;11Hbe available. This represents about 61.9% of the total average of 9.1 cycles between issuing two[14C[34m^^[54;1HM[0m[55;104H[K[55;104H103,105[8C2%[47;105H[34h[?25h[?25l[55;94Hk[47;105H[55;94H [45;105H[55;106H2[45;105H[34h[?25h[?25l[55;94H:[45;105H[55;94H[K[55;1H:[34h[?25hq![?25l[55;1H[K[55;1H[?1l>[34h[?25h[?1049l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# l
[?2004ltotal 496
-rw-rw-r-- 1 kyp  kyp   28072  9월  9  2024 ampere_tensorop_conv2dfprop.cu
drwxrwxr-x 3 kyp  kyp    4096  5월 15 08:58 [0m[01;34mbuild[0m
-rwxrwxr-x 1 kyp  kyp     141  9월  9  2024 [01;32mbuild.sh[0m
-rw-rw-r-- 1 kyp  kyp    2362  9월  9  2024 CMakeLists.txt
-rw-rw-r-- 1 kyp  kyp    5084  9월  9  2024 DS_definitions.h
-rw-rw-r-- 1 kyp  kyp    7940  9월  9  2024 DS_timer.cpp
-rw-rw-r-- 1 kyp  kyp    2015  9월  9  2024 DS_timer.h
-rw-r--r-- 1 root root 436002  5월 15 09:00 report.txt
-rwxr-xr-x 1 root root    108  5월 15 09:00 [01;32mrun_ncu.sh[0m
-rwxrwxrwx 1 kyp  kyp      64  9월  9  2024 [01;32mrun.sh[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# vim run_ncu.sh
[?2004l[?1049h[?1h=[1;55r[23m[24m[0m[H[J[?25l[55;1H"run_ncu.sh" 3L, 108B[2;1H▽[6n[2;1H  [3;1HPzz\[0%m[6n[3;1H           [1;1H[1;1H[34m#script ncu_report.txt[0m
sudo /usr/local/cuda/bin/ncu [35m--set[0m full [35m--target-processes[0m all ./build/program[2;79H[K[3;1H[34m#exit[0m[3;6H[K[4;1H[1m[34m~                                                                                                                        [5;1H~                                                                                                                        [6;1H~                                                                                                                        [7;1H~                                                                                                                        [8;1H~                                                                                                                        [9;1H~                                                                                                                        [10;1H~                                                                                                                        [11;1H~                                                                                                                        [12;1H~                                                                                                                        [13;1H~                                                                                                                        [14;1H~                                                                                                                        [15;1H~                                                                                                                        [16;1H~                                                                                                                        [17;1H~                                                                                                                        [18;1H~                                                                                                                        [19;1H~                                                                                                                        [20;1H~                                                                                                                        [21;1H~                                                                                                                        [22;1H~                                                                                                                        [23;1H~                                                                                                                        [24;1H~                                                                                                                        [25;1H~                                                                                                                        [26;1H~                                                                                                                        [27;1H~                                                                                                                        [28;1H~                                                                                                                        [29;1H~                                                                                                                        [30;1H~                                                                                                                        [31;1H~                                                                                                                        [32;1H~                                                                                                                        [33;1H~                                                                                                                        [34;1H~                                                                                                                        [35;1H~                                                                                                                        [36;1H~                                                                                                                        [37;1H~                                                                                                                        [38;1H~                                                                                                                        [39;1H~                                                                                                                        [40;1H~                                                                                                                        [41;1H~                                                                                                                        [42;1H~                                                                                                                        [43;1H~                                                                                                                        [44;1H~                                                                                                                        [45;1H~                                                                                                                        [46;1H~                                                                                                                        [47;1H~                                                                                                                        [48;1H~                                                                                                                        [49;1H~                                                                                                                        [50;1H~                                                                                                                        [51;1H~                                                                                                                        [52;1H~                                                                                                                        [53;1H~                                                                                                                        [54;1H~                                                                                                                        [0m[55;104H1,1[11CAll[1;1H[34h[?25h[?25l[55;94H:[1;1H[55;1H[K[55;1H:[34h[?25hq![?25l[55;1H[K[55;1H[?1l>[34h[?25h[?1049l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# l
[?2004ltotal 496
-rw-rw-r-- 1 kyp  kyp   28072  9월  9  2024 ampere_tensorop_conv2dfprop.cu
drwxrwxr-x 3 kyp  kyp    4096  5월 15 08:58 [0m[01;34mbuild[0m
-rwxrwxr-x 1 kyp  kyp     141  9월  9  2024 [01;32mbuild.sh[0m
-rw-rw-r-- 1 kyp  kyp    2362  9월  9  2024 CMakeLists.txt
-rw-rw-r-- 1 kyp  kyp    5084  9월  9  2024 DS_definitions.h
-rw-rw-r-- 1 kyp  kyp    7940  9월  9  2024 DS_timer.cpp
-rw-rw-r-- 1 kyp  kyp    2015  9월  9  2024 DS_timer.h
-rw-r--r-- 1 root root 436002  5월 15 09:00 report.txt
-rwxr-xr-x 1 root root    108  5월 15 09:00 [01;32mrun_ncu.sh[0m
-rwxrwxrwx 1 kyp  kyp      64  9월  9  2024 [01;32mrun.sh[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# lvim run_ncu.sh
[?2004l[?1049h[?1h=[1;55r[23m[24m[0m[H[J[?25l[55;1H"run_ncu.sh" 3L, 108B[2;1H▽[6n[2;1H  [3;1HPzz\[0%m[6n[3;1H           [1;1H[1;1H[34m#script ncu_report.txt[0m
sudo /usr/local/cuda/bin/ncu [35m--set[0m full [35m--target-processes[0m all ./build/program[2;79H[K[3;1H[34m#exit[0m[3;6H[K[4;1H[1m[34m~                                                                                                                        [5;1H~                                                                                                                        [6;1H~                                                                                                                        [7;1H~                                                                                                                        [8;1H~                                                                                                                        [9;1H~                                                                                                                        [10;1H~                                                                                                                        [11;1H~                                                                                                                        [12;1H~                                                                                                                        [13;1H~                                                                                                                        [14;1H~                                                                                                                        [15;1H~                                                                                                                        [16;1H~                                                                                                                        [17;1H~                                                                                                                        [18;1H~                                                                                                                        [19;1H~                                                                                                                        [20;1H~                                                                                                                        [21;1H~                                                                                                                        [22;1H~                                                                                                                        [23;1H~                                                                                                                        [24;1H~                                                                                                                        [25;1H~                                                                                                                        [26;1H~                                                                                                                        [27;1H~                                                                                                                        [28;1H~                                                                                                                        [29;1H~                                                                                                                        [30;1H~                                                                                                                        [31;1H~                                                                                                                        [32;1H~                                                                                                                        [33;1H~                                                                                                                        [34;1H~                                                                                                                        [35;1H~                                                                                                                        [36;1H~                                                                                                                        [37;1H~                                                                                                                        [38;1H~                                                                                                                        [39;1H~                                                                                                                        [40;1H~                                                                                                                        [41;1H~                                                                                                                        [42;1H~                                                                                                                        [43;1H~                                                                                                                        [44;1H~                                                                                                                        [45;1H~                                                                 [47;1H~                                                                                                                        [48;1H~                                                                                                                        [49;1H~                                                                                                                        [50;1H~                                                                                                                        [51;1H~                                                                                                                        [52;1H~                                                                                                                        [53;1H~                                                                                                                        [54;1H~                                                                                                                        [0m[55;104H1,1[11CAll[1;1H[34h[?25h[?25l[55;94Hd[1;1H[34h[?25h[?25l[55;95H~@k[1;1H[55;94H    [1;1H[34h[?25h[?25l[55;94H^[[1;1H[55;94H  [1;1H[55;94H^[[1;1H[55;94H  [1;1H[34h[?25h[?25l[55;94H<3163>[1;1H[55;94H      [1;1H[34h[?25h[?25l[55;94H<3163>[1;1H[55;94H      [1;1H[34h[?25h[?25l[55;94H<3163>[1;1H[55;94H      [1;1H[34h[?25h[?25l[55;94H<3153>[1;1H[55;94H      [1;1H[34h[?25h[?25l[55;94H<3153>[1;1H[55;94H      [1;1H[34h[?25h[?25l[55;94H<3153>[1;1H[55;94H      [1;1H[34h[?25h[?25l[55;94H<314f>[1;1H[55;94H      [1;1H[34h[?25h[?25l[55;94H<314f>[1;1H[55;94H      [1;1H[34h[?25h[?25l[55;94H<3153>[1;1H[55;94H      [1;1H[34h[?25h[?25l[55;94H<3153>[1;1H[55;94H      [1;1H[34h[?25h[?25l[55;94Hk[1;1H[55;94H [1;1H[34h[?25h[?25l[55;94Hj[1;1H[55;94H [2;1H[55;104H2[2;1H[34h[?25h[?25l[55;94Hj[2;1H[55;94H [3;1H[55;104H3[3;1H[34h[?25h[?25l[55;94Hk[3;1H[55;94H [2;1H[55;104H2[2;1H[34h[?25h[?25l[55;94Hk[2;1H[55;94H [1;1H[55;104H1[1;1H[34h[?25h[?25l[55;94Hj[1;1H[55;94H [2;1H[55;104H2[2;1H[34h[?25h[?25l[55;94Ho[2;1H[55;94H [3;1H[55;1H[1m-- INSERT --[0m[55;14H[K[55;104H3,1[11CTop[3;54r[3;1H[L[1;55r[55;104H[K[55;104H3,1[11CAll[3;1H[34h[?25h[?25ls[55;106H2[3;2H[34h[?25h[?25lu[55;106H3[3;3H[34h[?25h[?25ld[55;106H4[3;4H[34h[?25h[?25lo[55;106H5[3;5H[34h[?25h[?25l[55;106H6[3;6H[34h[?25h[?25l.[55;106H7[3;7H[34h[?25h[?25l[3;6H[K[55;106H6[3;6H[34h[?25h[?25l/[55;106H7[3;7H[34h[?25h[?25lu[55;106H8[3;8H[34h[?25h[?25ls[55;106H9[3;9H[34h[?25h[?25lr[55;106H10[3;10H[34h[?25h[?25l/[55;107H1[3;11H[34h[?25h[?25lc[55;107H2[3;12H[34h[?25h[?25ll[55;107H3[3;13H[34h[?25h[?25l[3;12H[K[55;107H2[3;12H[34h[?25h[?25l[3;11H[K[55;107H1[3;11H[34h[?25h[?25ll[55;107H2[3;12H[34h[?25h[?25lo[55;107H3[3;13H[34h[?25h[?25lc[55;107H4[3;14H[34h[?25h[?25la[55;107H5[3;15H[34h[?25h[?25l[33mlocal[0m[55;107H6[3;16H[34h[?25h[?25llocal/[55;107H7[3;17H[34h[?25h[?25lc[55;107H8[3;18H[34h[?25h[?25lu[55;107H9[3;19H[34h[?25h[?25ld[55;106H20[3;20H[34h[?25h[?25la[55;107H1[3;21H[34h[?25h[?25l/[55;107H2[3;22H[34h[?25h[?25lb[55;107H3[3;23H[34h[?25h[?25li[55;107H4[3;24H[34h[?25h[?25ln[55;107H5[3;25H[34h[?25h[?25l/[55;107H6[3;26H[34h[?25h[?25ln[55;107H7[3;27H[34h[?25h[?25lc[55;107H8[3;28H[34h[?25h[?25lu[55;107H9[3;29H[34h[?25h[?25l[55;106H30[3;30H[34h[?25h[?25l-[55;107H1[3;31H[34h[?25h[?25l[35m--[0m[55;107H2[3;32H[34h[?25h[?25l[35mi[0m[55;107H3[3;33H[34h[?25h[?25l[35mm[0m[55;107H4[3;34H[34h[?25h[?25l[35mp[0m[55;107H5[3;35H[34h[?25h[?25l[35mo[0m[55;107H6[3;36H[34h[?25h[?25l[35mr[0m[55;107H7[3;37H[34h[?25h[?25l[35mt[0m[55;107H8[3;38H[34h[?25h[55;1H[K[?25l[55;104H3,37[10CAll[3;37H[34h[?25h[?25l[55;94Hu[3;37H[55;1H1 line less; before #2  10 seconds ago[55;94H[K[2;1H[3;54r[35mt[0m[55;107H8[3;38H[34h[?25h[55;1H[K[?25l[55;104H3,37[10CAll[3;37H[34h[?25h[?25l[55;94Hu[3;37H[55;1H1 line less; before #2  10 seconds ago[55;94H[K[2;1H[3;54r[54;1H
[1;55r[54;1H[1m[34m~                                                                                                                        [0m[55;1H[K[55;104H2,1[11CAll1 line less; before #2  10 seconds ago[55;104H[K[55;104H2,1[11CAll[2;1H[34h[?25h[?25l[34h[?25h[?25l[55;94H:[2;1H[55;1H[K[55;1H:[34h[?25hq![?25l[55;1H[K[55;1H[?1l>[34h[?25h[?1049l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# ㅏㄴ[Kroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# ㅏㄴ[Kroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# ㅏㄴ\[K[K[Kl
[?2004ltotal 496
-rw-rw-r-- 1 kyp  kyp   28072  9월  9  2024 ampere_tensorop_conv2dfprop.cu
drwxrwxr-x 3 kyp  kyp    4096  5월 15 08:58 [0m[01;34mbuild[0m
-rwxrwxr-x 1 kyp  kyp     141  9월  9  2024 [01;32mbuild.sh[0m
-rw-rw-r-- 1 kyp  kyp    2362  9월  9  2024 CMakeLists.txt
-rw-rw-r-- 1 kyp  kyp    5084  9월  9  2024 DS_definitions.h
-rw-rw-r-- 1 kyp  kyp    7940  9월  9  2024 DS_timer.cpp
-rw-rw-r-- 1 kyp  kyp    2015  9월  9  2024 DS_timer.h
-rw-r--r-- 1 root root 436002  5월 15 09:00 report.txt
-rwxr-xr-x 1 root root    108  5월 15 09:00 [01;32mrun_ncu.sh[0m
-rwxrwxrwx 1 kyp  kyp      64  9월  9  2024 [01;32mrun.sh[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# vim build.sh
[?2004l[?1049h[?1h=[1;55r[23m[24m[0m[H[J[?25l[55;1H"build.sh" 4L, 141B[2;1H▽[6n[2;1H  [3;1HPzz\[0%m[6n[3;1H           [1;1H[1;1H[33mmkdir[0m build
[33mcd[0m build[2;9H[K[3;1Hcmake .. [35m-DCMAKE_CUDA_COMPILER[0m[33m=[0m/usr/local/cuda/bin/nvcc [35m-DCMAKE_CXX_COMPILER[0m[33m=[0mg++[31m-10[0m [35m-DCMAKE_C_COMPILER[0m[33m=[0mgcc-10[3;110H[K[4;1Hmake [35m-j[0m12
[1m[34m~                                                                                                                        [6;1H~                                                                                                                        [7;1H~                                                                                                                        [8;1H~                                                                                                                        [9;1H~                                                                                                                        [10;1H~                                                                                                                        [11;1H~                                                                                                                        [12;1H~                                                                                                                        [13;1H~                                                                                                                        [14;1H~                                                                                                                        [15;1H~                                                                                                                        [16;1H~                                                                                                                        [17;1H~                                                                                                                        [18;1H~                                                                                                                        [19;1H~                                                                                                                        [20;1H~                                                                                                                        [21;1H~                                                                                                                        [22;1H~                                                                                                                        [23;1H~                                                                                                                        [24;1H~                                                                                                                        [25;1H~                                                                                                                        [26;1H~                                                                                                                        [27;1H~                                                                                                                        [28;1H~                                                                                                                        [29;1H~                                                                                                                        [30;1H~                                                                                                                        [31;1H~                                                                                                                        [32;1H~                                                                                                                        [33;1H~                                                                                                                        [34;1H~                                                                                                                        [35;1H~                                                                                                                        [36;1H~                                                                                                                        [37;1H~                                                                                                                        [38;1H~                                                                                                                        [39;1H~                                                                                                                        [40;1H~                                                                                                                        [41;1H~                                                                                                                        [42;1H~                                                                                                                        [43;1H~                                                                                                                        [44;1H~                                                                                                                        [45;1H~                                                                                                                        [46;1H~                                                                                                                        [47;1H~                                                                                                                        [48;1H~                                                                                                                        [49;1H~                                                                                                                        [50;1H~                                                                                                                        [51;1H~                                                                                                                        [52;1H~                                                                                                                        [53;1H~                                                                                                                        [54;1H~                                                                                                                        [0m[55;104H1,1[11CAll[1;1H[34h[?25h[?25l[55;94H:[1;1H[55;1H[K[55;1H:[34h[?25hq![?25l[55;1H[K[55;1H[?1l>[34h[?25h[?1049l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# l
[?2004ltotal 496
-rw-rw-r-- 1 kyp  kyp   28072  9월  9  2024 ampere_tensorop_conv2dfprop.cu
drwxrwxr-x 3 kyp  kyp    4096  5월 15 08:58 [0m[01;34mbuild[0m
-rwxrwxr-x 1 kyp  kyp     141  9월  9  2024 [01;32mbuild.sh[0m
-rw-rw-r-- 1 kyp  kyp    2362  9월  9  2024 CMakeLists.txt
-rw-rw-r-- 1 kyp  kyp    5084  9월  9  2024 DS_definitions.h
-rw-rw-r-- 1 kyp  kyp    7940  9월  9  2024 DS_timer.cpp
-rw-rw-r-- 1 kyp  kyp    2015  9월  9  2024 DS_timer.h
-rw-r--r-- 1 root root 436002  5월 15 09:00 report.txt
-rwxr-xr-x 1 root root    108  5월 15 09:00 [01;32mrun_ncu.sh[0m
-rwxrwxrwx 1 kyp  kyp      64  9월  9  2024 [01;32mrun.sh[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# vim ampere_tensorop_conv2dfprop.cu 
[?2004l[?1049h[?1h=[1;55r[23m[24m[0m[H[J[?25l[55;1H"ampere_tensorop_conv2dfprop.cu" 872L, 28072B[2;1H▽[6n[2;1H  [3;1HPzz\[0%m[6n[3;1H           [1;1H[1;1H[34m/***************************************************************************************************
 * Copyright (c) [0m[31m2017[0m[34m - [0m[31m2024[0m[34m NVIDIA CORPORATION & AFFILIATES. All rights reserved.[0m[2;83H[K[3;1H[34m * SPDX-License-Identifier: BSD-[0m[31m3[0m[34m-Clause[0m[3;41H[K[4;1H[34m *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 * [0m[31m1.[0m[34m Redistributions of source code must retain the above copyright notice, this
 * list of conditions and the following disclaimer.
 *
 * [0m[31m2.[0m[34m Redistributions in binary form must reproduce the above copyright notice,
 * this list of conditions and the following disclaimer in the documentation
 * and/or other materials provided with the distribution.
 *
 * [0m[31m3.[0m[34m Neither the name of the copyright holder nor the names of its
 * contributors may be used to endorse or promote products derived from
 * this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS [0m[31m"AS IS"[0m
[34m * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 **************************************************************************************************/

/**

This example shows how to run CUTLASS's convolution kernels
based on the Implicit GEMM algorithm, that use the Tensor Cores
on an NVIDIA Ampere GPU.

Writing a single high-performance convolution kernel is hard enough,
let alone writing kernels that perform well for multiple problem sizes
and use good software abstractions.
CUTLASS provides simplified abstractions
to compose multiple sections of a convolution kernel.
When used properly, the kernels can reach peak GPU performance.

CUTLASS divides a kernel into hierarchical composable sections
for each level of the GPU hardware hierarchy:
the tile shape each thread computes)
can be used to form warp tiles (the tile shape each warp computes),
and multiple warp tiles can be used to compute threadblock tiles
(the tile shape computed by a threadblock).[0m[55;104H1,1[11CTop[1;1H[34h[?25h[?25l[55;106H2[1;2H[34h[?25h[?25l[55;94H^M[1;2H[55;94H  [2;2H[55;104H2[2;2H[34h[?25h[?25l[55;94H:[2;2H[55;1H[K[55;1H:[34h[?25hq![?25l[55;1H[K[55;1H[?1l>[34h[?25h[?1049l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# [Kroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# [Kroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# [Kroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# [Kroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# l
[?2004ltotal 496
-rw-rw-r-- 1 kyp  kyp   28072  9월  9  2024 ampere_tensorop_conv2dfprop.cu
drwxrwxr-x 3 kyp  kyp    4096  5월 15 08:58 [0m[01;34mbuild[0m
-rwxrwxr-x 1 kyp  kyp     141  9월  9  2024 [01;32mbuild.sh[0m
-rw-rw-r-- 1 kyp  kyp    2362  9월  9  2024 CMakeLists.txt
-rw-rw-r-- 1 kyp  kyp    5084  9월  9  2024 DS_definitions.h
-rw-rw-r-- 1 kyp  kyp    7940  9월  9  2024 DS_timer.cpp
-rw-rw-r-- 1 kyp  kyp    2015  9월  9  2024 DS_timer.h
-rw-r--r-- 1 root root 436002  5월 15 09:00 report.txt
-rwxr-xr-x 1 root root    108  5월 15 09:00 [01;32mrun_ncu.sh[0m
-rwxrwxrwx 1 kyp  kyp      64  9월  9  2024 [01;32mrun.sh[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/07.conv2d_cutlss# cd ..
[?2004l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study# l
[?2004ltotal 2464
drwxrwxr-x  3 kyp kyp    4096 12월  7 14:04 [0m[01;34m01.hello_cuda[0m
drwxrwxr-x  3 kyp kyp    4096  6월  6  2024 [01;34m02.gpu_adder[0m
drwxrwxr-x  3 kyp kyp    4096  8월 18  2024 [01;34m03.gpu_thread[0m
drwxrwxr-x  3 kyp kyp    4096  9월  1  2024 [01;34m04.device_info[0m
drwxrwxr-x  3 kyp kyp    4096  9월 20  2024 [01;34m05.1.gemm_naive[0m
drwxrwxr-x  3 kyp kyp    4096  5월 15 08:39 [01;34m05.2.gemm_occupancy_ctrl[0m
drwxrwxr-x  3 kyp kyp    4096  9월 20  2024 [01;34m05.3.gemm_shared_mem[0m
drwxrwxr-x  3 kyp kyp    4096  9월 23  2024 [01;34m05.4.gemm_shared_mem_bank[0m
drwxrwxr-x  3 kyp kyp    4096  5월 15 08:40 [01;34m05.5.gemm_shared_mem_coalesced[0m
drwxrwxr-x  3 kyp kyp    4096 12월  8 08:25 [01;34m05.6.gemm_wmma[0m
drwxrwxr-x  3 kyp kyp    4096  5월 15 08:57 [01;34m06.gemm_cutlass[0m
drwxrwxr-x  3 kyp kyp    4096  5월 15 13:16 [01;34m07.conv2d_cutlss[0m
drwxrwxr-x 10 kyp kyp    4096  5월  8 13:57 [01;34m08.convolution[0m
drwxrwxr-x  4 kyp kyp    4096 12월 10 10:23 [01;34mcompute_ai[0m
drwxrwxr-x  4 kyp kyp    4096 12월 10 00:17 [01;34mcompute_ai_batch[0m
-rw-rw-r--  1 kyp kyp 2459018 12월  9 00:28 [01;31mcompute_ai.tar.gz[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study# cd ..[K[K[K[Kcd [K[K[Kd 08.convolution/
[?2004l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# l
[?2004ltotal 32
drwxrwxr-x 3 kyp kyp 4096  5월  3 00:10 [0m[01;34m00.cufft_1d_c2c[0m
drwxrwxr-x 3 kyp kyp 4096  5월  8 15:17 [01;34m01.cufft_1d_r2c[0m
drwxrwxr-x 3 kyp kyp 4096  5월  6 22:41 [01;34m02.cufft_1d_r2c_runtime[0m
drwxrwxr-x 3 kyp kyp 4096  5월  5 23:21 [01;34m02.cufft_1d_r2c_runtime_2[0m
drwxrwxr-x 3 kyp kyp 4096  5월  9 13:10 [01;34m03.cufft_1d_conv[0m
drwxrwxr-x 4 kyp kyp 4096  5월  8 15:15 [01;34m04.cufft_2d_conv[0m
drwxrwxr-x 3 kyp kyp 4096  5월  8 15:23 [01;34m05.cufftDX_1d_c2c[0m
drwxrwxr-x 4 kyp kyp 4096  5월  7 23:01 [01;34m08.cufftDX_2d_conv[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# cd [K[K[Kcd [K[K[Kcd [K[K[Kcd 05.cufftDX_1d_c2c/
[?2004l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution/05.cufftDX_1d_c2c# l
[?2004ltotal 136
drwxrwxr-x 3 kyp kyp  4096  5월  8 13:57 [0m[01;34mbuild[0m
-rwxrwxr-x 1 kyp kyp   123  5월  8 13:57 [01;32mbuild.sh[0m
-rw-rw-r-- 1 kyp kyp   702  5월  8 13:57 CMakeLists.txt
-rw-rw-r-- 1 kyp kyp  1051  5월  8 13:57 gpuTimer.h
-rw-rw-r-- 1 kyp kyp 13212  5월  8 15:14 main.cu
-rw-rw-r-- 1 kyp kyp 99741  5월  8 13:57 matplotlibcpp.h
-rwxrwxr-x 1 kyp kyp   352  5월  8 13:57 [01;32mrun.sh[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution/05.cufftDX_1d_c2c# cd ..
[?2004l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# l
[?2004ltotal 32
drwxrwxr-x 3 kyp kyp 4096  5월  3 00:10 [0m[01;34m00.cufft_1d_c2c[0m
drwxrwxr-x 3 kyp kyp 4096  5월  8 15:17 [01;34m01.cufft_1d_r2c[0m
drwxrwxr-x 3 kyp kyp 4096  5월  6 22:41 [01;34m02.cufft_1d_r2c_runtime[0m
drwxrwxr-x 3 kyp kyp 4096  5월  5 23:21 [01;34m02.cufft_1d_r2c_runtime_2[0m
drwxrwxr-x 3 kyp kyp 4096  5월  9 13:10 [01;34m03.cufft_1d_conv[0m
drwxrwxr-x 4 kyp kyp 4096  5월  8 15:15 [01;34m04.cufft_2d_conv[0m
drwxrwxr-x 3 kyp kyp 4096  5월  8 15:23 [01;34m05.cufftDX_1d_c2c[0m
drwxrwxr-x 4 kyp kyp 4096  5월  7 23:01 [01;34m08.cufftDX_2d_conv[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# cp -0[Kr 01.cufft_1d_r2c/ 02c[K[K[K02.u[K[K[K1.cufft_2d_rc[K2c
[?2004l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# l
[?2004ltotal 36
drwxrwxr-x 3 kyp  kyp  4096  5월  3 00:10 [0m[01;34m00.cufft_1d_c2c[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  8 15:17 [01;34m01.cufft_1d_r2c[0m
drwxr-xr-x 3 root root 4096  5월 22 10:28 [01;34m01.cufft_2d_r2c[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  6 22:41 [01;34m02.cufft_1d_r2c_runtime[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  5 23:21 [01;34m02.cufft_1d_r2c_runtime_2[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  9 13:10 [01;34m03.cufft_1d_conv[0m
drwxrwxr-x 4 kyp  kyp  4096  5월  8 15:15 [01;34m04.cufft_2d_conv[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  8 15:23 [01;34m05.cufftDX_1d_c2c[0m
drwxrwxr-x 4 kyp  kyp  4096  5월  7 23:01 [01;34m08.cufftDX_2d_conv[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# l[Kc d[K[Kd 01.cufft_1[K2d_r2c/
[?2004l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution/01.cufft_2d_r2c# l
[?2004ltotal 132
drwxr-xr-x 3 root root  4096  5월 22 10:28 [0m[01;34mbuild[0m
-rwxr-xr-x 1 root root   123  5월 22 10:28 [01;32mbuild.sh[0m
-rw-r--r-- 1 root root   702  5월 22 10:28 CMakeLists.txt
-rw-r--r-- 1 root root  1051  5월 22 10:28 gpuTimer.h
-rw-r--r-- 1 root root  7902  5월 22 10:28 main.cu
-rw-r--r-- 1 root root  4959  5월 22 10:28 main.cu.save
-rw-r--r-- 1 root root 99741  5월 22 10:28 matplotlibcpp.h
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution/01.cufft_2d_r2c# d[Kcd ..
[?2004l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# l
[?2004ltotal 36
drwxrwxr-x 3 kyp  kyp  4096  5월  3 00:10 [0m[01;34m00.cufft_1d_c2c[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  8 15:17 [01;34m01.cufft_1d_r2c[0m
drwxr-xr-x 3 root root 4096  5월 22 10:28 [01;34m01.cufft_2d_r2c[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  6 22:41 [01;34m02.cufft_1d_r2c_runtime[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  5 23:21 [01;34m02.cufft_1d_r2c_runtime_2[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  9 13:10 [01;34m03.cufft_1d_conv[0m
drwxrwxr-x 4 kyp  kyp  4096  5월  8 15:15 [01;34m04.cufft_2d_conv[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  8 15:23 [01;34m05.cufftDX_1d_c2c[0m
drwxrwxr-x 4 kyp  kyp  4096  5월  7 23:01 [01;34m08.cufftDX_2d_conv[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# mv 00.cufft_1d_c2c/[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[Kmv 01.cufft_2d_r2c/ 02.cufft_2d_32[K[Kr2c
[?2004l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# l
[?2004ltotal 36
drwxrwxr-x 3 kyp  kyp  4096  ufft_2d_r2c/ 02.cufft_2d_32[K[Kr2c
[?2004l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# l
[?2004ltotal 36
drwxrwxr-x 3 kyp  kyp  4096  5월  3 00:10 [0m[01;34m00.cufft_1d_c2c[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  8 15:17 [01;34m01.cufft_1d_r2c[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  6 22:41 [01;34m02.cufft_1d_r2c_runtime[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  5 23:21 [01;34m02.cufft_1d_r2c_runtime_2[0m
drwxr-xr-x 3 root root 4096  5월 22 10:28 [01;34m02.cufft_2d_r2c[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  9 13:10 [01;34m03.cufft_1d_conv[0m
drwxrwxr-x 4 kyp  kyp  4096  5월  8 15:15 [01;34m04.cufft_2d_conv[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  8 15:23 [01;34m05.cufftDX_1d_c2c[0m
drwxrwxr-x 4 kyp  kyp  4096  5월  7 23:01 [01;34m08.cufftDX_2d_conv[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# c[Kmv 02.cufft_1d_r2c_runtime[Ke[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[Kmv 02.cufft_2d_r2c/ 03.cufft_2d_r2c
[?2004l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# l
[?2004ltotal 36
drwxrwxr-x 3 kyp  kyp  4096  5월  3 00:10 [0m[01;34m00.cufft_1d_c2c[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  8 15:17 [01;34m01.cufft_1d_r2c[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  6 22:41 [01;34m02.cufft_1d_r2c_runtime[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  5 23:21 [01;34m02.cufft_1d_r2c_runtime_2[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  9 13:10 [01;34m03.cufft_1d_conv[0m
drwxr-xr-x 3 root root 4096  5월 22 10:28 [01;34m03.cufft_2d_r2c[0m
drwxrwxr-x 4 kyp  kyp  4096  5월  8 15:15 [01;34m04.cufft_2d_conv[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  8 15:23 [01;34m05.cufftDX_1d_c2c[0m
drwxrwxr-x 4 kyp  kyp  4096  5월  7 23:01 [01;34m08.cufftDX_2d_conv[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# mv 03.cufft_2d_r2c/ 04.cufft_2d_r2c
[?2004l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# l
[?2004ltotal 36
drwxrwxr-x 3 kyp  kyp  4096  5월  3 00:10 [0m[01;34m00.cufft_1d_c2c[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  8 15:17 [01;34m01.cufft_1d_r2c[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  6 22:41 [01;34m02.cufft_1d_r2c_runtime[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  5 23:21 [01;34m02.cufft_1d_r2c_runtime_2[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  9 13:10 [01;34m03.cufft_1d_conv[0m
drwxrwxr-x 4 kyp  kyp  4096  5월  8 15:15 [01;34m04.cufft_2d_conv[0m
drwxr-xr-x 3 root root 4096  5월 22 10:28 [01;34m04.cufft_2d_r2c[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  8 15:23 [01;34m05.cufftDX_1d_c2c[0m
drwxrwxr-x 4 kyp  kyp  4096  5월  7 23:01 [01;34m08.cufftDX_2d_conv[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# mv 04.cufft_2d_conv 05.cufft_2d_conv
[?2004l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# l
[?2004ltotal 36
drwxrwxr-x 3 kyp  kyp  4096  5월  3 00:10 [0m[01;34m00.cufft_1d_c2c[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  8 15:17 [01;34m01.cufft_1d_r2c[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  6 22:41 [01;34m02.cufft_1d_r2c_runtime[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  5 23:21 [01;34m02.cufft_1d_r2c_runtime_2[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  9 13:10 [01;34m03.cufft_1d_conv[0m
drwxr-xr-x 3 root root 4096  5월 22 10:28 [01;34m04.cufft_2d_r2c[0m
drwxrwxr-x 4 kyp  kyp  4096  5월  8 15:15 [01;34m05.cufft_2d_conv[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  8 15:23 [01;34m05.cufftDX_1d_c2c[0m
drwxrwxr-x 4 kyp  kyp  4096  5월  7 23:01 [01;34m08.cufftDX_2d_conv[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# mv 05.cufftc[KDX_1d_c2c/ 06.cufftDX_1d_c2c
[?2004l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# fl
[?2004lfl: command not found
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# l
[?2004ltotal 36
drwxrwxr-x 3 kyp  kyp  4096  5월  3 00:10 [0m[01;34m00.cufft_1d_c2c[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  8 15:17 [01;34m01.cufft_1d_r2c[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  6 22:41 [01;34m02.cufft_1d_r2c_runtime[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  5 23:21 [01;34m02.cufft_1d_r2c_runtime_2[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  9 13:10 [01;34m03.cufft_1d_conv[0m
drwxr-xr-x 3 root root 4096  5월 22 10:28 [01;34m04.cufft_2d_r2c[0m
drwxrwxr-x 4 kyp  kyp  4096  5월  8 15:15 [01;34m05.cufft_2d_conv[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  8 15:23 [01;34m06.cufftDX_1d_c2c[0m
drwxrwxr-x 4 kyp  kyp  4096  5월  7 23:01 [01;34m08.cufftDX_2d_conv[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# mv 087[K.cufftDX_2d_conv/ 07.cufftDX_2d_conv
[?2004l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# l
[?2004ltotal 36
drwxrwxr-x 3 kyp  kyp  4096  5월  3 00:10 [0m[01;34m00.cufft_1d_c2c[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  8 15:17 [01;34m01.cufft_1d_r2c[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  6 22:41 [01;34m02.cufft_1d_r2c_runtime[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  5 23:21 [01;34m02.cufft_1d_r2c_runtime_2[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  9 13:10 [01;34m03.cufft_1d_conv[0m
drwxr-xr-x 3 root root 4096  5월 22 10:28 [01;34m04.cufft_2d_r2c[0m
drwxrwxr-x 4 kyp  kyp  4096  5월  8 15:15 [01;34m05.cufft_2d_conv[0m
drwxrwxr-x 3 kyp  kyp  4096  5월  8 15:23 [01;34m06.cufftDX_1d_c2c[0m
drwxrwxr-x 4 kyp  kyp  4096  5월  7 23:01 [01;34m07.cufftDX_2d_conv[0m
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution# cd 04.cufft_2d_r2c/
[?2004l[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution/04.cufft_2d_r2c# l
[?2004ltotal 132
drwxr-xr-x 3 root root  4096  5월 22 10:28 [0m[01;34mbuild[0m
-rwxr-xr-x 1 root root   123  5월 22 10:28 [01;32mbuild.sh[0m
-rw-r--r-- 1 root root   702  5월 22 10:28 CMakeLists.txt
-rw-r--r-- 1 root root  1051  5월 22 10:28 gpuTimer.h
-rw-r--r-- 1 root root  7902  5월 22 10:28 main.cu
-rw-r--r-- 1 root root  4959  5월 22 10:28 main.cu.save
-rw-r--r-- 1 root root 99741  5월 22 10:28 matplotlibcpp.h
[?2004hroot@kypserver:/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution/04.cufft_2d_r2c# vim b[Kmain.cu
[?2004l[?1049h[?1h=[1;55r[23m[24m[0m[H[J[?25l[55;1H"main.cu" 272L, 7902B[2;1H▽[6n[2;1H  [3;1HPzz\[0%m[6n[3;1H           [1;1H[1;1H[35m#include [0m[31m<cstdio>[0m
[35m#include [0m[31m<iostream>[0m[2;20H[K[3;1H[35m#include [0m[31m<cufft.h>[0m[3;19H[K[4;1H[35m#include [0m[31m<cuda_runtime.h>[0m
[35m#include [0m[31m<vector>[0m

[35m#include [0m[31m"device_launch_parameters.h"[0m
[35m#include [0m[31m"gpuTimer.h"[0m
[35m#include [0m[31m"matplotlibcpp.h"[0m

[35m#define cudaCheckError() {                                  \
    [0m[32mcudaError_t[0m[35m e = cudaGetLastError();                     \
    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \
        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \
            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \
        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \
    }                                                       \
}                                                           \[0m

[32mnamespace[0m plt = matplotlibcpp;

[32mvoid[0m checkDeviceMemory([32mvoid[0m)
{
    [32msize_t[0m free, total;
    cudaMemGetInfo(&free, &total);
    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;
    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;
    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb << [31m"MB[0m[35m\n[0m[31m"[0m << s[0m>& x
    , std::vector<[32mfloat[0m>& k
    , [32mfloat[0m dx, [32mfloat[0m dk
    , [32mconst[0m [32mint[0m N
    )
{
    [33mfor[0m ([32mauto[0m i = [31m0[0m; i < N; ++i) {[40;9Hx[i] = (i - N / [31m2[0m) * dx;[41;9Hk[i] = (i - N / [31m2[0m) * dk;
    }
}

[32mvoid[0m generateGaussian(
    std::vector<[32mfloat[0m>& data
    , [32mfloat[0m x0 [34m// shift[0m
    , [32mfloat[0m dx
    , [32mfloat[0m sigma
    , [32mfloat[0m coeff = [31m1.f[0m
    )
{
    [32mint[0m N = data.size();
    [33mfor[0m ([32mauto[0m i = [31m0[0m; i < N; ++i) {[55;152H1,1[11CTop[1;1H[34h[?25h[?25l[55;142Hj[1;1H[55;142H [2;1H[55;152H2[2;1H[34h[?25h[?25l[55;142Hj[2;1H[55;142H [3;1H[55;152H3[3;1H[34h[?25h[?25l[55;142Hj[3;1H[55;142H [4;1H[55;152H4[4;1H[34h[?25h[?25l[55;142Hj[4;1H[55;142H [5;1H[55;152H5[5;1H[34h[?25h[?25l[55;142Hj[5;1H[55;142H [6;1H[55;152H6,0-1[6;1H[34h[?25h[?25l[55;142Hj[6;1H[55;142H [7;1H[55;152H7,1  [7;1H[34h[?25h[?25l[55;142Hj[7;1H[55;142H [8;1H[55;152H8[8;1H[34h[?25h[?25l[55;142Hj[8;1H[55;142H [9;1H[55;152H9[9;1H[34h[?25h[?25l[55;142Hj[9;1H[55;142H [10;1H[55;152H10,0-1[10;1H[34h[?25h[?25l[55;142Hj[10;1H[55;142H [11;1H[55;153H1,1  [11;1H[34h[?25h[?25l[55;142Hj[11;1H[55;142H [12;1H[55;153H2[12;1H[34h[?25h[?25l[55;142Hj[12;1H[55;142H [13;1H[55;153H3[13;1H[34h[?25h[?25l[55;142Hj[13;1H[55;142H [14;1H[55;153H4[14;1H[34h[?25h[?25l[55;142Hj[14;1H[55;142H [15;1H[55;153H5[15;1H[34h[?25h[?25l[55;142Hj[15;1H[55;142H [16;1H[55;153H6[16;1H[34h[?25h[?25l[55;142Hj[16;1H[55;142H [17;1H[55;153H7[17;1H[34h[?25h[?25l[55;142Hj[17;1H[55;142H [18;1H[11;26H[35m[46m{[18;1H}[0m[55;153H8[18;1H[34h[?25h[?25l[55;142Hj[18;1H[55;142H [19;1H[11;26H[35m{[18;1H}[0m[55;153H9,0-1[19;1H[34h[?25h[?25l[55;142Hj[19;1H[55;142H [20;1H[55;152H20,1  [20;1H[34h[?25h[?25l[55;142Hj[20;1H[55;142H [21;1H[55;153H1,0-1[21;1H[34h[?25h[?25l[55;142Hj[21;1H[55;142H [22;1H[55;153H2,1  [22;1H[34h[?25h[?25l[55;142Hj[22;1H[55;142H [23;1H[46m{[30;1H}[0m[55;153H3[23;1H[34h[?25h[?25l[55;142Hj[23;1H[55;142H [24;1H[23;1H{[30;1H}[55;153H4[24;1H[34h[?25h[?25l[55;142Hj[24;1H[55;142H [25;1H[55;153H5[25;1H[34h[?25h[?25l[55;142Hj[25;1H[55;142H [26;1H[55;153H6[26;1H[34h[?25h[?25l[55;142Hj[26;1H[55;142H [27;1H[55;153H7[27;1H[34h[?25h[?25l[55;142Hj[27;1H[55;142H [28;1H[55;153H8[28;1H[34h[?25h[?25l[55;142Hj[28;1H[55;142H [29;1H[55;153H9[29;1H[34h[?25h[?25l[55;142Hj[29;1H[55;142H [30;1H[23;1H[46m{[30;1H}[0m[55;152H30[30;1H[34h[?25h[?25l[55;142Hj[30;1H[55;142H [31;1H[23;1H{[30;1H}[55;153H1,0-1[31;1H[34h[?25h[?25l[55;142Hj[31;1H[55;142H [32;1H[55;153H2,1  [32;1H[34h[?25h[?25l[55;142Hj[32;1H[55;142H [33;1H[55;153H3[33;1H[34h[?25h[?25l[55;142Hj[33;1H[55;142H [34;1H[55;153H4[34;1H[34h[?25h[?25l[55;142Hj[34;1H[55;142H [35;1H[55;153H5[35;1H[34h[?25h[?25l[55;142Hj[35;1H[55;142H [36;1H[55;153H6[36;1H[34h[?25h[?25l[55;142Hj[36;1H[55;142H [37;1H[55;153H7[37;1H[34h[?25h[?25l[55;142Hj[37;1H[55;142H [38;1H[46m{[43;1H}[0m[55;153H8[38;1H[34h[?25h[?25l[55;142Hj[38;1H[55;142H [39;1H[38;1H{[43;1H}[55;153H9[39;1H[34h[?25h[?25l[55;142Hj[39;1H[55;142H [40;1H[55;152H40[40;1H[34h[?25h[?25l[55;142Hj[40;1H[55;142H [41;1H[55;153H1[41;1H[34h[?25h[?25l[55;142Hj[41;1H[55;142H [42;1H[55;153H2[42;1H[34h[?25h[?25l[55;142Hj[42;1H[55;142H [43;1H[38;1H[46m{[43;1H}[0m[55;153H3[43;1H[34h[?25h[?25l[55;142Hj[43;1H[55;142H [44;1H[38;1H{[43;1H}[55;153H4,0-1[44;1H[34h[?25h[?25l[55;142Hj[44;1H[55;142H [45;1H[55;153H5,1  [45;1H[34h[?25h[?25l[55;142Hj[45;1H[55;142H [46;1H[55;153H6[46;1H[34h[?25h[?25l[55;142Hj[46;1H[55;142H [47;1H[55;153H7[47;1H[34h[?25h[?25l[55;142Hj[47;1H[55;142H [48;1H[55;153H8[48;1H[34h[?25h[?25l[55;142Hj[48;1H[55;142H [49;1H[55;153H9[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;9H[32mauto[0m x = (i - N / [31m2[0m) * dx;[55;1H[K[55;152H50,1[11C0%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;9Hdata[i] = coeff * expf(- (x - x0) * (x - x0) / ([31m2.f[0m * sigma * sigma));[55;152H[K[55;152H51,1[11C0%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5H}[55;152H[K[55;152H52,1[11C1%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H}[55;152H[K[55;152H53,1[11C1%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[55;152H[K[55;152H54,1[11C2%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H[32mvoid[0m generateAnswer([55;152H[K[55;152H55,1[11C2%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5Hstd::vector<[32mfloat[0m>& real[55;152H[K[55;152H56,1[11C3%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5H, std::vector<[32mfloat[0m>& imag[55;152H[K[55;152H57,1[11C3%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[43;1H[46m{[49;1H}[0m[54;5H, [32mfloat[0m x0[55;152H[K[55;152H58,1[11C4%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[42;1H{[48;1H}[54;5H, [32mfloat[0m dk[55;152H[K[55;152H59,0-1[9C4%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5H, [32mfloat[0m sigma[55;152H[K[55;152H60,1[11C5%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5H, [32mfloat[0m coeff[55;152H[K[55;152H61,1[11C5%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5H)[55;152H[K[55;152H62,1[11C5%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H{[55;152H[K[55;152H63,1[11C6%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5H[32mint[0m N = real.size();[55;152H[K[55;152H64,1[11C6%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5H[33mfor[0m ([32mauto[0m i = [31m0[0m; i < N; ++i) {[55;152H[K[55;152H65,1[11C7%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;9H[32mauto[0m k = (i - N / [31m2[0m) * dk;[55;152H[K[55;152H66,1[11C7%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;9H[32mauto[0m envelope = coeff * expf(- [31m0.5f[0m * k * k * sigma * sigma);[55;152H[K[55;152H67,1[11C8%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;9Hreal[i] = envelope * cosf(-k*x0);[55;152H[K[55;152H68,1[11C8%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;9Himag[i] = envelope * sinf(-k*x0);[55;152H[K[55;152H69,1[11C9%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5H}[55;152H[K[55;152H70,1[11C9%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H}[55;152H[K[55;152H71,1[10C10%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[55;152H[K[55;152H72,1[10C10%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H[32m__global__[0m [32mvoid[0m toComplex([32mconst[0m [32mfloat[0m* data, cufftComplex* out, [32mint[0m N) {[55;152H[K[55;152H73,1[10C11%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5H[32mint[0m i = [36mblockIdx[0m.x * [36mblockDim[0m.x + [36mthreadIdx[0m.x;[55;152H[K[55;152H74,1[10C11%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5H[33mif[0m (i < N) {[55;152H[K[55;152H75,1[10C11%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[41;1H[46m{[49;1H}[0m[54;9Hout[i].x = data[i];[55;152H[K[55;152H76,1[10C12%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[40;1H{[48;1H}[54;9Hout[i].y = [31m0.f[0m;[55;152H[K[55;152H77,0-1[8C12%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5H}[55;152H[K[55;152H78,1[10C13%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H}[55;152H[K[55;152H79,1[10C13%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[55;152H[K[55;152H80,1[10C14%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H[32mtemplate[0m<[32mtypename[0m T>[55;152H[K[55;152H81,1[10C14%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H[32m__global__[0m [32mvoid[0m fftshift(T* data, [32mint[0m N) {[55;152H[K[55;152H82,1[10C15%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5H[32mint[0m i = [36mblockIdx[0m.x * [36mblockDim[0m.x + [36mthreadIdx[0m.x;[55;152H[K[55;152H83,1[10C15%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[43;72H[46m{[49;1H}[0m[54;5H[33mif[0m (i < N / [31m2[0m) {[55;152H[K[55;152H84,1[10C16%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[42;72H{[48;1H}[54;9HT tmp = data[i];[55;152H[K[55;152H85,0-1[8C16%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;9Hdata[i] = data[i + N / [31m2[0m];[55;152H[K[55;152H86,1[10C16%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;9Hdata[i + N / [31m2[0m] = tmp;[55;152H[K[55;152H87,1[10C17%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5H}[55;152H[K[55;152H88,1[10C17%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H}[55;152H[K[55;152H89,1[10C18%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[55;152H[K[55;152H90,1[10C18%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H[32mtemplate[0m<[32mtypename[0m T>[55;152H[K[55;152H91,1[10C19%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H[32m__global__[0m [32mvoid[0m scale(T* input, T scale, [32mint[0m N) {[55;152H[K[55;152H92,1[10C19%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5H[32mint[0m i = [36mblockIdx[0m.x * [36mblockDim[0m.x + [36mthreadIdx[0m.x;[55;152H[K[55;152H93,1[10C20%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[42;42H[46m{[49;1H}[0m[54;5H[33mif[0m (i < N) {[55;152H[K[55;152H94,1[10C20%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[41;42H{[48;1H}[54;9Hinput[i] = scale * input[i];[55;152H[K[55;152H95,0-1[8C21%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5H}[55;152H[K[55;152H96,1[10C21%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H}[55;152H[K[55;152H97,1[10C22%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[55;152H[K[55;152H98,1[10C22%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H[32mtemplate[0m<>[55;152H[K[55;152H99,1[10C22%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H[32m__global__[0m [32mvoid[0m scale<cufftComplex>(cufftComplex* input, cufftComplex scale, [32mint[0m N) {[55;152H[K[55;152H100,1[9C23%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5H[32mint[0m i = [36mblockIdx[0m.x * [36mblockDim[0m.x + [36mthreadIdx[0m.x;[55;152H[K[55;152H101,1[9C23%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[44;49H[46m{[49;1H}[0m[54;5H[33mif[0m (i < N) {[55;152H[K[55;152H102,1[9C24%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[43;49H{[48;1H}[54;9Hinput[i] = cuCmulf(input[i], scale);[55;152H[K[55;152H103,0-1[7C24%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5H}[55;152H[K[55;152H104,1[9C25%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H}[55;152H[K[55;152H105,1[9C25%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[55;152H[K[55;152H106,1[9C26%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H[34m// in-place[0m[55;152H[K[55;152H107,1[9C26%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H[32mtemplate[0m<[32mtypename[0m T>[55;152H[K[55;152H108,1[9C27%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H[32mvoid[0m api_fftshift(T* input, [32mconst[0m [32mint[0m size, cudaStream_t stream) {[55;152H[K[55;152H109,1[9C27%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[44;85H[46m{[49;1H}[0m[54;5H[32mint[0m block_size = [31m128[0m;[55;152H[K[55;152H110,1[9C27%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[43;85H{[48;1H}[54;5H[32mint[0m grid_shift = (size / [31m2[0m + block_size - [31m1[0m) / block_size;[55;152H[K[55;152H111,0-1[7C28%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5Hfftshift<<<grid_shift, block_size, [31m0[0m, stream>>>(input, size); [34m// which one is faster? thrust? or this?[0m[55;152H[K[55;152H112,1[9C28%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H}[55;152H[K[55;152H113,1[9C29%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[55;152H[K[55;152H114,1[9C29%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H[34m// in-place[0m[55;152H[K[55;152H115,1[9C30%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H[32mtemplate[0m<[32mtypename[0m T>[55;152H[K[55;152H116,1[9C30%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H[32mvoid[0m api_scale(T* input, T scale_coeff, [32mconst[0m [32mint[0m size, cudaStream_t stream) {[55;152H[K[55;152H117,1[9C31%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[45;66H[46m{[49;1H}[0m[54;5H[32mint[0m block_size = [31m128[0m;[55;152H[K[55;152H118,1[9C31%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[44;66H{[48;1H}[54;5H[32mint[0m grid_size = (size + block_size - [31m1[0m) / block_size;[55;152H[K[55;152H119,0-1[7C32%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5Hscale<<<grid_size, block_size, [31m0[0m, stream>>>(input, scale_coeff, size);[55;152H[K[55;152H120,1[9C32%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H}[55;152H[K[55;152H121,1[9C33%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[55;152H[K[55;152H122,1[9C33%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H[34m// out-of-place[0m[55;152H[K[55;152H123,1[9C33%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H[32mvoid[0m api_makeComplex([32mconst[0m [32mfloat[0m* in, cufftComplex* out, [32mconst[0m [32mint[0m size, cudaStream_t stream) {[55;152H[K[55;152H124,1[9C34%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5H[32mint[0m block_size = [31m128[0m;[55;152H[K[55;152H125,1[9C34%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[45;78H[46m{[49;1H}[0m[54;5H[32mint[0m grid_size = (size + block_size - [31m1[0m) / block_size;[55;152H[K[55;152H1l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[45;78H[46m{[49;1H}[0m[54;5H[32mint[0m grid_size = (size + block_size - [31m1[0m) / block_size;[55;152H[K[55;152H126,1[9C35%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[44;78H{[48;1H}[54;5HtoComplex<<<grid_size, block_size, [31m0[0m, stream>>>(in, out, size);[55;152H[K[55;152H127,0-1[7C35%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H}[55;152H[K[55;152H128,1[9C36%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[55;152H[K[55;152H129,1[9C36%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H[34m// out-of-place[0m[55;152H[K[55;152H130,1[9C37%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H[32mvoid[0m api_fftC2C(cufftComplex* in, cufftComplex* out, [32mconst[0m [32mint[0m size, [32mint[0m direction, cudaStream_t stream) {[55;152H[K[55;152H131,1[9C37%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5HcufftHandle plan;[55;152H[K[55;152H132,1[9C38%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[45;95H[46m{[49;1H}[0m[54;5HcufftPlan1d(&plan, size, CUFFT_C2C, [31m1[0m);[55;152H[K[55;152H133,1[9C38%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[44;95H{[48;1H}[54;5HcufftSetStream(plan, stream);[55;152H[K[55;152H134,1[9C38%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5HcufftExecC2C(plan, in, out, direction);[55;152H[K[55;152H135,1[9C39%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5HcufftDestroy(plan);[55;152H[K[55;152H136,1[9C39%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H}[55;152H[K[55;152H137,1[9C40%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[55;152H[K[55;152H138,1[9C40%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H[34m// out-of-place[0m[55;152H[K[55;152H139,1[9C41%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H[32mvoid[0m api_fftR2C([32mfloat[0m* in, cufftComplex* out, [32mconst[0m [32mint[0m size, [32mint[0m direction, cudaStream_t stream) {[55;152H[K[55;152H140,1[9C41%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5HcufftHandle plan;[55;152H[K[55;152H141,1[9C42%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[43;106H[46m{[49;1H}[0m[54;5HcufftPlan1d(&plan, size, CUFFT_R2C, [31m1[0m);[55;152H[K[55;152H142,1[9C42%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[42;106H{[48;1H}[54;5HcufftSetStream(plan, stream);[55;152H[K[55;152H143,0-1[7C43%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5HcufftExecR2C(plan, in, out); [34m// R2C has no direction[0m[55;152H[K[55;152H144,1[9C43%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;5HcufftDestroy(plan);[55;152H[K[55;152H145,1[9C44%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[54;1H}[55;152H[K[55;152H146,1[9C44%[49;1H[34h[?25h[?25l[55;142Hj[49;1H[55;142H [49;1H[1;54r[54;1H
[1;55r[55;152H[K[55;152H147,1[9C44%[49;1H[34h[?25h[?25l[55;142H:[49;1H[55;142H[K[55;1H:[34h[?25hser [?25l[34h[?25h[?25l[55;4H[K[55;4H[34h[?25ht number[?25l[49;5H[1;1H[33m 99 [0m    [33mif[0m (i < N) {
[33m100 [0m        input[i] = scale * input[i];
[33m101 [0m    }
[33m102 [0m}
[33m103 
104 [0m[32mtemplate[0m<>
[33m105 [0m[32m__global__[0m [32mvoid[0m scale<cufftComplex>(cufftComplex* input, cufftComplex scale, [32mint[0m N) {
[33m106 [0m    [32mint[0m i = [36mblockIdx[0m.x * [36mblockDim[0m.x + [36mthre[0m
[33m113 [0m[32mtemplate[0m<[32mtypename[0m T>
[33m114 [0m[32mvoid[0m api_fftshift(T* input, [32mconst[0m [32mint[0m size, cudaStream_t stream) {
[33m115 [0m    [32mint[0m block_size = [31m128[0m;
[33m116 [0m    [32mint[0m grid_shift = (size / [31m2[0m + block_size - [31m1[0m) / block_size;
[33m117 [0m    fftshift<<<grid_shift, block_size, [31m0[0m, stream>>>(input, size); [34m// which one is faster? thrust? or this?[0m
[33m118 [0m}
[33m119 
120 [0m[34m// in-place[0m
[33m121 [0m[32mtemplate[0m<[32mtypename[0m T>
[33m122 [0m[32mvoid[0m api_scale(T* input, T scale_coeff, [32mconst[0m [32mint[0m size, cudaStream_t stream) {
[33m123 [0m    [32mint[0m block_size = [31m128[0m;
[33m124 [0m    [32mint[0m grid_size = (size + block_size - [31m1[0m) / block_size;
[33m125 [0m    scale<<<grid_size, block_size, [31m0[0m, stream>>>(input, scale_coeff, size);
[33m126 [0m}
[33m127 
128 [0m[34m// out-of-place[0m
[33m129 [0m[32mvoid[0m api_makeComplex([32mconst[0m [32mfloat[0m* in, cufftComplex* out, [32mconst[0m [32mint[0m size, cudaStream_t stream) {
[33m130 [0m    [32mint[0m block_size = [31m128[0m;
[33m131 [0m    [32mint[0m grid_size = (size + block_size - [31m1[0m) / block_size;
[33m132 [0m    toComplex<<<grid_size, block_size, [31m0[0m, stream>>>(in, out, size);
[33m133 [0m}
[33m134 
135 [0m[34m// out-of-place[0m
[33m136 [0m[32mvoid[0m api_fftC2C(cufftComplex* in, cufftComplex* out, [32mconst[0m [32mint[0m size, [32mint[0m direction, cudaStream_t stream) {
[33m137 [0m    cufftHandle plan;
[33m138 [0m    cufftPlan1d(&plan, size, CUFFT_C2C, [31m1[0m);
[33m139 [0m    cufftSetStream(plan, stream);
[33m140 [0m    cufftExecC2C(plan, in, out, direction);
[33m141 [0m    cufftDestroy(plan);
[33m142 [0m}
[33m143 
144 [0m[34m// out-of-place[0m
[33m145 [0m[32mvoid[0m api_fftR2C([32mfloat[0m* in, cufftComplex* out, [32mconst[0m [32mint[0m size, [32mint[0m direction, cudaStream_t stream) {
[33m146 [0m    cufftHandle plan;
[33m147 [0m    cufftPlan1d(&plan, size, CUFFT_R2C, [31m1[0m);
[33m148 [0m    cufftSetStream(plan, stream);
[33m149 [0m    cufftExecR2C(plan, in, out); [34m// R2C has no direction[0m
[33m150 [0m    cufftDestroy(plan);
[33m151 [0m}
[33m152 [0m[55;152H147,1[9C44%[49;5H[34h[?25h[?25l[55;142Hk[49;5H[55;142H [48;5H[55;154H6[48;5H[34h[?25h[?25l[55;142Hk[48;5H[55;142H [47;5H[55;154H5[47;5H[34h[?25h[?25l[55;142Hk[47;5H[55;142H [46;5H[55;154H4[46;5H[34h[?25h[?25l[55;142Hk[46;5H[55;142H [45;5H[55;154H3,0-1[45;5H[34h[?25h[?25l[55;142Hk[45;5H[55;142H [44;5H[38;110H[46m{[44;5H}[0m[55;154H2,1  [44;5H[34h[?25h[?25l[55;142Hk[44;5H[55;142H [43;5H[38;110H{[44;5H}[55;154H1[43;5H[34h[?25h[?25l[55;142Hk[43;5H[55;142H [42;5H[55;154H0[42;5H[34h[?25h[?25l[55;142Hk[42;5H[55;142H [41;5H[55;153H39[41;5H[34h[?25h[?25l[55;142Hk[41;5H[55;142H [40;5H[55;154H8[40;5H[34h[?25h[?25l[55;142Hk[40;5H[55;142H [39;5H[55;154H7[39;5H[34h[?25h[?25l[55;142Hk[39;5H[55;142H [38;5H[55;154H6[38;5H[34h[?25h[?25l[55;142Hk[38;5H[55;142H [37;5H[55;154H5[37;5H[34h[?25h[?25l[55;142Hk[37;5H[55;142H [36;5H[55;154H4[36;5H[34h[?25h[?25l[55;142Hk[36;5H[55;142H [35;5H[31;99H[46m{[35;5H}[0m[55;154H3[35;5H[34h[?25h[?25l[55;142Hk[35;5H[55;142H [34;5H[31;99H{[35;5H}[55;154H2[34;5H[34h[?25h[?25l[55;142Hk[34;5H[55;142H [33;5H[55;154H1[33;5H[34h[?25h[?25l[55;142Hk[33;5H[55;142H [32;5H[55;154H0[32;5H[34h[?25h[?25l[55;142Hk[32;5H[55;142H [31;5H[55;153H29[31;5H[34h[?25h[?25l[55;142Hk[31;5H[55;142H [30;5H[55;154H8[30;5H[34h[?25h[?25l[55;142Hk[30;5H[55;142H [29;5H[55;154H7,0-1[29;5H[34h[?25h[?25l[55;142Hk[29;5H[55;142H [28;5H[24;82H[46m{[28;5H}[0m[55;154H6,1  [28;5H[34h[?25h[?25l[55;142Hk[28;5H[55;142H [27;5H[24;82H{[28;5H}[55;154H5[27;5H[34h[?25h[?25l[55;142Hk[27;5H[55;142H [26;5H[55;154H4[26;5H[34h[?25h[?25l[55;142Hk[26;5H[55;142H [25;5H[55;154H3[25;5H[34h[?25h[?25l[55;142Hk[25;5H[55;142H [24;5H[55;154H2[24;5H[34h[?25h[?25l[55;142Hk[24;5H[55;142H [23;5H[55;154H1[23;5H[34h[?25h[?25l[55;142Hk[23;5H[55;142H [22;5H[55;154H0[22;5H[34h[?25h[?25l[55;142Hk[22;5H[55;142H [21;5H[55;153H19,0-1[21;5H[34h[?25h[?25l[55;142Hk[21;5H[55;142H [20;5H[16;70H[46m{[20;5H}[0m[55;154H8,1  [20;5H[34h[?25h[?25l[55;142Hk[20;5H[55;142H [19;5H[16;70H{[20;5H}[55;154H7[19;5H[34h[?25h[?25l[55;142Hk[19;5H[55;142H [18;5H[55;154H6[18;5H[34h[?25h[?25l[55;142Hk[18;5H[55;142H [17;5H[55;154H5[17;5H[34h[?25h[?25l[55;142Hk[17;5H[55;142H [16;5H[55;154H4[16;5H[34h[?25h[?25l[55;142Hk[16;5H[55;142H [15;5H[55;154H3[15;5H[34h[?25h[?25l[55;142Hk[15;5H[55;142H [14;5H[55;154H2[14;5H[34h[?25h[?25l[55;142Hk[14;5H[55;142H [13;5H[55;154H1,0-1[13;5H[34h[?25h[?25l[55;142Hk[13;5H[55;142H [12;5H[7;89H[46m{[12;5H}[0m[55;154H0,1  [12;5H[34h[?25h[?25l[55;142Hk[12;5H[55;142H [11;5H[7;89H{[12;5H}[55;153H09[11;5H[34h[?25h[?25l[55;142Hk[11;5H[55;142H [10;5H[55;154H8[10;5H[34h[?25h[?25l[55;142Hk[10;5H[55;142H [9;5H[55;154H7[9;5H[34h[?25h[?25l[55;142Hk[9;5H[55;142H [8;5H[55;154H6[8;5H[34h[?25h[?25l[55;142Hk[8;5H[55;142H [7;5H[55;154H5[7;5H[34h[?25h[?25l[55;142Hk[7;5H[55;142H [6;5H[55;154H4[6;5H[34h[?25h[?25l[55;142Hk[6;5H[55;142H [6;5H[1;54r[1;1H[L[1;55r[1;1H[33m 98 [0m    [32mint[0m i = [36mblockIdx[0m.x * [36mblockDim[0m.x + [36mthreadIdx[0m.x;[55;1H[K[55;152H103,0-1[7C44%[6;5H[34h[?25h[?25l[55;142Hj[6;5H[55;142H [7;5H[55;154H4,1  [7;5H[34h[?25h[?25l[55;142Hj[7;5H[55;142H [8;5H[55;154H5[8;5H[34h[?25h[?25l[55;142Hj[8;5H[55;142H [9;5H[55;154H6[9;5H[34h[?25h[?25l[55;142Hj[9;5H[55;142H [10;5H[55;154H7[10;5H[34h[?25h[?25l[55;142H:[10;5H[55;142H[K[55;1H:[34h[?25hvs ...[?25lCMakeLists.txt...[55;5H[K[55;5H[54;1H[1m[7mCMakeLists.txt  build/  build.sh  gpuTimer.h  main.cu  main.cu.save  matplotlibcpp.h                                                                                     [0m[55;5H...[54;1H[30m[43mCMakeLists.txt[0m
:vs CMakeLists.txt[34h[?25h...[54;1H[1m[7mCMakeLists.txt[0m[2C[30m[43mbuild/[?25l[0m
:vs build/[55;11H[K[55;11H[34h[?25h[?25l[54;1H[33m151 [0m}[54;6H[K[55;152H107,1[9C44%[55;152H[K[55;10H[K[55;10H[34h[?25h[?25l[55;9H[K[55;9H[34h[?25h[?25l[55;8H[K[55;8H[34h[?25h[?25l[55;7H[K[55;7H[34h[?25h[?25l[55;6H[K[55;6H[34h[?25h[?25l[55;5H[K[55;5H[34h[?25h../...[?25l00.cufft_1d_c2c/...[55;8H[K[55;8H[54;1H[1m[7m00.cufft_1d_c2c/  01.cufft_1d_r2c/  02.cufft_1d_r2c_runtime/  02.cufft_1d_r2c_runtime_2/  03.cufft_1d_conv/  04.cufft_2d_r2c/  05.cufft_2d_conv/  06.cufftDX_1d_c2c/  >  [0m[55;8H...[54;1H[30m[43m00.cufft_1d_c2c/[0m[55;8H00.cufft_1d_c2c/[34h[?25h...[54;1H[1m[7m00.cufft_1d_c2c/[0m[2C[30m[43m01.cufft_1d_r2c/[?25l[0m[55;9H1[10Cr[55;24H[K[55;24H[34h[?25h...[54;19H[1m[7m01.cufft_1d_r2c/[0m[2C[30m[43m02.cufft_1d_r2c_runtime/[?25l[0m[55;9H2[13C_runtime/[34h[?25h...[54;37H[1m[7m02.cufft_1d_r2c_runtime/[0m[2C[30m[43m02.cufft_1d_r2c_runtime_2/[?25l[0m[55;31H_2/[55;34H[K[55;34H[34h[?25h...[54;63H[1m[7m02.cufft_1d_r2c_runtime_2/[0m[2C[30m[43m03.cufft_1d_conv/[?25l[0m[55;9H3[10Cconv/[55;25H[K[55;25H[34h[?25h...[54;91H[1m[7m03.cufft_1d_conv/[0m[2C[30m[43m04.cufft_2d_r2c/[?25l[0m[55;9H4[7C2d_r2c/[55;24H[K[55;24H[34h[?25h...[54;110H[1m[7m04.cufft_2d_r2c/[0m[2C[30m[43m05.cufft_2d_conv/[?25l[0m[55;9H5[10Cconv/[55;25H[K[55;25H[34h[?25h[?25l[54;1H[33m151 [0m}[54;6H[K[55;152H107,1[9C44%[55;152H[K[55;1H"../05.cufft_2d_conv/" is a directory[1;1H[34m" ============================================================================[0m      [7m|[0m
[34m" Netrw Directory Listing                              is a directory[1;1H[34m" ============================================================================[0m      [7m|[0m
[34m" Netrw Directory Listing                                        [0m[36m(netrw v171)[0m[7C[7m|[0m
[34m"   /home/kyp/Workspace/00.test/Test/cuda_study/08.convolution/05.cufft_2d_conv[0m     [7m|[0m
[34m"   Sorted by[0m[33m      name[0m[61C[7m|[0m
[34m"   Sort sequence:[0m[33m [\/]$[0m[34m,[0m[33m\<core\%(\.\d\+\)\=\>[0m[34m,[0m[33m\.h$[0m[34m,[0m[33m\.c$[0m[34m,[0m[33m\.cpp$[0m[34m,[0m[33m\~\=\*$[0m[34m,[0m[33m*[0m[34m,[0m[33m\.o$[0m[34m,[0m[33m\.obj[0m[7m|[0m
[34m"   Quick Help: [0m[36m<F1>[0m[35m:[0mhelp  [36m-[0m[35m:[0mgo up dir  [36mD[0m[35m:[0mdelete  [36mR[0m[35m:[0mrename  [36ms[0m[35m:[0msort-by  [36mx[0m[35m:[0mspecial    [7m|[0m
[34m" ==============================================================================[0m    [7m|[0m
[4m[34m..[24m[0m[4m[36m/[24m[0m[4m                                                                                 [24m[7m|[0m
[34m.[0m[36m/[0m                                                    [30C[7m|[0m
[34mbuild[0m[36m/[0m              [64C[7m|[0m
[34mbuild_debug[0m[36m/[0m                                    [36C[7m|[0m
gpuTimer.h[74C[7m|[0m
matplotlibcpp.h[69C[7m|[0m
[35mbuild.sh[0m[36m*[0m[75C[7m|[0m
[35mbuild_debug.sh[0m[36m*[0m[69C[7m|[0m
[35mrun.sh[0m[36m*[0m                 [60C[7m|[0m
[35mrun_debug.sh[0m[36m*[0m                                                         [14C[7m|[0m
1                            [55C[7m|[0m
CMakeLists.txt                                                    [18C[7m|[0m
main.cu                                                                             [7m|[21;85H|[22;85H|[23;85H|[24;85H|[25;85H|[26;85H|[27;85H|[28;85H|[29;85H|[30;85H|[31;85H|[32;85H|[33;85H|[34;85H|[35;85H|[36;85H|[37;85H|[38;85H|[39;85H|[40;85H|[41;85H|[42;85H|[43;85H|[44;85H|[45;85H|[46;85H|[47;85H|[48;85H|[49;85H|[50;85H|[51;85H|[52;85H|[53;85H|[0m[21;1H[1m[34m~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   
~                                                                                   [0m
[1m[7m<ace/00.test/Test/cuda_study/08.convolution/05.cufft_2d_conv [RO] 8,1            All [0m[1;86H[33m 98 [0m    [32mint[0m i = [36mblockIdx[0m.x * [36mblockDim[0m.x + [36mthreadIdx[0m.x;[2;86H[33m 99 [0m    [33mif[0m (i < N) {[3;86H[33m100 [0m[8Cinput[i] = scale * input[i];[4;86H[33m101 [0m    }[5;86H[33m102 [0m}[6;86H[33m103 [7;86H104 [0m[32mtemplate[0m<>[8;86H[33m105 [0m[32m__global__[0m [32mvoid[0m scale<cufftComplex>(cufftComplex* input, cufftComplex scale, [32mint[0m[9;86H[33m    [0m N) {[10;86H[33m106 [0m    [32mint[0m i = [36mblockIdx[0m.x * [36mblockDim[0m.x + [36mthreadIdx[0m.x;[11;86H[33m107 [0m    [33mif[0m (i < N) {[12;86H[33m108 [0m[8Cinput[i] = cuCmulf(input[i], scale);[13;86H[33m109 [0m    }[14;86H[33m110 [0m}[15;86H[33m111 [16;86H112 [0m[34m// in-place[0m[17;86H[33m113 [0m[32mtemplate[0m<[32mtypename[0m T>[18;86H[33m114 [0m[32mvoid[0m api_fftshift(T* input, [32mconst[0m [32mint[0m size, cudaStream_t stream) {[19;86H[33m115 [0m    [32mint[0m block_size = [31m128[0m;[20;86H[33m116 [0m    [32mint[0m grid_shift = (size / [31m2[0m + block_size - [31m1[0m) / block_size;[21;86H[33m117 [0m    fftshift<<<grid_shift, block_size, [31m0[0m, stream>>>(input, size); [34m// which one i[0m[22;86H[33m    [0m[34ms faster? thrust? or this?[0m[23;86H[33m118 [0m}[24;86H[33m119 [25;86H120 [0m[34m// in-place[0m[26;86H[33m121 [0m[32mtemplate[0m<[32mtypename[0m T>[27;86H[33m122 [0m[32mvoid[0m api_scale(T* input, T scale_coeff, [32mconst[0m [32mint[0m size, cudaStream_t stream) {[28;86H[33m123 [0m    [32mint[0m block_size = [31m128[0m;[29;86H[33m124 [0m    [32mint[0m grid_size = (size + block_size - [31m1[0m) / block_size;[30;86H[33m125 [0m    scale<<<grid_size, block_size, [31m0[0m, stream>>>(input, scale_coeff, size);[31;86H[33m126 [0m}[32;86H[33m127 [0m[32;91H[K[33;86H[33m128 [0m[34m// out-of-place[0m[34;86H[33m129 [0m[32mvoid[0m api_makeComplex([32mconst[0m [32mfloat[0m* in, cufftComplex* out, [32mconst[0m [32mint[0m size, cudaStr[35;86H[33m    [0meam_t stream) {[36;86H[33m130 [0m    [32mint[0m block_size = [31m128[0m;[37;86H[33m131 [0m    [32mint[0m grid_size = (size + block_size - [31m1[0m) / block_size;[38;86H[33m132 [0m    toComplex<<<grid_size, block_size, [31m0[0m, stream>>>(in, out, size);[39;86H[33m133 [0m}[39;91H[K[40;86H[33m134 [41;86H135 [0m[34m// out-of-place[0m[42;86H[33m136 [0m[32mvoid[0m api_fftC2C(cufftComplex* in, cufftComplex* out, [32mconst[0m [32mint[0m size, [32mint[0m directi[43;86H[33m    [0mon, cudaStream_t stream) {[44;86H[33m137 [0m    cufftHandle plan;[45;86H[33m138 [0m    cufftPlan1d(&plan, size, CUFFT_C2C, [31m1[0m);[46;86H[33m139 [0m    cufftSetStream(plan, stream);[47;86H[33m140 [0m    cufftExecC2C(plan, in, out, direction);[48;86H[33m141 [0m    cufftDestroy(plan);[49;86H[33m142 [0m}[50;86H[33m143 [51;86H144 [0m[34m// out-of-place[0m[52;86H[33m145 [0m[32mvoid[0m api_fftR2C([32mfloat[0m* in, cufftComplex* out, [32mconst3%[8;1H[34h[?25h[?25l[0m[55;159Hj[8;1H[55;159H [9;1H[8;1H[34m..[0m[36m/[0m                                                                                 
[4m[34m.[24m[0m[4m[36m/[24m[0m[4m                                                                                  [24m[54;67H[1m[7m9[9;1H[34h[?25h[?25l[0m[55;159Hj[9;1H[55;159H [10;1H[9;1H[34m.[0m[36m/[0m                                                                                  
[4m[34mbuild[24m[0m[4m[36m/[24m[0m[4m                                                                              [24m[54;67H[1m[7m10,1[10;1H[34h[?25h[?25l[0m[55;159Hj[10;1H[55;159H [11;1H[10;1H[34mbuild[0m[36m/[0m                                                                              
[4m[34mbuild_debug[24m[0m[4m[36m/[24m[0m[4m                                                                        [24m[54;68H[1m[7m1[11;1H[34h[?25h[?25l[0m[55;159Hj[11;1H[55;159H [12;1H[11;1H[34mbuild_debug[0m[36m/[0m                                                                        
[4mgpuTimer.h                                                                          [24m[54;68H[1m[7m2[12;1H[34h[?25h[?25l[0m[55;159Hj[12;1H[55;159H [13;1H[12;1HgpuTimer.h                                                                          
[4mmatplotlibcpp.h                                                                     [24m[54;68H[1m[7m3[13;1H[34h[?25h[?25l[0m[55;159Hj[13;1H[55;159H [14;1H[13;1Hmatplotlibcpp.h                                                                     
[4m[35mbuild.sh[24m[0m[4m[36m*[24m[0m[4m                                                                           [24m[54;68H[1m[7m4[14;1H[34h[?25h[?25l[0m[55;159Hj[14;1H[55;159H [15;1H[14;1H[35mbuild.sh[0m[36m*[0m                                                                           
[4m[35mbuild_debug.sh[24m[0m[4m[36m*[24m[0m[4m                                                                     [24m[54;68H[1m[7m5[15;1H[34h[?25h[?25l[0m[55;159Hj[15;1H[55;159H [16;1H[15;1H[35mbuild_debug.sh[0m[36m*[0m                                                                     
[4m[35mrun.sh[24m[0m[4m[36m*[24m[0m[4m                                                                             [24m[54;68H[1m[7m6[16;1H[34h[?25h[?25l[0m[55;159Hj[16;1H[55;159H [17;1H[16;1H[35mrun.sh[0m[36m*[0m                                                                             
[4m[35mrun_debug.sh[24m[0m[4m[36m*[24m[0m[4m                                                                       [24m[54;68H[1m[7m7[17;1H[34h[?25h[?25l[0m[55;159Hj[17;1H[55;159H [18;1H[17;1H[35mrun_debug.sh[0m[36m*[0m                                                                       
[4m1                                                                                   [24m[54;68H[1m[7m8[18;1H[34h[?25h[?25l[0m[55;159Hk[18;1H[55;159H [17;1H[4m[35mrun_debug.sh[24m[0m[4m[36m*[24m[0m[4m                                                                       [24m
1                                                                                   [54;68H[1m[7m7[17;1H[34h[?25h[?25l[0m[55;159Hk[17;1H[55;159H [16;1H[4m[35mrun.sh[24m[0m[4m[36m*[24m[0m[4m                                                                             [24m
[35mrun_debug.sh[0m[36m*[0m                                                                       [54;68H[1m[7m6[16;1H[34h[?25h[?25l[0m[55;159Hj[16;1H[55;159H [17;1H[16;1H[35mrun.sh[0m[36m*[0m                                                                             
[4m[35mrun_debug.sh[24m[0m[4m[36m*[24m[0m[4m                                                                       [24m[54;68H[1m[7m7[17;1H[34h[?25h[?25l[0m[55;159Hj[17;1H[55;159H [18;1H[17;1H[35mrun_debug.sh[0m[36m*[0m                                                              [24m[54;68H[1m[7m7[17;1H[34h[?25h[?25l[0m[55;159Hj[17;1H[55;159H [18;1H[17;1H[35mrun_debug.sh[0m[36m*[0m                                                                       
[4m1                                                                                   [24m[54;68H[1m[7m8[18;1H[34h[?25h[?25l[0m[55;159Hk[18;1H[55;159H [17;1H[4m[35mrun_debug.sh[24m[0m[4m[36m*[24m[0m[4m                                                                       [24m
1                                                                                   [54;68H[1m[7m7[17;1H[34h[?25h[?25l[0m[55;159Hj[17;1H[55;159H [18;1H[17;1H[35mrun_debug.sh[0m[36m*[0m                                                                       
[4m1                                                                                   [24m[54;68H[1m[7m8[18;1H[34h[?25h[?25l[0m[55;159Hj[18;1H[55;159H [19;1H[18;1H1                                                                                   
[4mCMakeLists.txt                                                                      [24m[54;68H[1m[7m9[19;1H[34h[?25h[?25l[0m[55;159Hj[19;1H[55;159H [20;1H[19;1HCMakeLists.txt                                                                      
[4mmain.cu                                                                             [24m[54;67H[1m[7m20[20;1H[34h[?25h[?25l[0m[55;159Hj[20;1H[55;159H [20;1H[34h[?25h[?25l[55;2H/home/kyp/Workspace/00.test/Test/cuda_study/08.convolution/05.cufft_2d_conv/main.cu" 586L, 17940B[1;1H[33m  1 [0m[35m#include [0m[31m<cstdio>[0m                                                         
[33m  2 [0m[35m#include [0m[31m<iostream>[0m                                                      
[33m  3 [0m[35m#include [0m[31m<cufft.h>[0m                                                         
[33m  4 [0m[35m#include [0m[31m<cuda_runtime.h>[0m
[33m  5 [0m[35m#include [0m[31m<vector>[0m                                                               
[33m  6 [0m                                                                            
[33m  7 [0m[35m#include [0m[31m"device_launch_parameters.h"[0m                                       
[33m  8 [0m[35m#include [0m[31m"gpuTimer.h"[0m
[33m  9 [0m[35m#include [0m[31m"matplotlibcpp.h"[0m
[33m 10 [0m  
[33m 11 [0m[35m#define cudaCheckError() {                                  \[0m
[33m 12 [0m[35m    [0m[32mcudaError_t[0m[35m e = cudaGetLastError();                     \[0m
[33m 13 [0m[35m    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \[0m
[33m 14 [0m[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m
[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m
[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m
[33m 17 [0m[35m    }                                                       \[0m
[33m 18 [0m[35m}                                                           \[0m
[33m 19 [0m          
[33m 20 [0m[32mconstexpr[0m [32mdouble[0m pi = [31m3.141592655358979323846[0m;                                  
[33m 21 [0m                                                                                
[33m 22 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  
[33m 23 [0m                                                                                
[33m 24 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    
[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              [54;2H[1m[7m/00.test/Test/cuda_study/08.convolution/05.cufft_2d_conv/main.cu 1,1 [11CTop[1;5H[34h[?25h[?25l[0m[55;159H^W[1;5H[34h[?25h[?25l[55;161Hl[1;5H[55;159H   [11;90H[54;1H[7m</00.test/Test/cuda_study/08.convolution/05.cufft_2d_conv/main.cu 1,1            Top [0m[1m[7mmain.cu                                                           107,1          43%[11;90H[34h[?25h[?25l[0m[55;159Hk[11;90H[55;159H [10;90H[54;154H[1m[7m6[10;90H[34h[?25h[?25l[0m[55;159Hk[10;90H[55;159H [8;90H[54;154H[1m[7m5[8;90H[34h[?25h[?25l[0m[55;159Hk[8;90H[55;159H [7;90H[54;154H[1m[7m4[7;90H[34h[?25h[?25l[0m[55;159Hk[7;90H[55;159H [6;90H[54;154H[1m[7m3,0-1[6;90H[34h[?25h[?25l[0m[55;159H1[6;90H[34h[?25h[?25l[55;160HG[6;90H[55;159H  [1;90H[33m 1[0m[1C[35m#include [0m[31m<cstdio>[0m[1;107H[K[2;87H[33m 2[0m[1C[35m#include [0m[31m<iostream>[0m[3;86H[33m  3[0m[1C[35m#include [0m[31m<cufft.h>[0m[3;109H[K[4;86H[33m  4[0m[1C[35m#include [0m[31m<cuda_runtime.h>[0m[5;86H[33m  5[0m[1C[35m#include [0m[31m<vector>[0m[6;86H[33m  6[7;86H  7[0m[1C[35m#include [0m[31m"device_launch_parameters[33m 11[0m[1C[35m#define cudaCheckError() {                                  \[0m[12;86H[33m 12[0m[1C[35m    [0m[32mcudaError_t[0m[35m e = cudaGetLastError();                     \[0m[13;86H[33m 13[0m[1C[35m    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \[0m[14;86H[33m 14[0m[1C[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m[15;86H[33m 15[0m[1C[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m[16;86H[33m 16[0m[1C[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m[17;86H[33m 17[0m[1C[35m    }                                                       \[0m[18;86H[33m 18[0m[1C[35m}                                                           \[0m[18;151H[K[19;86H[33m 19[0m[19;94H[K[20;86H[33m 20[0m[1C[32mnamespace[0m plt = matplotlibcpp;[20;121H[K[21;86H[33m 21[0m[21;94H[K[22;87H[33m22[0m[1C[32mvoid[0m checkDeviceMemory([32mvoid[0m)[23;86H[33m 23[0m[1C{[24;86H[33m 24[0m[5C[32msize_t[0m free, total;[25;86H[33m 25[0m[1C    cudaMemGetInfo(&free, &total);[26;86H[33m 26[0m[1C    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;[27;86H[33m 27[0m[1C    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;[27;150H[K[28;86H[33m 28[0m[5Cstd::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <[29;86H[33m   [0m[1C< [31m"MB[0m[35m\n[0m[31m"[0m << std::endl;[29;112H[K[30;86H[33m 29[0m[6Ctd::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m"MB[0m[35m\n[0m[31m"[0m << std::endl;[31;86H[33m 30[32;86H 31[33;86H 32[0m[1C[32mvoid[0m setGrid([33;103H[K[34;86H[33m 33[0m[1C    std::vector<[32mfloat[0m>& x[34;115H[K[35;87H[33m34[0m[1C    , std::vector<[32mfloat[0m>& k[36;86H[33m 35[0m[5C, [32mfloat[0m dx, [32mfloat[0m dk[36;114H[K[37;86H[33m 36[0m[5C, [32mconst[0m [32mint[0m N[37;108H[K[38;86H[33m 37[0m[5C)[38;95H[K[39;86H[33m 38[0m[1C{[40;86H[33m 39 [0m    [33mfor[0m ([32mauto[0m i = [31m0[0m; i < N; ++i) {[41;86H[33m 40[0m[1C        x[i] = (i - N / [31m2[0m) * dx;[42;86H[33m 41[0m[1C        k[i] = (i - N / [31m2[0m) * dk; [42;124H[K[43;87H[33m42[0m[1C    }[43;95H[K[44;86H[33m 43[0m[1C}[44;94H[K[45;86H[33m 44[0m[45;94H[K[46;86H[33m 45[0m[1C[32mvoid[0m generateGaussian([46;112H[K[47;86H[33m 46[0m[5Cstd::vector<[32mfloat[0m>& data[47;118H[K[48;86H[33m 47[0m[5C, [32mfloat[0m x0 [34m// shift[0m[49;86H[33m 48[0m[1C    , [32mfloat[0m dx[50;86H[33m 49[0m[5C, [32mfloat[0m sigma[51;86H[33m 50[0m[1C    , [32mfloat[0m coeff = [31m1.f[0m[52;86H[33m 51[0m[1C    ) [52;96H[K[53;87H[33m52[0m[1C{[53;91H[K[54;153H[1m[7m,1    [8CTop[1;90H[34h[?25h[?25l[0m[55;159HB[1;90H[55;159H [1;90H[34h[?25h[?25l[55;159Hj[1;90H[55;159H [2;90H[54;152H[1m[7m2[2;90H[34h[?25h[?25l[0m[55;159Hj[2;90H[55;159H [3;90H[54;152H[1m[7m3[3;90H[34h[?25h[?25l[0m[55;159Hj[3;90H[55;159H [4;90H[54;152H[1m[7m4[4;90H[34h[?25h[?25l[0m[55;159Hj[4;90H[55;159H [5;90H[54;152H[1m[7m5[5;90H[34h[?25h[?25l[0m[55;159Hj[5;90H[55;159H [6;90H[54;152H[1m[7m6,0-1[6;90H[34h[?25h[?25l[0m[55;159Hj[6;90H[55;159H [7;90H[54;152H[1m[7m7,1  [7;90H[34h[?25h[?25l[0m[55;159Hj[7;90H[55;159H [8;90H[54;152H[1m[7m8[8;90H[34h[?25h[?25l[0m[55;159Hj[8;90H[55;159H [9;90H[54;152H[1m[7m9[9;90H[34h[?25h[?25l[0m[55;159Hj[9;90H[55;159H [10;90H[54;152H[1m[7m10,0-1[10;90H[34h[?25h[?25l[0m[55;159Hj[10;90H[55;159H [11;90H[54;153H[1m[7m1,1  [11;90H[34h[?25h[?25l[0m[55;159Hj[11;90H[55;159H [12;90H[54;153H[1m[7m2[12;90H[34h[?25h[?25l[0m[55;159Hj[12;90H[55;159H [13;90H[54;153H[1m[7m3[13;90H[34h[?25h[?25l[0m[55;159Hj[13;90H[55;159H [14;90H[54;153H[1m[7m4[14;90H[34h[?25h[?25l[0m[55;159Hj[14;90H[55;159H [15;90H[54;153H[1m[7m5[15;90H[34h[?25h[?25l[0m[55;159Hj[15;90H[55;159H [16;90H[54;153H[1m[7m6[16;90H[34h[?25h[?25l[0m[55;159Hj[16;90H[55;159H [17;90H[54;153H[1m[7m7[17;90H[34h[?25h[?25l[0m[55;159Hw[17;90H[55;159H [17;94H[13;116H[35m[46m{[17;94H}[0m[54;155H[1m[7m5[17;94H[34h[?25h[?25l[0m[55;159Hh[17;94H[55;159H [17;93H[13;116H[35m{[17;94H}[0m[54;155H[1m[7m4[17;93H[34h[?25h[?25l[0m[55;159Hj[17;93H[55;159H [18;93H[54;153H[1m[7m8[18;93H[34h[?25h[?25l[0m[55;159H^Wh[18;93H[55;159H   [1;5H[54;1H[1m[7m</00.test/Test/cuda_study/08.convolution/05.cufft_2d_conv/main.cu 1,1            Top [0m[7mmain.cu                                                           18,4           Top[1;5H[34h[?25h[?25l[0m[55;159Hj[1;5H[55;159H [2;5H[54;67H[1m[7m2[2;5H[34h[?25h[?25l[0m[55;159Hk[2;5H[55;159H [1;5H[54;67H[1m[7m1[1;5H[34h[?25h[?25l[0m[55;159Hj[1;5H[55;159H [2;5H[54;67H[1m[7m2[2;5H[34h[?25h[?25l[0m[55;159Hj[2;5H[55;159H [3;5H[54;67H[1m[7m3[3;5H[34h[?25h[?25l[0m[55;159Hj[3;5H[55;159H [4;5H[54;67H[1m[7m4[4;5H[34h[?25h[?25l[0m[55;159Hj[4;5H[55;159H [5;5H[54;67H[1m[7m5[5;5H[34h[?25h[?25l[0m[55;159Hj[5;5H[55;159H [6;5H[54;67H[1m[7m6,0-1[6;5H[34h[?25h[?25l[0m[55;159Hw[6;5H[55;159H [7;5H[54;67H[1m[7m7,1  [7;5H[34h[?25h[?25l[0m[55;159Hl[7;5H[55;159H [7;6H[54;69H[1m[7m2[7;6H[34h[?25h[?25l[0m[55;159H^W^L[7;6H[55;159H    [18;93H[54;1H[7m</00.test/Test/cuda_study/08.convolution/05.cufft_2d_conv/main.cu 7,2            Top [0m[1m[7mmain.cu                                                           18,4           Top[18;93H[34h[?25h[?25l[0m[55;159Hj[18;93H[55;159H [19;90H[54;153H[1m[7m9,0-1[19;90H[34h[?25h[?25l[0m[55;159Hj[19;90H[55;159H [20;93H[54;152H[1m[7m20,4  [20;93H[34h[?25h[?25l[0m[55;159Hj[20;93H[55;159H [21;90H[54;153H[1m[7m1,0-1[21;90H[34h[?25h[?25l[0m[55;159Hk[21;90H[55;159H [20;93H[54;153H[1m[7m0,4  [20;93H[34h[?25h[?25l[0m[55;159Hj[20;93H[55;159H [21;90H[54;153H[1m[7m1,0-1[21;90H[34h[?25h[?25l[0m[55;159Hj[21;90H[55;159H [22;93H[54;153H[1m[7m2,4  [22;93H[34h[?25h[?25l[0m[55;159Hj[22;93H[55;159H [23;90H[46m{[31;90H}[0m[54;153H[1m[7m3,1[23;90H[34h[?25h[?25l[0m[55;159Hj[23;90H[55;159H [24;93H[23;90H{[31;90H}[54;153H[1m[7m4,4[24;93H[34h[?25h[?25l[0m[55;159Hj[24;93H[55;159H [25;93H[54;153H[1m[7m5[25;93H[34h[?25h[?25l[0m[55;159Hj[25;93H[55;159H [26;93H[54;153H[1m[7m6[26;93H[34h[?25h[?25l[0m[55;159Hj[26;93H[55;159H [27;93H[54;153H[1m[7m7[27;93H[34h[?25h[?25l[0m[55;159Hj[27;93H[55;159H [28;93H[54;153H[1m[7m8[28;93H[34h[?25h[?25l[0m[55;159Hj[28;93H[55;159H [30;93H[54;153H[1m[7m9[30;93H[34h[?25h[?25l[0m[55;159Hj[30;93H[55;159H [31;90H[23;90H[46m{[31;90H}[0m[54;152H[1m[7m30,1[31;90H[34h[?25h[?25l[0m[55;159Hj[31;90H[55;159H [32;90H[23;90H{[31;90H}[54;153H[1m[7m1,0-1[32;90H[34h[?25h[?25l[0m[55;159Hj[32;90H[55;159H [33;93H[54;153H[1m[7m2,4  [33;93H[34h[?25h[?25l[0m[55;159Hj[33;93H[55;159H [34;93H[54;153H[1m[7m3[34;93H[34h[?25h[?25l[0m[55;159Hj[34;93H[55;159H [35;93H[54;153H[1m[7m4[35;93H[34h[?25h[?25l[0m[55;159Hj[35;93H[55;159H [36;93H[54;153H[1m[7m5[36;93H[34h[?25h[?25l[0m[55;159Hj[36;93H[55;159H [37;93H[54;153H[1m[7m6[37;93H[34h[?25h[?25l[0m[55;159Hj[37;93H[55;159H [38;93H[54;153H[1m[7m7[38;93H[34h[?25h[?25l[0m[55;159Hj[38;93H[55;159H [39;90H[46m{[44;90H}[0m[54;153H[1m[7m8,1[39;90H[34h[?25h[?25l[0m[55;159Hj[39;90H[55;159H [40;93H[39;90H{[44;90H}[54;153H[1m[7m9,4[40;93H[34h[?25h[?25l[0m[55;159Hj[40;93H[55;159H [41;93H[54;152H[1m[7m40[41;93H[34h[?25h[?25l[0m[55;159H^W[41;93H[34h[?25;93H[39;90H{[44;90H}[54;153H[1m[7m9,4[40;93H[34h[?25h[?25l[0m[55;159Hj[40;93H[55;159H [41;93H[54;152H[1m[7m40[41;93H[34h[?25h[?25l[0m[55;159H^W[41;93H[34h[?25h[?25l[55;161Hh[41;93H[55;159H   [7;6H[54;1H[1m[7m</00.test/Test/cuda_study/08.convolution/05.cufft_2d_conv/main.cu 7,2            Top [0m[7mmain.cu                                                           40,4           Top[7;6H[34h[?25h[?25l[0m[55;159Hj[7;6H[55;159H [8;6H[54;67H[1m[7m8[8;6H[34h[?25h[?25l[0m[55;159Hj[8;6H[55;159H [9;6H[54;67H[1m[7m9[9;6H[34h[?25h[?25l[0m[55;159Hj[9;6H[55;159H [10;5H[54;67H[1m[7m10,0-1[10;5H[34h[?25h[?25l[0m[55;159Hj[10;5H[55;159H [11;6H[54;68H[1m[7m1,2  [11;6H[34h[?25h[?25l[0m[55;159Hj[11;6H[55;159H [12;6H[54;68H[1m[7m2[12;6H[34h[?25h[?25l[0m[55;159Hj[12;6H[55;159H [13;6H[54;68H[1m[7m3[13;6H[34h[?25h[?25l[0m[55;159Hj[13;6H[55;159H [14;6H[54;68H[1m[7m4[14;6H[34h[?25h[?25l[0m[55;159Hj[14;6H[55;159H [15;6H[54;68H[1m[7m5[15;6H[34h[?25h[?25l[0m[55;159Hj[15;6H[55;159H [16;6H[54;68H[1m[7m6[16;6H[34h[?25h[?25l[0m[55;159Hj[16;6H[55;159H [17;6H[54;68H[1m[7m7[17;6H[34h[?25h[?25l[0m[55;159Hj[17;6H[55;159H [18;6H[54;68H[1m[7m8[18;6H[34h[?25h[?25l[0m[55;159Hj[18;6H[55;159H [19;5H[54;68H[1m[7m9,0-1[19;5H[34h[?25h[?25l[0m[55;159Hj[19;5H[55;159H [20;6H[54;67H[1m[7m20,2  [20;6H[34h[?25h[?25l[0m[55;159Hj[20;6H[55;159H [21;5H[54;68H[1m[7m1,0-1[21;5H[34h[?25h[?25l[0m[55;159Hj[21;5H[55;159H [22;6H[54;68H[1m[7m2,2  [22;6H[34h[?25h[?25l[0m[55;159Hj[22;6H[55;159H [23;5H[54;68H[1m[7m3,0-1[23;5H[34h[?25h[?25l[0m[55;159Hj[23;5H[55;159H [24;6H[54;68H[1m[7m4,2  [24;6H[34h[?25h[?25l[0m[55;159Hj[24;6H[55;159H [25;5H[46m{[33;5H}[0m[54;68H[1m[7m5,1[25;5H[34h[?25h[?25l[0m[55;159Hj[25;5H[55;159H [26;6H[25;5H{[33;5H}[54;68H[1m[7m6,2[26;6H[34h[?25h[?25l[0m[55;159Hj[26;6H[55;159H [27;6H[54;68H[1m[7m7[27;6H[34h[?25h[?25l[0m[55;159Hj[27;6H[55;159H [28;6H[54;68H[1m[7m8[28;6H[34h[?25h[?25l[0m[55;159Hj[28;6H[55;159H [29;6H[54;68H[1m[7m9[29;6H[34h[?25h[?25l[0m[55;159Hj[29;6H[55;159H [30;6H[54;67H[1m[7m30[30;6H[34h[?25h[?25l[0m[55;159Hj[30;6H[55;159H [32;6H[54;68H[1m[7m1[32;6H[34h[?25h[?25l[0m[55;159Hj[32;6H[55;159H [33;5H[25;5H[46m{[33;5H}[0m[54;68H[1m[7m2,1[33;5H[34h[?25h[?25l[0m[55;159Hj[33;5H[55;159H [34;5H[25;5H{[33;5H}[54;68H[1m[7m3,0-1[34;5H[34h[?25h[?25l[0m[55;159Hj[34;5H[55;159H [35;6H[54;68H[1m[7m4,2  [35;6H[34h[?25h[?25l[0m[55;159Hj[35;6H[55;159H [36;6H[54;68H[1m[7m5[36;6H[34h[?25h[?25l[0m[55;159Hj[36;6H[55;159H [37;6H[54;68H[1m[7m6[37;6H[34h[?25h[?25l[0m[55;159Hj[37;6H[55;159H [38;6H[54;68H[1m[7m7[38;6H[34h[?25h[?25l[0m[55;159Hj[38;6H[55;159H [39;6H[54;68H[1m[7m8[39;6H[34h[?25h[?25l[0m[55;159Hj[39;6H[55;159H [40;6H[54;68H[1m[7m9[40;6H[34h[?25h[?25l[0m[55;159Hj[40;6H[55;159H [41;6H[54;67H[1m[7m40[41;6H[34h[?25h[?25l[0m[55;159Hj[41;6H[55;159H [42;6H[54;68H[1m[7m1[42;6H[34h[?25h[?25l[0m[55;159Hj[42;6H[55;159H [44;6H[54;68H[1m[7m2[44;6H[34h[?25h[?25l[0m[55;159Hj[44;6H[55;159H [45;6H[54;68H[1m[7m3[45;6H[34h[?25h[?25l[0m[55;159Hj[45;6H[55;159H [46;6H[54;68H[1m[7m4[46;6H[34h[?25h[?25l[0m[55;159Hj[46;6H[55;159H [47;6H[54;68H[1m[7m5[47;6H[34h[?25h[?25l[0m[55;159Hj[47;6H[55;159H [48;6H[54;68H[1m[7m6[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m  2 [0m[35m#include [0m[31m<iostream>[0m                                                             
[33m  3 [0m[35m#include [0m[31m<cufft.h>[0m                                                              
[33m  4 [0m[35m#include [0m[31m<cuda_runtime.h>[0m                                                       
[33m  5 [0m[35m#include [0m[31m<vector>[0m                                                               
[33m  6 [0m                                                                                
[33m  7 [0m[35m#include [0m[31m"device_launch_parameters.h"[0m                                           
[33m  8 [0m[35m#include [0m[31m"gpuTimer.h"[0m                                                           
[33m  9 [0m[35m#include [0m[31m"matplotlibcpp.h"[0m                                                      
[33m 10 [0m                                                                                
[33m 11 [0m[35m#define cudaCheckError() {                                  \[0m                   
[33m 12 [0m[35m    [0m[32mcudaError_t[0m[35m e = cudaGetLastError();                     \[0m                   
[33m 13 [0m[35m    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \[0m                   
[33m 14 [0m[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m                   
[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m                   
[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   
[33m 17 [0m[35m    }                                                       \[0m                   
[33m 18 [0m[35m}                                                           \[0m                   
[33m 19 [0m                                                                                
[33m 20 [0m[32mconstexpr[0m [32mdouble[0m pi = [31m3.141592655358979323846[0m;                                  
[33m 21 [0m                                                                                
[33m 22 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  
[33m 23 [0m                                                                                
[33m 24 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    
[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
                                                                                    [33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {[55;1H[K[54;68H[1m[7m7[13C 0%[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m  3 [0m[35m#include [0m[31m<cufft.h>[0m                                                              
[33m  4 [0m[35m#include [0m[31m<cuda_runtime.h>[0m                                                       
[33m  5 [0m[35m#include [0m[31m<vector>[0m                                                               
[33m  6 [0m                                                                                
[33m  7 [0m[35m#include [0m[31m"device_launch_parameters.h"[0m                                           
[33m  8 [0m[35m#include [0m[31m"gpuTimer.h"[0m                                                           
[33m  9 [0m[35m#include [0m[31m"matplotlibcpp.h"[0m                                                      
[33m 10 [0m                                                                                
[33m 11 [0m[35m#define cudaCheckError() {                                  \[0m                   
[33m 12 [0m[35m    [0m[32mcudaError_t[0m[35m e = cudaGetLastError();                     \[0m                   
[33m 13 [0m[35m    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \[0m                   
[33m 14 [0m[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m                   
[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m                   
[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   
[33m 17 [0m[35m    }                                                       \[0m                   
[33m 18 [0m[35m}                                                           \[0m                   
[33m 19 [0m                                                                                
[33m 20 [0m[32mconstexpr[0m [32mdouble[0m pi = [31m3.141592655358979323846[0m;                                  
[33m 21 [0m                                                                                
[33m 22 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  
[33m 23 [0m                                                                                
[33m 24 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    
[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
                                                                                    [33m 53 [0m[8Cstd::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;[54;68H[1m[7m8[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;5H[1;1H[33m  4 [0m[35m#include [0m[31m<cuda_runtime.h>[0m                                                       
[33m  5 [0m[35m#include [0m[31m<vector>[0m                                                               
[33m  6 [0m                                                                                
[33m  7 [0m[35m#include [0m[31m"device_launch_parameters.h"[0m                                           
[33m  8 [0m[35m#include [0m[31m"gpuTimer.h"[0m                                                           
[33m  9 [0m[35m#include [0m[31m"matplotlibcpp.h"[0m                                                      
[33m 10 [0m                                                                                
[33m 11 [0m[35m#define cudaCheckError() {                                  \[0m                   
[33m 12 [0m[35m    [0m[32mcudaError_t[0m[35m e = cudaGetLastError();                     \[0m                   
[33m 13 [0m[35m    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \[0m                   
[33m 14 [0m[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m                   
[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m                   
[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   
[33m 17 [0m[35m    }                                                       \[0m                   
[33m 18 [0m[35m}                                                           \[0m                   
[33m 19 [0m                                                                                
[33m 20 [0m[32mconstexpr[0m [32mdouble[0m pi = [31m3.141592655358979323846[0m;                                  
[33m 21 [0m                                                                                
[33m 22 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  
[33m 23 [0m                                                                                
[33m 24 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    
[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
                                                                                    [32;70H[46m{[48;5H}[0m[53;1H[33m 54 [8Cswitch[0m (result) {[54;68H[1m[7m9,1[48;5H[34h[?25h[?25l[0m[55;159Hj[48;5H[55;159H [48;5H[1;1H[33m  5 [0m[35m#include [0m[31m<vector>[0m                                                               
[33m  6 [0m                                                                                
[33m  7 [0m[35m#include [0m[31m"device_launch_parameters.h"[0m                                           
[33m  8 [0m[35m#include [0m[31m"gpuTimer.h"[0m                                                           
[33m  9 [0m[35m#include [0m[31m"matplotlibcpp.h"[0m                                                      
[33m 10 [0m                                                                                
[33m 11 [0m[35m#define cudaCheckError() {                                  \[0m                   
[33m 12 [0m[35m    [0m[32mcudaError_t[0m[35m e = cudaGetLastError();                     \[0m                   
[33m 13 [0m[35m    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \[0m                   
[33m 14 [0m[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m                   
[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m                   
[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   
[33m 17 [0m[35m    }                                                       \[0m                   
[33m 18 [0m[35m}                                                           \[0m                   
[33m 19 [0m                                                                                
[33m 20 [0m[32mconstexpr[0m [32mdouble[0m pi = [31m3.141592655358979323846[0m;                                  
[33m 21 [0m                                                                                
[33m 22 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  
[33m 23 [0m                                                                                
[33m 24 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    
[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) [46m{[0m              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m[46m}[0m                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
                                                                                    [31;70H{[47;5H}[53;1H[33m 55 [12Ccase[0m CUFFT_INVALID_PLAN:[54;67H[1m[7m50,0-1[48;5H[34h[?25h[?25l[0m[55;159Hj[48;5H[55;159H [48;6H[1;1H[33m  6 [0m                                                                                
[33m  7 [0m[35m#include [0m[31m"device_launch_parameters.h"[0m                                           
[33m  8 [0m[35m#include [0m[31m"gpuTimer.h"[0m                                                           
[33m  9 [0m[35m#include [0m[31m"matplotlibcpp.h"[0m                                                      
[33m 10 [0m                                                                                
[33m 11 [0m[35m#define cudaCheckError() {                                  \[0m                   
[33m 12 [0m[35m    [0m[32mcudaError_t[0m[35m e = cudaGetLastError();                     \[0m                   
[33m 13 [0m[35m    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \[0m                   
[33m 14 [0m[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m                   
[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m                   
[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   
[33m 17 [0m[35m    }                                                       \[0m                   
[33m 18 [0m[35m}                                                           \[0m                   
[33m 19 [0m                                                                                
[33m 20 [0m[32mconstexpr[0m [32mdouble[0m pi = [31m3.141592655358979323846[0m;                                  
[33m 21 [0m                                                                                
[33m 22 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  
[33m 23 [0m                                                                                
[33m 24 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    
[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
                                                                                    [33m 56 [0m[16Cstd::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;[54;68H[1m[7m1,2  [48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m  7 [0m[35m#include [0m[31m"device_launch_parameters.h"[0m                                           
[33m  8 [0m[35m#include [0m[31m"gpuTimer.h"[0m                                                           
[33m  9 [0m[35m#include [0m[31m"matplotlibcpp.h"[0m                                                      
[33m 10 [0m                                                                                
[33m 11 [0m[35m#define cudaCheckError() {                                  \[0m                   
[33m 12 [0m[35m    [0m[32mcudaError_t[0m[35m e = cudaGetLastError();                     \[0m                   
[33m 13 [0m[35m    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \[0m                   
[33m 14 [0m[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m                   
[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m                   
[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   
[33m 17 [0m[35m    }                                                       \[0m                   
[33m 18 [0m[35m}                                                           \[0m                   
[33m 19 [0m                                                                                
[33m 20 [0m[32mconstexpr[0m [32mdouble[0m pi = [31m3.141592655358979323846[0m;                                  
[33m 21 [0m                                                                                
[33m 22 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  
[33m 23 [0m                                                                                
[33m 24 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    
[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
                                                                                    [33m 57 [12Ccase[0m CUFFT_ALLOC_FAILED:[54;68H[1m[7m2[14C1[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m  8 [0m[35m#include [0m[31m"gpuTimer.h"[0m                                                           
[33m  9 [0m[35m#include [0m[31m"matplotlibcpp.h"[0m                                                      
[33m 10 [0m                                                                                
[33m 11 [0m[35m#define cudaCheckError() {                                  \[0m                   
[33m 12 [0m[35m    [0m[32mcudaError_t[0m[35m e = cudaGetLastError();                     \[0m                   
[33m 13 [0m[35m    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \[0m                   
[33m 14 [0m[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m                   
[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m                   
[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   
[33m 17 [0m[35m    }                                                       \[0m                   
[33m 18 [0m[35m}                                                           \[0m                   
[33m 19 [0m                                                                                
[33m 20 [0m[32mconstexpr[0m [32mdouble[0m pi = [31m3.141592655358979323846[0m;                                  
[33m 21 [0m                                                                                
[33m 22 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  
[33m 23 [0m                                                                                
[33m 24 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    
[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
                                                                                    [33m 58 [0m[16Cstd::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;[54;68H[1m[7m3[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m  9 [0m[35m#include [0m[31m"matplotlibcpp.h"[0m                                                      
[33m 10 [0m                                                                                
[33m 11 [0m[35m#define cudaCheckError() {                                  \[0m                   
[33m 12 [0m[35m    [0m[32mcudaError_t[0m[35m e = cudaGetLastError();                     \[0m                   
[33m 13 [0m[35m    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \[0m                   
[33m 14 [0m[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m                   
[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m                   
[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   
[33m 17 [0m[35m    }                                                       \[0m                   
[33m 18 [0m[35m}                                                           \[0m                   
[33m 19 [0m                                                                                
[33m 20 [0m[32mconstexpr[0m [32mdouble[0m pi = [31m3.141592655358979323846[0m;                                  
[33m 21 [0m                                                                                
[33m 22 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  
[33m 23 [0m                                                                                
[33m 24 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    
[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
                                                                                    [33m 59 [12Ccase[0m CUFFT_INVALID_TYPE:[54;68H[1m[7m4[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 10 [0m                                                                                
[33m 11 [0m[35m#define cudaCheckError() {                                  \[0m                   
[33m 12 [0m[35m    [0m[32mcudaError_t[0m[35m e = cudaGetLastError();                     \[0m                   
[33m 13 [0m[35m    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \[0m                   
[33m 14 [0m[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m                   
[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m                   
[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   
[33m 17 [0m[35m    }                                                       \[0m                   
[33m 18 [0m[35m}                                                           \[0m                   
[33m 19 [0m                                                                                
[33m 20 [0m[32mconstexpr[0m [32mdouble[0m pi = [31m3.141592655358979323846[0m;                                  
[33m 21 [0m                                                                                
[33m 22 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  
[33m 23 [0m                                                                                
[33m 24 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    
[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
                                                                                    [33m 60 [0m[16Cstd::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;[54;68H[1m[7m5[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 11 [0m[35m#define cudaCheckError() {                                  \[0m                   
[33m 12 [0m[35m    [0m[32mcudaError_t[0m[35m e = cudaGetLastError();                     \[0m                   
[33m 13 [0m[35m    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \[0m                   
[33m 14 [0m[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m                   
[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m                   
[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   
[33m 17 [0m[35m    }                                                       \[0m                   
[33m 18 [0m[35m}                                                           \[0m                   
[33m 19 [0m                                                                                
[33m 20 [0m[32mconstexpr[0m [32mdouble[0m pi = [31m3.141592655358979323846[0m;                                  
[33m 21 [0m                                                                                
[33m 22 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  
[33m 23 [0m                                                                                
[33m 24 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    
[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
                                                                                    [33m 61 [12Ccase[0m CUFFT_INVALID_VALUE:[54;68H[1m[7m6[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 12 [0m[35m    [0m[32mcudaError_t[0m[35m e = cudaGetLastError();                     \[0m                   
[33m 13 [0m[35m    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \[0m                   
[33m 14 [0m[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m                   
[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m                   
[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   
[33m 17 [0m[35m    }                                                       \[0m                   
[33m 18 [0m[35m}                                                           \[0m                   
[33m 19 [0m                                                                                
[33m 20 [0m[32mconstexpr[0m [32mdouble[0m pi = [31m3.141592655358979323846[0m;                                  
[33m 21 [0m                                                                                
[33m 22 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  
[33m 23 [0m                                                                                
[33m 24 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    
[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
                                                                                    [33m 62 [0m[16Cstd::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;[54;68H[1m[7m7[14C2[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 13 [0m[35m    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \[0m                   
[33m 14 [0m[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m                   
[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m                   
[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   
[33m 17 [0m[35m    }                                                       \[0m                   
[33m 18 [0m[35m}                                                           \[0m                   
[33m 19 [0m                                                                                
[33m 20 [0m[32mconstexpr[0m [32mdouble[0m pi = [31m3.141592655358979323846[0m;                                  
[33m 21 [0m                                                                                
[33m 22 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  
[33m 23 [0m                                                                                
[33m 24 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    
[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
                                                                                    [33m 63 [12Ccase[0m CUFFT_INTERNAL_ERROR:[54;68H[1m[7m8[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 14 [0m[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m                   
[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m                   
[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   
[33m 17 [0m[35m    }                                                       \[0m                   
[33m 18 [0m[35m}                                                           \[0m                   
[33m 19 [0m                                                                                
[33m 20 [0m[32mconstexpr[0m [32mdouble[0m pi = [31m3.141592655358979323846[0m;                                  
[33m 21 [0m                                                                                
[33m 22 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  
[33m 23 [0m                                                                                
[33m 24 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    
[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
                                                                                    [33m 64 [0m[16Cstd::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;[54;68H[1m[7m9[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m                   
[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   
[33m 17 [0m[35m    }                                                       \[0m                   
[33m 18 [0m[35m}                                                           \[0m                   
[33m 19 [0m                                                                                
[33m 20 [0m[32mconstexpr[0m [32mdouble[0m pi = [31m3.141592655358979323846[0m;                                  
[33m 21 [0m                                                                                
[33m 22 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  
[33m 23 [0m                                                                                
[33m 24 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    
[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
                                                                                    [33m 65 [12Ccase[0m CUFFT_EXEC_FAILED:[54;67H[1m[7m60[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   
[33m 17 [0m[35m    }                                                       \[0m                   
[33m 18 [0m[35m}                                                           \[0m                   
[33m 19 [0m                                                                                
[33m 20 [0m[32mconstexpr[0m [32mdouble[0m pi = [31m3.141592655358979323846[0m;                                  
[33m 21 [0m                                                                                
[33m 22 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  
[33m 23 [0m                                                                                
[33m 24 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    
[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
                                                                                    [33m 66 [0m[16Cstd::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;[54;68H[1m[7m1[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 17 [0m[35m    }                                                       \[0m                   
[33m 18 [0m[35m}                                                           \[0m                   
[33m 19 [0m                                                                                
[33m 20 [0m[32mconstexpr[0m [32mdouble[0m pi = [31m3.141592655358979323846[0m;                                  
[33m 21 [0m                                                                                
[33m 22 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  
[33m 23 [0m                                                                                
[33m 24 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    
[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << _SETUP_FAILED:[54;68H[1m[7m2[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 18 [0m[35m}                                                           \[0m                   
[33m 19 [0m                                                                                
[33m 20 [0m[32mconstexpr[0m [32mdouble[0m pi = [31m3.141592655358979323846[0m;                                  
[33m 21 [0m                                                                                
[33m 22 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  
[33m 23 [0m                                                                                
[33m 24 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    
[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Errar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
                                                                                    [33m 68 [0m[16Cstd::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m;[54;68H[1m[7m3[14C3[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 19 [0m                                                                                
[33m 20 [0m[32mconstexpr[0m [32mdouble[0m pi = [31m3.141592655358979323846[0m;                                  
[33m 21 [0m                                                                                
[33m 22 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  
[33m 23 [0m                                                                                
[33m 24 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    
[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
                                                                                    [33m 69 [12Ccase[0m CUFFT_INVALID_SIZE:[54;68H[1m[7m4[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 20 [0m[32mconstexpr[0m [32mdouble[0m pi = [31m3.141592655358979323846[0m;                                  
[33m 21 [0m                                                                                
[33m 22 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  
[33m 23 [0m                                                                                
[33m 24 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    
[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
                                                                                    [33m 70 [0m[16Cstd::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;[54;68H[1m[7m5[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 21 [0m                                                                                
[33m 22 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  
[33m 23 [0m                                                                                
[33m 24 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    
[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
                                                                                    [33m 71 [12Ccase[0m CUFFT_UNALIGNED_DATA:[54;68H[1m[7m6[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 22 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  
[33m 23 [0m                                                                                
[33m 24 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    
[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
                                                                                    [33m 72 [0m[16Cstd::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;[54;68H[1m[7m7[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 23 [0m                                                                                
[33m 24 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    
[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
                                                                                    [33m 73 [12Cdefault[0m:[54;68H[1m[7m8[14C4[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [47;6H[1;1H[33m 25 [0m{                                                                               
[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);        exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
                                                                                    
                                                                                    [52;1H[33m 74 [0m[16Cstd::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;[54;68H[1m[7m9[47;6H[34h[?25h[?25l[0m[55;159Hj[47;6H[55;159H [48;6H[54;67H[1m[7m70[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 26 [0m    [32msize_t[0m free, total;                                                         
[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                         std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
                                                                                    [33m 75 [0m[8C}[54;68H[1m[7m1[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 27 [0m    cudaMemGetInfo(&free, &total);                                              
[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
                                                                                    [33m 76 [0m[8Cstd::exit([31mEXIT_FAILURE[0m);[54;68H[1m[7m2[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 28 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      
[33m 29 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    
[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
                                                                                    [33m 77 [0m    }[54;68H[1m[7m3[14C5[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [47;6H[1;1H[33m 30 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <
[33m    [0m< [31m" MB"[0m << std::endl;                                                           
[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
                                                                                    
                                                                                    [52;1H[33m 78 [0m}
[33m 79 [0m[54;68H[1m[7m4[47;6H[34h[?25h[?25l[0m[55;159Hj[47;6H[55;159H [47;6H[1;1H[33m 31 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m" MB"[0m << std::endl;  
[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
                                                                                    
                                                                                    [52;1H[33m 80 [0m[32mvoid[0m generateGaussian(
[33m 81 [0m    std::vector<[32mfloat[0m>& data[54;68H[1m[7m5[47;6H[34h[?25h[?25l[0m[55;159Hj[47;6H[55;159H [48;6H[54;68H[1m[7m6[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 32 [0m}                                                                               
[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
                                                                                    [33m 82 [0m    , [32mint[0m width[54;68H[1m[7m7[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;5H[1;1H[33m 33 [0m                                                                                
[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
[33m 82 [0m    , [32mint[0m width                                                                 
                                                                                    [20;70H[46m{[48;5H}[0m[53;1H[33m 83 [0m    , [32mint[0m height[54;68H[1m[7m8,1[48;5H[34h[?25h[?25l[0m[55;159Hj[48;5H[55;159H [48;5H[1;1H[33m 34 [0m[32mint[0m parse_option([32mint[0m argc, [32mchar[0m* argv[], [32mconst[0m std::string& key) {              
[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) [46m{[0m              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m[46m}[0m                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
[33m 82 [0m    , [32mint[0m width                                                                 
[33m 83 [0m    , [32mint[0m height                                                                
                                                                                    [19;70H{[47;5H}[53;1H[33m 84 [0m    , [32mfloat[0m x0 [34m// shift[0m[54;68H[1m[7m9,0-1[10C6[48;5H[34h[?25h[?25l[0m[55;159Hj[48;5H[55;159H [48;6H[1;1H[33m 35 [0m    [33mfor[0m ([32mint[0m i = [31m1[0m; i < argc - [31m1[0m; ++i) {                                        
[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
[33m 82 [0m    , [32mint[0m width                                                                 
[33m 83 [0m    , [32mint[0m height                                                                
[33m 84 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         
                                                                                    [33m 85 [0m    , [32mfloat[0m y0 [34m// shift[0m[54;67H[1m[7m80,2  [48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 36 [0m        [33mif[0m (argv[i] == key) {                                                   
[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
[33m 82 [0m    , [32mint[0m width                                                                 
[33m 83 [0m    , [32mint[0m height                                                                
[33m 84 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         
[33m 85 [0m    , [32mfloat[0m y0 [34m// shift[0m                                                         
                                                                                    [33m 86 [0m    , [32mfloat[0m dx[54;68H[1m[7m1[48;6H[34h[?25h[?25l[0m[55;159H^W[48;6H[34h[?25h[?25l[55;161Hl[48;6H[55;159H   [41;93H[54;1H[7m</00.test/Test/cuda_study/08.convolution/05.cufft_2d_conv/main.cu 81,2            6% [0m[1m[7mmain.cu                                                           40,4           Top[41;93H[34h[?25h[?25l[0m[55;159Hj[41;93H[55;159H [42;93H[54;153H[1m[7m1[42;93H[34h[?25h[?25l[0m[55;159Hj[42;93H[55;159H [43;93H[54;153H[1m[7m2[43;93H[34h[?25h[?25l[0m[55;159Hj[43;93H[55;159H [44;90H[39;90H[46m{[44;90H}[0m[54;153H[1m[7m3,1[44;90H[34h[?25h[?25l[0m[55;159Hj[44;90H[55;159H [45;90H[39;90H{[44;90H}[54;153H[1m[7m4,0-1[45;90H[34h[?25h[?25l[0m[55;159Hj[45;90H[55;159H [46;93H[54;153H[1m[7m5,4  [46;93H[34h[?25h[?25l[0m[55;159Hj[46;93H[55;159H [47;93H[54;153H[1m[7m6[47;93H[34h[?25h[?25l[0m[55;159Hj[47;93H[55;159H [48;93H[54;153H[1m[7m7[48;93H[34h[?25h[?25l[0m[55;159Hj[48;93H[55;159H [48;93H[1;86H[33m  2 [0m[35m#include [0m[31m<iostream>[0m                                                             [2;86H[33m  3 [0m[35m#include [0m[31m<cufft.h>[0m                                                              [3;86H[33m  4 [0m[35m#include [0m[31m<cuda_runtime.h>[0m                                                       [4;86H[33m  5 [0m[35m#include [0m[31m<vector>[0m                                                               [5;86H[33m  6 [0m                                                                                [6;86H[33m  7 [0m[35m#include [0m[31m"device_launch_parameters.h"[0m                                           [7;86H[33m  8 [0m[35m#include [0m[31m"gpuTimer.h"[0m                                                           [8;86H[33m  9 [0m[35m#include [0m[31m"matplotlibcpp.h"[0m                                                      [9;86H[33m 10 [0m                                                                                [10;86H[33m 11 [0m[35m#define cudaCheckError() {                                  \[0m                   [11;86H[33m 12 [0m[35m    [0m[32mcudaError_t[0m[35m e = cudaGetLastError();                     \[0m                   [12;86H[33m 13 [0m[35m    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \[0m                   [13;86H[33m 14 [0m[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m                   [14;86H[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m                   [15;86H[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   [16;86H[33m 17 [0m[35m    }                                                       \[0m                   [17;86H[33m 18 [0m[35m}                                                           \[0m                   [18;86H[33m 19 [0m                                                                                [19;86H[33m 20 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  [20;86H[33m 21 [0m                                                                                [21;86H[33m 22 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    [22;86H[33m 23 [0m{                                                                               [23;86H[33m 24 [0m    [32msize_t[0m free, total;                                                         [24;86H[33m 25 [0m    cudaMemGetInfo(&free, &total);                                              [25;86H[33m 26 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      [26;86H[33m 27 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    [27;86H[33m 28 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <[28;86H[33m    [0m< [31m"MB[0m[35m\n[0m[31m"[0m << std::endl;                                                          [29;86H[33m 29 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m"MB[0m[35m\n[0m[31m"[0m << std::endl; [30;86H[33m 30 [0m}                                                                               [31;86H[33m 31 [0m                                                                                [32;86H[33m 32 [0m[32mvoid[0m setGrid(                                                                   [33;86H[33m 33 [0m    std::vector<[32mfloat[0m>& x                                                       [34;86H[33m 34 [0m    , std::vector<[32mfloat[0m>& k                                                     [35;86H[33m 35 [0m    , [32mfloat[0m dx, [32mfloat[0m dk                                                        [36;86H[33m 36 [0m    , [32mconst[0m [32mint[0m N                                                               [37;86H[33m 37 [0m    )                                                                           [38;86H[33m 38 [0m{                                                                               [39;86H[33m 39 [0m    [33mfor[0m ([32mauto[0m i = [31m0[0m; i < N; ++i) {                                              [40;86H[33m 40 [0m        x[i] = (i - N / [31m2[0m) * dx;                                                [41;86H[33m 41 [0m        k[i] = (i - N / [31m2[0m) * dk;                                                [42;86H[33m 42 [0m    }                                                                           [43;86H[33m 43 [0m}                                                                               [44;86H[33m 44 [0m                                                                                [45;86H[33m 45 [0m[32mvoid[0m generateGaussian(                                                          [46;86H[33m 46 [0m    std::vector<[32mfloat[0m>& data                                                    [47;86H[33m 47 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         [48;86H[33m 48 [0m    , [32mfloat[0m dx                                                                  [49;86H[33m 49 [0m    , [32mfloat[0m sigma                                                               [50;86H[33m 50 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         [51;86H[33m 51 [0m    )                                                                           [52;86H[33m 52 [0m{                                                                               [53;86H                                                                                    [53;86H[33m 53 [0m    [32mint[0m N = data.size();[54;153H[1m[7m8[13C 0%[48;93H[34h[?25h[?25l[0m[55;159Hj[48;93H[55;159H [48;93H[1;86H[33m  3 [0m[35m#include [0m[31m<cufft.h>[0m                                                              [2;86H[33m  4 [0m[35m#include [0m[31m<cuda_runtime.h>[0m                                                       [3;86H[33m  5 [0m[35m#include [0m[31m<vector>[0m                                                               [4;86H[33m  6 [0m                                                                                [5;86H[33m  7 [0m[35m#include [0m[31m"device_launch_parameters.h"[0m                                           [6;86H[33m  8 [0m[35m#include [0m[31m"gpuTimer.h"[0m                                                           [7;86H[33m  9 [0m[35m#include [0m[31m"matplotlibcpp.h"[0m                                                      [8;86H[33m 10 [0m                                                                                [9;86H[33m 11 [0m[35m#define cudaCheckError() {                                  \[0m                   [10;86H[33m 12 [0m[35m    [0m[32mcudaError_t[0m[35m e = cudaGetLastError();                     \[0m                   [11;86H[33m 13 [0m[35m    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \[0m                   [12;86H[33m 14 [0m[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m                   [13;86H[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m                   [14;86H[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   [15;86H[33m 17 [0m[35m    }                                                       \[0m                   [16;86H[33m 18 [0m[35m}                                                           \[0m                   [17;86H[33m 19 [0m                                                                                [18;86H[33m 20 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  [19;86H[33m 21 [0m                                                                                [20;86H[33m 22 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    [21;86H[33m 23 [0m{                                                                               [22;86H[33m 24 [0m    [32msize_t[0m free, total;                                                         [23;86H[33m 25 [0m    cudaMemGetInfo(&free, &total);                                              [24;86H[33m 26 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      [25;86H[33m 27 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    [26;86H[33m 28 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <[27;86H[33m    [0m< [31m"MB[0m[35m\n[0m[31m"[0m << std::endl;                                                          [28;86H[33m 29 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m"MB[0m[35m\n[0m[31m"[0m << std::endl; [29;86H[33m 30 [0m}                                                                               [30;86H[33m 31 [0m                                                                                [31;86H[33m 32 [0m[32mvoid[0m setGrid(                                                                   [32;86H[33m 33 [0m    std::vector<[32mfloat[0m>& x                                                       [33;86H[33m 34 [0m    , std::vector<[32mfloat[0m>& k                                                     [34;86H[33m 35 [0m    , [32mfloat[0m dx, [32mfloat[0m dk                                                        [35;86H[33m 36 [0m    , [32mconst[0m [32mint[0m N                                                               [36;86H[33m 37 [0m    )                                                                           [37;86H[33m 38 [0m{                                                                               [38;86H[33m 39 [0m    [33mfor[0m ([32mauto[0m i = [31m0[0m; i < N; ++i) {                                              [39;86H[33m 40 [0m        x[i] = (i - N / [31m2[0m) * dx;                                                [40;86H[33m 41 [0m        k[i] = (i - N / [31m2[0m) * dk;                                                [41;86H[33m 42 [0m    }                                                                           [42;86H[33m 43 [0m}                                                                               [43;86H[33m 44 [0m                                                                                [44;86H[33m 45 [0m[32mvoid[0m generateGaussian(                                                          [45;86H[33m 46 [0m    std::vector<[32mfloat[0m>& data                                                    [46;86H[33m 47 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         [47;86H[33m 48 [0m    , [32mfloat[0m dx                                                                  [48;86H[33m 49 [0m    , [32mfloat[0m sigma                                                               [49;86H[33m 50 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         [50;86H[33m 51 [0m    )                                                                           [51;86H[33m 52 [0m{                                                                               [52;86H[33m 53 [0m    [32mint[0m N = data.size();                                                        [53;86H                                                                                    [53;86H[33m 54 [0m    [33mfor[0m ([32mauto[0m i = [31m0[0m; i < N; ++i) {[54;153H[1m[7m9[48;93H[34h[?25h[?25l[0m[55;159Hj[48;93H[55;159H [48;93H[1;86H[33m  4 [0m[35m#include [0m[31m<cuda_runtime.h>[0m                                                       [2;86H[33m  5 [0m[35m#include [0m[31m<vector>[0m                                                               [3;86H[33m  6 [0m                                                                                [4;86H[33m  7 [0m[35m#include [0m[31m"device_launch_parameters.h"[0m                                           [5;86H[33m  8 [0m[35m#include [0m[31m"gpuTimer.h"[0m                                                           [6;86H[33m  9 [0m[35m#include [0m[31m"matplotlibcpp.h"[0m                                                      [7;86H[33m 10 [0m                                                                                [8;86H[33m 11 [0m[35m#define cudaCheckError() {                                  \[0m                   [9;86H[33m 12 [0m[35m    [0m[32mcudaError_t[0m[35m e = cudaGetLastError();                     \[0m                   [10;86H[33m 13 [0m[35m    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \[0m                   [11;86H[33m 14 [0m[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m                   [12;86H[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m                   [13;86H[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   [14;86H[33m 17 [0m[35m    }                                                       \[0m                   [15;86H[33m 18 [0m[35m}                                                           \[0m                   [16;86H[33m 19 [0m                                                                                [17;86H[33m 20 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  [18;86H[33m 21 [0m                                                                                [19;86H[33m 22 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    [20;86H[33m 23 [0m{                                                                               [21;86H[33m 24 [0m    [32msize_t[0m free, total;                                                         [22;86H[33m 25 [0m    cudaMemGetInfo(&free, &total);                                              [23;86H[33m 26 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      [24;86H[33m 27 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    [25;86H[33m 28 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <[26;86H[33m    [0m< [31m"MB[0m[35m\n[0m[31m"[0m << std::endl;                                                          [27;86H[33m 29 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m"MB[0m[35m\n[0m[31m"[0m << std::endl; [28;86H[33m 30 [0m}                                                                               [29;86H[33m 31 [0m                                                                                [30;86H[33m 32 [0m[32mvoid[0m setGrid(                                                                   [31;86H[33m 33 [0m    std::vector<[32mfloat[0m>& x                                                       [32;86H[33m 34 [0m    , std::vector<[32mfloat[0m>& k                                                     [33;86H[33m 35 [0m    , [32mfloat[0m dx, [32mfloat[0m dk                                                        [34;86H[33m 36 [0m    , [32mconst[0m [32mint[0m N                                                               [35;86H[33m 37 [0m    )                                                                           [36;86H[33m 38 [0m{                                                                               [37;86H[33m 39 [0m    [33mfor[0m ([32mauto[0m i = [31m0[0m; i < N; ++i) {                                              [38;86H[33m 40 [0m        x[i] = (i - N / [31m2[0m) * dx;                                                [39;86H[33m 41 [0m        k[i] = (i - N / [31m2[0m) * dk;                                                [40;86H[33m 42 [0m    }                                                                           [41;86H[33m 43 [0m}                                                                               [42;86H[33m 44 [0m                                                                                [43;86H[33m 45 [0m[32mvoid[0m generateGaussian(                                                          [44;86H[33m 46 [0m    std::vector<[32mfloat[0m>& data                                                    [45;86H[33m 47 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         [46;86H[33m 48 [0m    , [32mfloat[0m dx                                                                  [47;86H[33m 49 [0m    , [32mfloat[0m sigma                                                               [48;86H[33m 50 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         [49;86H[33m 51 [0m    )                                                                           [50;86H[33m 52 [0m{                                                                               [51;86H[33m 53 [0m    [32mint[0m N = data.size();                                                        [52;86H[33m 54 [0m    [33mfor[0m ([32mauto[0m i = [31m0[0m; i < N; ++i) {                                              [53;86H                                                                                    [53;86H[33m 55 [0m[8C[32mauto[0m x = (i - N / [31m2[0m) * dx;[54;152H[1m[7m50[14C1[48;93H[34h[?25h[?25l[0m[55;159Hj[48;93H[55;159H [48;93H[1;86H[33m  5 [0m[35m#include [0m[31m<vector>[0m                                                               [2;86H[33m  6 [0m                                                                                [3;86H[33m  7 [0m[35m#include [0m[31m"device_launch_parameters.h"[0m                                           [4;86H[33m  8 [0m[35m#include [0m[31m"gpuTimer.h"[0m                                                           [5;86H[33m  9 [0m[35m#include [0m[31m"matplotlibcpp.h"[0m                                                      [6;86H[33m 10 [0m                                                                                [7;86H[33m 11 [0m[35m#define cudaCheckError() {                                  \[0m                   [8;86H[33m 12 [0m[35m    [0m[32mcudaError_t[0m[35m e = cudaGetLastError();                     \[0m                   [9;86H[33m 13 [0m[35m    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \[0m                   [10;86H[33m 14 [0m[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m                   [11;86H[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m                   [12;86H[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   [13;86H[33m 17 [0m[35m    }                                                       \[0m                   [14;86H[33m 18 [0m[35m}                                                           \[0m                   [15;86H[33m 19 [0m                                                                                [16;86H[33m 20 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  [17;86H[33m 21 [0m                                                                                [18;86H[33m 22 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    [19;86H[33m 23 [0m{                                                                               [20;86H[33m 24 [0m    [32msize_t[0m free, total;                                                         [21;86H[33m 25 [0m    cudaMemGetInfo(&free, &total);                                              [22;86H[33m 26 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      [23;86H[33m 27 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    [24;86H[33m 28 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <[25;86H[33m    [0m< [31m"MB[0m[35m\n[0m[31m"[0m << std::endl;                                                          [26;86H[33m 29 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m"MB[0m[35m\n[0m[31m"[0m << std::endl; [27;86H[33m 30 [0m}                                                                               [28;86H[33m 31 [0m                                                                                [29;86H[33m 32 [0m[32mvoid[0m setGrid(                                                                   [30;86H[33m 33 [0m    std::vector<[32mfloat[0m>& x                                                       [31;86H[33m 34 [0m    , std::vector<[32mfloat[0m>& k                                                     [32;86H[33m 35 [0m    , [32mfloat[0m dx, [32mfloat[0m dk                                                        [33;86H[33m 36 [0m    , [32mconst[0m [32mint[0m N                                                               [34;86H[33m 37 [0m    )                                                                           [35;86H[33m 38 [0m{                                                                               [36;86H[33m 39 [0m    [33mfor[0m ([32mauto[0m i = [31m0[0m; i < N; ++i) {                                              [37;86H[33m 40 [0m        x[i] = (i - N / [31m2[0m) * dx;                                                [38;86H[33m 41 [0m        k[i] = (i - N / [31m2[0m) * dk;                                                [39;86H[33m 42 [0m    }                                                                           [40;86H[33m 43 [0m}                                                                               [41;86H[33m 44 [0m                                                                                [42;86H[33m 45 [0m[32mvoid[0m generateGaussian(                                                          [43;86H[33m 46 [0m    std::vector<[32mfloat[0m>& data                                                    [44;86H[33m 47 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         [45;86H[33m 48 [0m    , [32mfloat[0m dx                                                                  [46;86H[33m 49 [0m    , [32mfloat[0m sigma                                                               [47;86H[33m 50 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         [48;86H[33m 51 [0m    )                                                                           [49;86H[33m 52 [0m{                                                                               [50;86H[33m 53 [0m    [32mint[0m N = data.size();                                                        [51;86H[33m 54 [0m    [33mfor[0m ([32mauto[0m i = [31m0[0m; i < N; ++i) {                                              [52;86H[33m 55 [0m        [32mauto[0m x = (i - N / [31m2[0m) * dx;                                              [53;86H                                                                                    [53;86H[33m 56 [0m[8Cdata[i] = coeff * expf(- (x - x0) * (x - x0) / ([31m2.f[0m * sigma * sigma));[54;153H[1m[7m1[48;93H[34h[?25h[?25l[0m[55;159Hj[48;93H[55;159H [48;90H[1;86H[33m  6 [0m                                                                                [2;86H[33m  7 [0m[35m#include [0m[31m"device_launch_parameters.h"[0m                                           [3;86H[33m  8 [0m[35m#include [0m[31m"gpuTimer.h"[0m                                                           [4;86H[33m  9 [0m[35m#include [0m[31m"matplotlibcpp.h"[0m                                                      [5;86H[33m 10 [0m                                                                                [6;86H[33m 11 [0m[35m#define cudaCheckError() {                                  \[0m                   [7;86H[33m 12 [0m[35m    [0m[32mcudaError_t[0m[35m e = cudaGetLastError();                     \[0m                   [8;86H[33m 13 [0m[35m    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \[0m                   [9;86H[33m 14 [0m[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m                   [10;86H[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m                   [11;86H[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   [12;86H[33m 17 [0m[35m    }                                                       \[0m                   [13;86H[33m 18 [0m[35m}                                                           \[0m                   [14;86H[33m 19 [0m                                                                                [15;86H[33m 20 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  [16;86H[33m 21 [0m                                                                                [17;86H[33m 22 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    [18;86H[33m 23 [0m{                                                                               [19;86H[33m 24 [0m    [32msize_t[0m free, total;                                                         [20;86H[33m 25 [0m    cudaMemGetInfo(&free, &total);                                              [21;86H[33m 26 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      [22;86H[33m 27 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    [23;86H[33m 28 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <[24;86H[33m    [0m< [31m"MB[0m[35m\n[0m[31m"[0m << std::endl;                                                          [25;86H[33m 29 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m"MB[0m[35m\n[0m[31m"[0m << std::endl; [26;86H[33m 30 [0m}                                                                               [27;86H[33m 31 [0m                                                                                [28;86H[33m 32 [0m[32mvoid[0m setGrid(                                                                   [29;86H[33m 33 [0m    std::vector<[32mfloat[0m>& x                                                       [30;86H[33m 34 [0m    , std::vector<[32mfloat[0m>& k                                                     [31;86H[33m 35 [0m    , [32mfloat[0m dx, [32mfloat[0m dk                                                        [32;86H[33m 36 [0m    , [32mconst[0m [32mint[0m N                                                               [33;86H[33m 37 [0m    )                                                                           [34;86H[33m 38 [0m{                                                                               [35;86H[33m 39 [0m    [33mfor[0m ([32mauto[0m i = [31m0[0m; i < N; ++i) {                                              [36;86H[33m 40 [0m        x[i] = (i - N / [31m2[0m) * dx;                                                [37;86H[33m 41 [0m        k[i] = (i - N / [31m2[0m) * dk;                                                [38;86H[33m 42 [0m    }                                                                           [39;86H[33m 43 [0m}                                                                               [40;86H[33m 44 [0m                                                                                [41;86H[33m 45 [0m[32mvoid[0m generateGaussian(                                                          [42;86H[33m 46 [0m    std::vector<[32mfloat[0m>& data                                                    [43;86H[33m 47 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         [44;86H[33m 48 [0m    , [32mfloat[0m dx                                                                  [45;86H[33m 49 [0m    , [32mfloat[0m sigma                                                               [46;86H[33m 50 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         [47;86H[33m 51 [0m    )                                                                           [48;86H[33m 52 [0m{                                                                               [49;86H[33m 53 [0m    [32mint[0m N = data.size();                                                        [50;86H[33m 54 [0m    [33mfor[0m ([32mauto[0m i = [31m0[0m; i < N; ++i) {                                              [51;86H[33m 55 [0m        [32mauto[0m x = (i - N / [31m2[0m) * dx;                                              [52;86H[33m 56 [0m        data[i] = coeff * expf(- (x - x0) * (x - x0) / ([31m2.f[0m * sigma * sigma));  [53;86H                                                                                    [53;86H[33m 57 [0m    }[54;153H[1m[7m2,1[12C2[48;90H[34h[?25h[?25l[0m[55;159Hj[48;90H[55;159H [48;93H[1;86H[33m  7 [0m[35m#include [0m[31m"device_launch_parameters.h"[0m                                           [2;86H[33m  8 [0m[35m#include [0m[31m"gpuTimer.h"[0m                                                           [3;86H[33m  9 [0m[35m#include [0m[31m"matplotlibcpp.h"[0m                                                      [4;86H[33m 10 [0m                                                                                [5;86H[33m 11 [0m[35m#define cudaCheckError() {                                  \[0m                   [6;86H[33m 12 [0m[35m    [0m[32mcudaError_t[0m[35m e = cudaGetLastError();                     \[0m                   [7;86H[33m 13 [0m[35m    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \[0m                   [8;86H[33m 14 [0m[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m                   [9;86H[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m                   [10;86H[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   [11;86H[33m 17 [0m[35m    }                                                       \[0m                   [12;86H[33m 18 [0m[35m}                                                           \[0m                   [13;86H[33m 19 [0m                                                                                [14;86H[33m 20 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  [15;86H[33m 21 [0m                                                                                [16;86H[33m 22 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    [17;86H[33m 23 [0m{                                                                               [18;86H[33m 24 [0m    [32msize_t[0m free, total;                                                         [19;86H[33m 25 [0m    cudaMemGetInfo(&free, &total);                                              [20;86H[33m 26 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      [21;86H[33m 27 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    [22;86H[33m 28 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <[23;86H[33m    [0m< [31m"MB[0m[35m\n[0m[31m"[0m << std::endl;                                                          [24;86H[33m 29 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m"MB[0m[35m\n[0m[31m"[0m << std::endl; [25;86H[33m 30 [0m}                                                                               [26;86H[33m 31 [0m                                                                                [27;86H[33m 32 [0m[32mvoid[0m setGrid(                                                                   [28;86H[33m 33 [0m    std::vector<[32mfloat[0m>& x                                                       [29;86H[33m 34 [0m    , std::vector<[32mfloat[0m>& k                                                     [30;86H[33m 35 [0m    , [32mfloat[0m dx, [32mfloat[0m dk                                                        [31;86H[33m 36 [0m    , [32mconst[0m [32mint[0m N                                                               [32;86H[33m 37 [0m    )                                                                           [33;86H[33m 38 [0m{                                                                               [34;86H[33m 39 [0m    [33mfor[0m ([32mauto[0m i = [31m0[0m; i < N; ++i) {                                              [35;86H[33m 40 [0m        x[i] = (i - N / [31m2[0m) * dx;                                                [36;86H[33m 41 [0m        k[i] = (i - N / [31m2[0m) * dk;                                                [37;86H[33m 42 [0m    }                                                                           [38;86H[33m 43 [0m}                                                                               [39;86H[33m 44 [0m                                                                                [40;86H[33m 45 [0m[32mvoid[0m generateGaussian(                                                          [41;86H[33m 46 [0m    std::vector<[32mfloat[0m>& data                                                    [42;86H[33m 47 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         [43;86H[33m 48 [0m    , [32mfloat[0m dx                                                                  [44;86H[33m 49 [0m    , [32mfloat[0m sigma                                                               [45;86H[33m 50 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         [46;86H[33m 51 [0m    )                                                                           [47;86H[33m 52 [0m{                                                                               [48;86H[33m 53 [0m    [32mint[0m N = data.size();                                                        [49;86H[33m 54 [0m    [33mfor[0m ([32mauto[0m i = [31m0[0m; i < N; ++i) {                                              [50;86H[33m 55 [0m        [32mauto[0m x = (i - N / [31m2[0m) * dx;                                              [51;86H[33m 56 [0m        data[i] = coeff * expf(- (x - x0) * (x - x0) / ([31m2.f[0m * sigma * sigma));  [52;86H[33m 57 [0m    }                                                                           [53;86H                                                                                    [53;86H[33m 58 [0m}[54;153H[1m[7m3,4[48;93H[34h[?25h[?25l[0m[55;159Hj[48;93H[55;159H [48;93H[1;86H[33m  8 [0m[35m#include [0m[31m"gpuTimer.h"[0m                                                           [2;86H[33m  9 [0m[35m#include [0m[31m"matplotlibcpp.h"[0m                                                      [3;86H[33m 10 [0m                                                                                [4;86H[33m 11 [0m[35m#define cudaCheckError() {                                  \[0m                   [5;86H[33m 12 [0m[35m    [0m[32mcudaError_t[0m[35m e = cudaGetLastError();                     \[0m                   [6;86H[33m 13 [0m[35m    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \[0m                   [7;86H[33m 14 [0m[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m                   [8;86H[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m                   [9;86H[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   [10;86H[33m 17 [0m[35m    }                                                       \[0m                   [11;86H[33m 18 [0m[35m}                                                           \[0m                   [12;86H[33m 19 [0m                                                                                [13;86H[33m 20 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  [14;86H[33m 21 [0m                                                                                [15;86H[33m 22 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    [16;86H[33m 23 [0m{                                                                               [17;86H[33m 24 [0m    [32msize_t[0m free, total;                                                         [18;86H[33m 25 [0m    cudaMemGetInfo(&free, &total);                                              [19;86H[33m 26 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      [20;86H[33m 27 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    [21;86H[33m 28 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <[22;86H[33m    [0m< [31m"MB[0m[35m\n[0m[31m"[0m << std::endl;                                                          [23;86H[33m 29 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m"MB[0m[35m\n[0m[31m"[0m << std::endl; [24;86H[33m 30 [0m}                                                                               [25;86H[33m 31 [0m                                                                                [26;86H[33m 32 [0m[32mvoid[0m setGrid(                                                                   [27;86H[33m 33 [0m    std::vector<[32mfloat[0m>& x                                                       [28;86H[33m 34 [0m    , std::vector<[32mfloat[0m>& k                                                     [29;86H[33m 35 [0m    , [32mfloat[0m dx, [32mfloat[0m dk                                                        [30;86H[33m 36 [0m    , [32mconst[0m [32mint[0m N                                                               [31;86H[33m 37 [0m    )                                                                           [32;86H[33m 38 [0m{                                                                               [33;86H[33m 39 [0m    [33mfor[0m ([32mauto[0m i = [31m0[0m; i < N; ++i) {                                              [34;86H[33m 40 [0m        x[i] = (i - N / [31m2[0m) * dx;                                                [35;86H[33m 41 [0m        k[i] = (i - N / [31m2[0m) * dk;                                                [36;86H[33m 42 [0m    }                                                                           [37;86H[33m 43 [0m}                                                                               [38;86H[33m 44 [0m                                                                                [39;86H[33m 45 [0m[32mvoid[0m generateGaussian(                                                          [40;86H[33m 46 [0m    std::vector<[32mfloat[0m>& data                                                    [41;86H[33m 47 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         [42;86H[33m 48 [0m    , [32mfloat[0m dx                                                                  [43;86H[33m 49 [0m    , [32mfloat[0m sigma                                                               [44;86H[33m 50 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         [45;86H[33m 51 [0m    )                                                                           [46;86H[33m 52 [0m{                                                                               [47;86H[33m 53 [0m    [32mint[0m N = data.size();                                                        [48;86H[33m 54 [0m    [33mfor[0m ([32mauto[0m i = [31m0[0m; i < N; ++i) {                                              [49;86H[33m 55 [0m        [32mauto[0m x = (i - N / [31m2[0m) * dx;                                              [50;86H[33m 56 [0m        data[i] = coeff * expf(- (x - x0) * (x - x0) / ([31m2.f[0m * sigma * sigma));  [51;86H[33m 57 [0m    }                                                                           [52;86H[33m 58 [0m}                                                                               [53;86H                                                                                    [53;86H[33m 59 [0m[54;153H[1m[7m4[14C3[48;93H[34h[?25h[?25l[0m[55;159Hj[48;93H[55;159H [48;93H[1;86H[33m  9 [0m[35m#include [0m[31m"matplotlibcpp.h"[0m                                                      [2;86H[33m 10 [0m                                                                                [3;86H[33m 11 [0m[35m#define cudaCheckError() {                                  \[0m                   [4;86H[33m 12 [0m[35m    [0m[32mcudaError_t[0m[35m e = cudaGetLastError();                     \[0m                   [5;86H[33m 13 [0m[35m    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \[0m                   [6;86H[33m 14 [0m[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m                   [7;86H[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m                   [8;86H[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   [9;86H[33m 17 [0m[35m    }                                                       \[0m                   [10;86H[33m 18 [0m[35m}                                                           \[0m                   [11;86H[33m 19 [0m                                                                                [12;86H[33m 20 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  [13;86H[33m 21 [0m                                                                                [14;86H[33m 22 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    [15;86H[33m 23 [0m{                                                                               [16;86H[33m 24 [0m    [32msize_t[0m free, total;                                                         [17;86H[33m 25 [0m    cudaMemGetInfo(&free, &total);                                              [18;86H[33m 26 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      [19;86H[33m 27 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m;                    [20;86H[33m 28 [0m    std::cout << [31m"Device memory (free/total) = "[0m << free_mb << [31m"/"[0m << total_mb <[21;86H[33m    [0m< [31m"MB[0m[35m\n[0m[31m"[0m << std::endl;                                                          [22;86H[33m 29 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m"MB[0m[35m\n[0m[31m"[0m << std::endl; [23;86H[33m 30 [0m}                                                                               [24;86H[33m 31 [0m                                                                                [25;86H[33m 32 [0m[32mvoid[0m setGrid(                                                                   [26;86H[33m 33 [0m    std::vector<[32mfloat[0m>& x                                                       [27;86H[33m 34 [0m    , std::vector<[32mfloat[0m>& k                                                     [28;86H[33m 35 [0m    , [32mfloat[0m dx, [32mfloat[0m dk                                                        [29;86H[33m 36 [0m    , [32mconst[0m [32mint[0m N                                                               [30;86H[33m 37 [0m    )                                                                           [31;86H[33m 38 [0m{                                                                               [32;86H[33m 39 [0m    [33mfor[0m ([32mauto[0m i = [31m0[0m; i < N; ++i) {                                              [33;86H[33m 40 [0m        x[i] = (i - N / [31m2[0m) * dx;                                                [34;86H[33m 41 [0m        k[i] = (i - N / [31m2[0m) * dk;                                                [35;86H[33m 42 [0m    }                                                                           [36;86H[33m 43 [0m}                                                                               [37;86H[33m 44 [0m                                                                                [38;86H[33m 45 [0m[32mvoid[0m generateGaussian(                                                          [39;86H[33m 46 [0m    std::vector<[32mfloat[0m>& data                                                    [40;86H[33m 47 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         [41;86H[33m 48 [0m    , [32mfloat[0m dx                                                                  [42;86H[33m 49 [0m    , [32mfloat[0m sigma                                                               [43;86H[33m 50 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         [44;86H[33m 51 [0m    )                                                                           [45;86H[33m 52 [0m{                                                                               [46;86H[33m 53 [0m    [32mint[0m N = data.size();                                                        [47;86H[33m 54 [0m    [33mfor[0m ([32mauto[0m i = [31m0[0m; i < N; ++i) {                                              [48;86H[33m 55 [0m        [32mauto[0m x = (i - N / [31m2[0m) * dx;                                              [49;86H[33m 56 [0m        data[i] = coeff * expf(- (x - x0) * (x - x0) / ([31m2.f[0m * sigma * sigma));  [50;86H[33m 57 [0m    }                                                                           [51;86H[33m 58 [0m}                                                                               [52;86H[33m 59 [0m                                                                                [53;86H                                                                                    [53;86H[33m 60 [0m[32mvoid[0m generateAnswer([54;153H[1m[7m5[48;93H[34h[?25h[?25l[0m[55;159Hj[48;93H[55;159H [48;93H[1;86H[33m 10 [0m                                                                                [2;86H[33m 11 [0m[35m#define cudaCheckError() {                                  \[0m                   [3;86H[33m 12 [0m[35m    [0m[32mcudaError_t[0m[35m e = cudaGetLastError();                     \[0m                   [4;86H[33m 13 [0m[35m    [0m[33mif[0m[35m (e != [0m[31mcudaSuccess[0m[35m) {                                 \[0m                   [5;86H[33m 14 [0m[35m        printf([0m[31m"CUDA error [0m[35m%s[0m[31m [0m[35m%d[0m[31m: [0m[35m%s\n[0m[31m"[0m[35m,                    \[0m                   [6;86H[33m 15 [0m[35m            [0m[31m__FILE__[0m[35m, [0m[31m__LINE__[0m[35m, cudaGetErrorString(e));     \[0m                   [7;86H[33m 16 [0m[35m        exit([0m[31mEXIT_FAILURE[0m[35m);                                 \[0m                   [8;86H[33m 17 [0m[35m    }                                                       \[0m                   [9;86H[33m 18 [0m[35m}                                                           \[0m                   [10;86H[33m 19 [0m                                                                                [11;86H[33m 20 [0m[32mnamespace[0m plt = matplotlibcpp;                                                  [12;86H[33m 21 [0m                                                                                [13;86H[33m 22 [0m[32mvoid[0m checkDeviceMemory([32mvoid[0m)                                                    [14;86H[33m 23 [0m{                                                                               [15;86H[33m 24 [0m    [32msize_t[0m free, total;                                                         [16;86H[33m 25 [0m    cudaMemGetInfo(&free, &total);                                              [17;86H[33m 26 [0m    [32mauto[0m free_mb = [33mstatic_cast[0m<[32mfloat[0m>(free)/[31m1024.f[0m/[31m1024.f[0m;                      [18;86H[33m 27 [0m    [32mauto[0m total_mb = [33mstatic_cast[0m<[32mfloat[0m>(total)/[31m1024.f[0m/[31m1024.f[0m; [31m"[0m << std::endl;                                                          [21;86H[33m 29 [0m    std::cout << [31m"Used memory = "[0m << total_mb - free_mb << [31m"MB[0m[35m\n[0m[31m"[0m << std::endl; [22;86H[33m 30 [0m}                                                                               [23;86H[33m 31 [0m                                                                                [24;86H[33m 32 [0m[32mvoid[0m setGrid(                                                                   [25;86H[33m 33 [0m    std::vector<[32mfloat[0m>& x                                                       [26;86H[33m 34 [0m    , std::vector<[32mfloat[0m>& k                                                     [27;86H[33m 35 [0m    , [32mfloat[0m dx, [32mfloat[0m dk                                                        [28;86H[33m 36 [0m    , [32mconst[0m [32mint[0m N                                                               [29;86H[33m 37 [0m    )                                                                           [30;86H[33m 38 [0m{                                                                               [31;86H[33m 39 [0m    [33mfor[0m ([32mauto[0m i = [31m0[0m; i < N; ++i) {                                              [32;86H[33m 40 [0m        x[i] = (i - N / [31m2[0m) * dx;                                                [33;86H[33m 41 [0m        k[i] = (i - N / [31m2[0m) * dk;                                                [34;86H[33m 42 [0m    }                                                                           [35;86H[33m 43 [0m}                                                                               [36;86H[33m 44 [0m                                                                                [37;86H[33m 45 [0m[32mvoid[0m generateGaussian(                                                          [38;86H[33m 46 [0m    std::vector<[32mfloat[0m>& data                                                    [39;86H[33m 47 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         [40;86H[33m 48 [0m    , [32mfloat[0m dx                                                                  [41;86H[33m 49 [0m    , [32mfloat[0m sigma                                                               [42;86H[33m 50 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         [43;86H[33m 51 [0m    )                                                                           [44;86H[33m 52 [0m{                                                                               [45;86H[33m 53 [0m    [32mint[0m N = data.size();                                                        [46;86H[33m 54 [0m    [33mfor[0m ([32mauto[0m i = [31m0[0m; i < N; ++i) {                                              [47;86H[33m 55 [0m        [32mauto[0m x = (i - N / [31m2[0m) * dx;                                              [48;86H[33m 56 [0m        data[i] = coeff * expf(- (x - x0) * (x - x0) / ([31m2.f[0m * sigma * sigma));  [49;86H[33m 57 [0m    }                                                                           [50;86H[33m 58 [0m}                                                                               [51;86H[33m 59 [0m                                                                                [52;86H[33m 60 [0m[32mvoid[0m generateAnswer(                                                            [53;86H                                                                                    [53;86H[33m 61 [0m    std::vector<[32mfloat[0m>& real[54;153H[1m[7m6[14C4[48;93H[34h[?25h[?25l[0m[55;159H^W[48;93H[34h[?25h[?25l[55;161Hh[48;93H[55;159H   [48;6H[54;1H[1m[7m</00.test/Test/cuda_study/08.convolution/05.cufft_2d_conv/main.cu 81,2            6% [0m[7mmain.cu                                                           56,4            4%[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 37 [0m            [33mtry[0m {                                                               
[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m        ned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
[33m 82 [0m    , [32mint[0m width                                                                 
[33m 83 [0m    , [32mint[0m height                                                                
[33m 84 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         
[33m 85 [0m    , [32mfloat[0m y0 [34m// shift[0m                                                         
[33m 86 [0m    , [32mfloat[0m dx                                                                  
                                                                                    [33m 87 [0m    , [32mfloat[0m sigma[54;68H[1m[7m2[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 38 [0m                [33mreturn[0m std::stoi(argv[i + [31m1[0m]);                                  
[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
[33m 82 [0m    , [32mint[0m width                                                                 
[33m 83 [0m    , [32mint[0m height                                                                
[33m 84 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         
[33m 85 [0m    , [32mfloat[0m y0 [34m// shift[0m                                                         
[33m 86 [0m    , [32mfloat[0m dx                                                                  
[33m 87 [0m    , [32mfloat[0m sigma                                                               
                                                                                    [33m 88 [0m    , [32mfloat[0m coeff = [31m1.f[0m[54;68H[1m[7m3[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 39 [0m            }                                                                   
[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
[33m 82 [0m    , [32mint[0m width                                                                 
[33m 83 [0m    , [32mint[0m height                                                                
[33m 84 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         
[33m 85 [0m    , [32mfloat[0m y0 [34m// shift[0m                                                         
[33m 86 [0m    , [32mfloat[0m dx                                                                  
[33m 87 [0m    , [32mfloat[0m sigma                                                               
[33m 88 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         
                                                                                    [33m 89 [0m    )[54;68H[1m[7m4[14C7[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 40 [0m            [33mcatch[0m ([32mconst[0m std::exception& e) {                                   
[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
[33m 82 [0m    , [32mint[0m width                                                                 
[33m 83 [0m    , [32mint[0m height                                                                
[33m 84 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         
[33m 85 [0m    , [32mfloat[0m y0 [34m// shift[0m                                                         
[33m 86 [0m    , [32mfloat[0m dx                                                                  
[33m 87 [0m    , [32mfloat[0m sigma                                                               
[33m 88 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         
[33m 89 [0m    )                                                                           
                                                                                    [33m 90 [0m{[54;68H[1m[7m5[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 41 [0m                std::cerr << [31m"Invalid value for "[0m << key << [31m": "[0m << argv[i + [31m1[0m] 
[33m    [0m<< std::endl;                                                                   
[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
[33m 82 [0m    , [32mint[0m width                                                                 
[33m 83 [0m    , [32mint[0m height                                                                
[33m 84 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         
[33m 85 [0m    , [32mfloat[0m y0 [34m// shift[0m                                                         
[33m 86 [0m    , [32mfloat[0m dx                                                                  
[33m 87 [0m    , [32mfloat[0m sigma                                                               
[33m 88 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         
[33m 89 [0m    )                                                                           
[33m 90 [0m{                                                                               
                                                                                    [33m 91 [0m    [33mfor[0m ([32mauto[0m j = [31m0[0m; j < height; ++j) {[54;68H[1m[7m6[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [47;6H[1;1H[33m 42 [0m                std::exit([31m1[0m);                                                   
[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
[33m 82 [0m    , [32mint[0m width                                                                 
[33m 83 [0m    , [32mint[0m height                                                                
[33m 84 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         
[33m 85 [0m    , [32mfloat[0m y0 [34m// shift[0m                                                         
[33m 86 [0m    , [32mfloat[0m dx                                                                  
[33m 87 [0m    , [32mfloat[0m sigma                                                               
[33m 88 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         
[33m 89 [0m    )                                                                           
[33m 90 [0m{                                                                               
[33m 91 [0m    [33mfor[0m ([32mauto[0m j = [31m0[0m; j < height; ++j) {                                         
                                                                                    
                                                                                    [52;1H[33m 92 [8Cfor[0m ([32mauto[0m i = [31m0[0m; i < width; ++i) {
[33m 93 [0m[12C[32mauto[0m ind = width * j + i;[54;68H[1m[7m7[47;6H[34h[?25h[?25l[0m[55;159Hj[47;6H[55;159H [48;6H[54;68H[1m[7m8[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 43 [0m            }                                                                   
[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
[33m 82 [0m    , [32mint[0m width                                                                 
[33m 83 [0m    , [32mint[0m height                                                                
[33m 84 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         
[33m 85 [0m    , [32mfloat[0m y0 [34m// shift[0m                                                         
[33m 86 [0m    , [32mfloat[0m dx                                                                  
[33m 87 [0m    , [32mfloat[0m sigma                                                               
[33m 88 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         
[33m 89 [0m    )                                                                           
[33m 90 [0m{                                                                               
[33m 91 [0m    [33mfor[0m ([32mauto[0m j = [31m0[0m; j < height; ++j) {                                         
[33m 92 [0m        [33mfor[0m ([32mauto[0m i = [31m0[0m; i < width; ++i) {                                      
[33m 93 [0m            [32mauto[0m ind = width * j + i;                                           
                                                                                    [33m 94 [0m[12C[32mauto[0m x = (i - width / [31m2[0m) * dx;[54;68H[1m[7m9[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;5H[1;1H[33m 44 [0m        }                                                                       
[33m 45 [0m    }                                                                           
[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
[33m 82 [0m    , [32mint[0m width                                                                 
[33m 83 [0m    , [32mint[0m height                                                                
[33m 84 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         
[33m 85 [0m    , [32mfloat[0m y0 [34m// shift[0m                                                         
[33m 86 [0m    , [32mfloat[0m dx                                                                  
[33m 87 [0m    , [32mfloat[0m sigma                                                               
[33m 88 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         
[33m 89 [0m    )                                                                           
[33m 90 [0m{                                                                               
[33m 91 [0m    [33mfor[0m ([32mauto[0m j = [31m0[0m; j < height; ++j) {                                         
[33m 92 [0m        [33mfor[0m ([32mauto[0m i = [31m0[0m; i < width; ++i) {                                      
[33m 93 [0m            [32mauto[0m ind = width * j + i;                                           
[33m 94 [0m            [32mauto[0m x = (i - width / [31m2[0m) * dx;                                      
                                                                                    [33m 95 [0m[12C[32mauto[0m y = (j - height / [31m2[0m) * dx;[54;67H[1m[7m90,1[12C8[48;5H[34h[?25h[?25l[0m[55;159Hj[48;5H[55;159H [47;6H[1;1H[33m 46 [0m                                                                                
[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
[33m 82 [0m    , [32mint[0m width                                                                 
[33m 83 [0m    , [32mint[0m height                                                                
[33m 84 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         
[33m 85 [0m    , [32mfloat[0m y0 [34m// shift[0m                                                         
[33m 86 [0m    , [32mfloat[0m dx                                                                  
[33m 87 [0m    , [32mfloat[0m sigma                                                               
[33m 88 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         
[33m 89 [0m    )                                                                           
[33m 90 [0m{                                                                               
[33m 91 [0m    [33mfor[0m ([32mauto[0m j = [31m0[0m; j < height; ++j) {                                         
[33m 92 [0m        [33mfor[0m ([32mauto[0m i = [31m0[0m; i < width; ++i) {                                      
[33m 93 [0m            [32mauto[0m ind = width * j + i;                                           
[33m 94 [0m            [32mauto[0m x = (i - width / [31m2[0m) * dx;                                      
[33m 95 [0m            [32mauto[0m y = (j - height / [31m2[0m) * dx;                                     
                                                                                    
                                                                                    [52;1H[33m 96 [0m[12Cdata[ind] = coeff * expf(- (x - x0) * (y - y0) / ([31m2.f[0m * sigma * sigm
[33m    [0ma));[54;68H[1m[7m1,2[47;6H[34h[?25h[?25l[0m[55;159Hj[47;6H[55;159H [48;6H[54;68H[1m[7m2[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 47 [0m    std::cerr << [31m"Invalid value for "[0m << key << std::endl;                      
[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
[33m 82 [0m    , [32mint[0m width                                                                 
[33m 83 [0m    , [32mint[0m height                                                                
[33m 84 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         
[33m 85 [0m    , [32mfloat[0m y0 [34m// shift[0m                                                         
[33m 86 [0m    , [32mfloat[0m dx                                                                  
[33m 87 [0m    , [32mfloat[0m sigma                                                               
[33m 88 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         
[33m 89 [0m    )                                                                           
[33m 90 [0m{                                                                               
[33m 91 [0m    [33mfor[0m ([32mauto[0m j = [31m0[0m; j < height; ++j) {                                         
[33m 92 [0m        [33mfor[0m ([32mauto[0m i = [31m0[0m; i < width; ++i) {                                      
[33m 93 [0m            [32mauto[0m ind = width * j + i;                                           
[33m 94 [0m            [32mauto[0m x = (i - width / [31m2[0m) * dx;                                      
[33m 95 [0m            [32mauto[0m y = (j - height / [31m2[0m) * dx;                                     
[33m 96 [0m            data[ind] = coeff * expf(- (x - x0) * (y - y0) / ([31m2.f[0m * sigma * sigm
[33m    [0ma));                                                                            
                                                                                    [33m 97 [0m[8C}[54;68H[1m[7m3[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}    H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 48 [0m    std::exit([31m1[0m);                                                               
[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
[33m 82 [0m    , [32mint[0m width                                                                 
[33m 83 [0m    , [32mint[0m height                                                                
[33m 84 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         
[33m 85 [0m    , [32mfloat[0m y0 [34m// shift[0m                                                         
[33m 86 [0m    , [32mfloat[0m dx                                                                  
[33m 87 [0m    , [32mfloat[0m sigma                                                               
[33m 88 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         
[33m 89 [0m    )                                                                           
[33m 90 [0m{                                                                               
[33m 91 [0m    [33mfor[0m ([32mauto[0m j = [31m0[0m; j < height; ++j) {                                         
[33m 92 [0m        [33mfor[0m ([32mauto[0m i = [31m0[0m; i < width; ++i) {                                      
[33m 93 [0m            [32mauto[0m ind = width * j + i;                                           
[33m 94 [0m            [32mauto[0m x = (i - width / [31m2[0m) * dx;                                      
[33m 95 [0m            [32mauto[0m y = (j - height / [31m2[0m) * dx;                                     
[33m 96 [0m            data[ind] = coeff * expf(- (x - x0) * (y - y0) / ([31m2.f[0m * sigma * sigm
[33m    [0ma));                                                                            
[33m 97 [0m        }                                                                       
                                                                                    [33m 98 [0m    }[54;68H[1m[7m4[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 49 [0m}                                                                               
[33m 50 [0m                                                                                
[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
[33m 82 [0m    , [32mint[0m width                                                                 
[33m 83 [0m    , [32mint[0m height                                                                
[33m 84 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         
[33m 85 [0m    , [32mfloat[0m y0 [34m// shift[0m                                                         
[33m 86 [0m    , [32mfloat[0m dx                                                                  
[33m 87 [0m    , [32mfloat[0m sigma                                                               
[33m 88 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         
[33m 89 [0m    )                                                                           
[33m 90 [0m{                                                                               
[33m 91 [0m    [33mfor[0m ([32mauto[0m j = [31m0[0m; j < height; ++j) {                                         
[33m 92 [0m        [33mfor[0m ([32mauto[0m i = [31m0[0m; i < width; ++i) {                                      
[33m 93 [0m            [32mauto[0m ind = width * j + i;                                           
[33m 94 [0m            [32mauto[0m x = (i - width / [31m2[0m) * dx;                                      
[33m 95 [0m            [32mauto[0m y = (j - height / [31m2[0m) * dx;                                     
[33m 96 [0m            data[ind] = coeff * expf(- (x - x0) * (y - y0) / ([31m2.f[0m * sigma * sigm
[33m    [0ma));                                                                            
[33m 97 [0m        }                                                                       
[33m 98 [0m    }                                                                           
                                                                                    [33m 99 [0m}[54;68H[1m[7m5[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [47;6H[1;1H[33m 51 [0m[32minline[0m [32mvoid[0m checkCufftError(cufftResult result, [32mconst[0m [32mchar[0m* msg) {              
[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
[33m 82 [0m    , [32mint[0m width                                                                 
[33m 83 [0m    , [32mint[0m height                                                                
[33m 84 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         
[33m 85 [0m    , [32mfloat[0m y0 [34m// shift[0m                                                         
[33m 86 [0m    , [32mfloat[0m dx                                                                  
[33m 87 [0m    , [32mfloat[0m sigma                                                               
[33m 88 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         
[33m 89 [0m    )                                                                           
[33m 90 [0m{                                                                               
[33m 91 [0m    [33mfor[0m ([32mauto[0m j = [31m0[0m; j < height; ++j) {                                         
[33m 92 [0m        [33mfor[0m ([32mauto[0m i = [31m0[0m; i < width; ++i) {                                      
[33m 93 [0m            [32mauto[0m ind = width * j + i;                                           
[33m 94 [0m            [32mauto[0m x = (i - width / [31m2[0m) * dx;                                      
[33m 95 [0m            [32mauto[0m y = (j - height / [31m2[0m) * dx;                                     
[33m 96 [0m            data[ind] = coeff * expf(- (x - x0) * (y - y0) / ([31m2.f[0m * sigma * sigm
[33m    [0ma));                                                                            
[33m 97 [0m        }                                                                       
[33m 98 [0m    }                                                                           
[33m 99 [0m}                                                                               
                                                                                    
                                                                                    [52;1H[33m100 
101 [0m[32m__global__[0m [32mvoid[0m toComplex([32mconst[0m [32mfloat[0m* data, cufftComplex* out, [32mint[0m N) {[54;68H[1m[7m6[14C9[47;6H[34h[?25h[?25l[0m[55;159Hj[47;6H[55;159H [48;6H[1;1H[33m 52 [0m    [33mif[0m (result != CUFFT_SUCCESS) {                                              
[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
[33m 82 [0m    , [32mint[0m width                                                                 
[33m 83 [0m    , [32mint[0m height                                                                
[33m 84 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         
[33m 85 [0m    , [32mfloat[0m y0 [34m// shift[0m                                                         
[33m 86 [0m    , [32mfloat[0m dx                                                                  
[33m 87 [0m    , [32mfloat[0m sigma                                                               
[33m 88 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         
[33m 89 [0m    )                                                                           
[33m 90 [0m{                                                                               
[33m 91 [0m    [33mfor[0m ([32mauto[0m j = [31m0[0m; j < height; ++j) {                                         
[33m 92 [0m        [33mfor[0m ([32mauto[0m i = [31m0[0m; i < width; ++i) {                                      
[33m 93 [0m            [32mauto[0m ind = width * j + i;                                           
[33m 94 [0m            [32mauto[0m x = (i - width / [31m2[0m) * dx;                                      
[33m 95 [0m            [32mauto[0m y = (j - height / [31m2[0m) * dx;                                     
[33m 96 [0m            data[ind] = coeff * expf(- (x - x0) * (y - y0) / ([31m2.f[0m * sigma * sigm
[33m    [0ma));                                                                            
[33m 97 [0m        }                                                                       
[33m 98 [0m    }                                                                           
[33m 99 [0m}                                                                               
[33m100 [0m                                                                                
[33m101 [0m[32m__global__[0m [32mvoid[0m toComplex([32mconst[0m [32mfloat[0m* data, cufftComplex* out, [32mint[0m N) {        
                                                                                    [33m102 [0m    [32mint[0m i = [36mblockIdx[0m.x * [36mblockDim[0m.x + [36mthreadIdx[0m.x;[54;68H[1m[7m7[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 53 [0m        std::cerr << [31m"[cuFFT Error] "[0m << msg << [31m": "[0m;                           
[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
[33m 82 [0m    , [32mint[0m width                                                                 
[33m 83 [0m    , [32mint[0m height                                                                
[33m 84 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         
[33m 85 [0m    , [32mfloat[0m y0 [34m// shift[0m                                                         
[33m 86 [0m    , [32mfloat[0m dx                                                                  
[33m 87 [0m    , [32mfloat[0m sigma                                                               
[33m 88 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         
[33m 89 [0m    )                                                                           
[33m 90 [0m{                                                                               
[33m 91 [0m    [33mfor[0m ([32mauto[0m j = [31m0[0m; j < height; ++j) {                                         
[33m 92 [0m        [33mfor[0m ([32mauto[0m i = [31m0[0m; i < width; ++i) {                                      
[33m 93 [0m            [32mauto[0m ind = width * j + i;                                           
[33m 94 [0m            [32mauto[0m x = (i - width / [31m2[0m) * dx;                                      
[33m 95 [0m            [32mauto[0m y = (j - height / [31m2[0m) * dx;                                     
[33m 96 [0m            data[ind] = coeff * expf(- (x - x0) * (y - y0) / ([31m2.f[0m * sigma * sigm
[33m    [0ma));                                                                            
[33m 97 [0m        }                                                                       
[33m 98 [0m    }                                                                           
[33m 99 [0m}                                                                               
[33m100 [0m                                                                                
[33m101 [0m[32m__global__[0m [32mvoid[0m toComplex([32mconst[0m [32mfloat[0m* data, cufftComplex* out, [32mint[0m N) {        
[33m102 [0m    [32mint[0m i = [36mblockIdx[0m.x * [36mblockDim[0m.x + [36mthreadIdx[0m.x;                              
                                                                                    [33m103 [0m    [33mif[0m (i < N) {[54;68H[1m[7m8[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;5H[1;1H[33m 54 [0m        [33mswitch[0m (result) {                                              < N) {[54;68H[1m[7m8[48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;5H[1;1H[33m 54 [0m        [33mswitch[0m (result) {                                                       
[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
[33m 82 [0m    , [32mint[0m width                                                                 
[33m 83 [0m    , [32mint[0m height                                                                
[33m 84 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         
[33m 85 [0m    , [32mfloat[0m y0 [34m// shift[0m                                                         
[33m 86 [0m    , [32mfloat[0m dx                                                                  
[33m 87 [0m    , [32mfloat[0m sigma                                                               
[33m 88 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         
[33m 89 [0m    )                                                                           
[33m 90 [0m{                                                                               
[33m 91 [0m    [33mfor[0m ([32mauto[0m j = [31m0[0m; j < height; ++j) {                                         
[33m 92 [0m        [33mfor[0m ([32mauto[0m i = [31m0[0m; i < width; ++i) {                                      
[33m 93 [0m            [32mauto[0m ind = width * j + i;                                           
[33m 94 [0m            [32mauto[0m x = (i - width / [31m2[0m) * dx;                                      
[33m 95 [0m            [32mauto[0m y = (j - height / [31m2[0m) * dx;                                     
[33m 96 [0m            data[ind] = coeff * expf(- (x - x0) * (y - y0) / ([31m2.f[0m * sigma * sigm
[33m    [0ma));                                                                            
[33m 97 [0m        }                                                                       
[33m 98 [0m    }                                                                           
[33m 99 [0m}                                                                               
[33m100 [0m                                                                                
[33m101 [0m[32m__global__[0m [32mvoid[0m toComplex([32mconst[0m [32mfloat[0m* data, cufftComplex* out, [32mint[0m N) {        
[33m102 [0m    [32mint[0m i = [36mblockIdx[0m.x * [36mblockDim[0m.x + [36mthreadIdx[0m.x;                              
[33m103 [0m    [33mif[0m (i < N) {                                                                
                                                                                    [38;5H[46m{[48;5H}[0m[53;1H[33m104 [0m[8Cout[i].x = data[i];[54;68H[1m[7m9,1[48;5H[34h[?25h[?25l[0m[55;159Hj[48;5H[55;159H [48;5H[1;1H[33m 55 [0m            [33mcase[0m CUFFT_INVALID_PLAN:                                            
[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
[33m 82 [0m    , [32mint[0m width                                                                 
[33m 83 [0m    , [32mint[0m height                                                                
[33m 84 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         
[33m 85 [0m    , [32mfloat[0m y0 [34m// shift[0m                                                         
[33m 86 [0m    , [32mfloat[0m dx                                                                  
[33m 87 [0m    , [32mfloat[0m sigma                                                               
[33m 88 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         
[33m 89 [0m    )                                                                           
[33m 90 [0m[46m{[0m                                                                               
[33m 91 [0m    [33mfor[0m ([32mauto[0m j = [31m0[0m; j < height; ++j) {                                         
[33m 92 [0m        [33mfor[0m ([32mauto[0m i = [31m0[0m; i < width; ++i) {                                      
[33m 93 [0m            [32mauto[0m ind = width * j + i;                                           
[33m 94 [0m            [32mauto[0m x = (i - width / [31m2[0m) * dx;                                      
[33m 95 [0m            [32mauto[0m y = (j - height / [31m2[0m) * dx;                                     
[33m 96 [0m            data[ind] = coeff * expf(- (x - x0) * (y - y0) / ([31m2.f[0m * sigma * sigm
[33m    [0ma));                                                                            
[33m 97 [0m        }                                                                       
[33m 98 [0m    }                                                                           
[33m 99 [0m[46m}[0m                                                                               
[33m100 [0m                                                                                
[33m101 [0m[32m__global__[0m [32mvoid[0m toComplex([32mconst[0m [32mfloat[0m* data, cufftComplex* out, [32mint[0m N) {        
[33m102 [0m    [32mint[0m i = [36mblockIdx[0m.x * [36mblockDim[0m.x + [36mthreadIdx[0m.x;                              
[33m103 [0m    [33mif[0m (i < N) {                                                                
[33m104 [0m        out[i].x = data[i];                                                     
                                                                                    [37;5H{[47;5H}[53;1H[33m105 [0m[8Cout[i].y = [31m0.f[0m;[54;67H[1m[7m100,0-1[8C10[48;5H[34h[?25h[?25l[0m[55;159Hj[48;5H[55;159H [48;6H[1;1H[33m 56 [0m                std::cerr << [31m"Invalid plan handle."[0m << std::endl; [33mbreak[0m;        
[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
[33m 82 [0m    , [32mint[0m width                                                                 
[33m 83 [0m    , [32mint[0m height                                                                
[33m 84 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         
[33m 85 [0m    , [32mfloat[0m y0 [34m// shift[0m                                                         
[33m 86 [0m    , [32mfloat[0m dx                                                                  
[33m 87 [0m    , [32mfloat[0m sigma                                                               
[33m 88 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         
[33m 89 [0m    )                                                                           
[33m 90 [0m{                                                                               
[33m 91 [0m    [33mfor[0m ([32mauto[0m j = [31m0[0m; j < height; ++j) {                                         
[33m 92 [0m        [33mfor[0m ([32mauto[0m i = [31m0[0m; i < width; ++i) {                                      
[33m 93 [0m            [32mauto[0m ind = width * j + i;                                           
[33m 94 [0m            [32mauto[0m x = (i - width / [31m2[0m) * dx;                                      
[33m 95 [0m            [32mauto[0m y = (j - height / [31m2[0m) * dx;                                     
[33m 96 [0m            data[ind] = coeff * expf(- (x - x0) * (y - y0) / ([31m2.f[0m * sigma * sigm
[33m    [0ma));                                                                            
[33m 97 [0m        }                                                                       
[33m 98 [0m    }                                                                           
[33m 99 [0m}                                                                               
[33m100 [0m                                                                                
[33m101 [0m[32m__global__[0m [32mvoid[0m toComplex([32mconst[0m [32mfloat[0m* data, cufftComplex* out, [32mint[0m N) {        
[33m102 [0m    [32mint[0m i = [36mblockIdx[0m.x * [36mblockDim[0m.x + [36mthreadIdx[0m.x;                              
[33m103 [0m    [33mif[0m (i < N) {                                                                
[33m104 [0m        out[i].x = data[i];                                                     
[33m105 [0m        out[i].y = [31m0.f[0m;                                                         
                                                                                    [33m106 [0m    }[54;69H[1m[7m1,2  [48;6H[34h[?25h[?25l[0m[55;159Hj[48;6H[55;159H [48;6H[1;1H[33m 57 [0m            [33mcase[0m CUFFT_ALLOC_FAILED:                                            
[33m 58 [0m                std::cerr << [31m"Memory allocation failed."[0m << std::endl; [33mbreak[0m;   
[33m 59 [0m            [33mcase[0m CUFFT_INVALID_TYPE:                                            
[33m 60 [0m                std::cerr << [31m"Invalid type."[0m << std::endl; [33mbreak[0m;               
[33m 61 [0m            [33mcase[0m CUFFT_INVALID_VALUE:                                           
[33m 62 [0m                std::cerr << [31m"Invalid value."[0m << std::endl; [33mbreak[0m;              
[33m 63 [0m            [33mcase[0m CUFFT_INTERNAL_ERROR:                                          
[33m 64 [0m                std::cerr << [31m"Internal cuFFT error."[0m << std::endl; [33mbreak[0m;       
[33m 65 [0m            [33mcase[0m CUFFT_EXEC_FAILED:                                             
[33m 66 [0m                std::cerr << [31m"FFT execution failed."[0m << std::endl; [33mbreak[0m;       
[33m 67 [0m            [33mcase[0m CUFFT_SETUP_FAILED:                                            
[33m 68 [0m                std::cerr << [31m"cuFFT library setup failed."[0m << std::endl; [33mbreak[0m; 
[33m 69 [0m            [33mcase[0m CUFFT_INVALID_SIZE:                                            
[33m 70 [0m                std::cerr << [31m"Invalid transform size."[0m << std::endl; [33mbreak[0m;     
[33m 71 [0m            [33mcase[0m CUFFT_UNALIGNED_DATA:                                          
[33m 72 [0m                std::cerr << [31m"Unaligned data error."[0m << std::endl; [33mbreak[0m;       
[33m 73 [0m            [33mdefault[0m:                                                            
[33m 74 [0m                std::cerr << [31m"Unknown error code: "[0m << result << std::endl; [33mbrea
    k[0m;                                                                              
[33m 75 [0m        }                                                                       
[33m 76 [0m        std::exit([31mEXIT_FAILURE[0m);                                                
[33m 77 [0m    }                                                                           
[33m 78 [0m}                                                                               
[33m 79 [0m                                                                                
[33m 80 [0m[32mvoid[0m generateGaussian(                                                          
[33m 81 [0m    std::vector<[32mfloat[0m>& data                                                    
[33m 82 [0m    , [32mint[0m width                                                                 
[33m 83 [0m    , [32mint[0m height                                                                
[33m 84 [0m    , [32mfloat[0m x0 [34m// shift[0m                                                         
[33m 85 [0m    , [32mfloat[0m y0 [34m// shift[0m                                                                                 
[33m 88 [0m    , [32mfloat[0m coeff = [31m1.f[0m                                                         
[33m 89 [0m    )                                                                           
[33m 90 [0m{                                                                               
[33m 91 [0m    [33mfor[0m ([32mauto[0m j = [31m0[0m; j < height; ++j) {                                         
[33m 92 [0m        [33mfor[0m ([32mauto[0m i = [31m0[0m; i < width; ++i) {                                      
[33m 93 [0m            [32mauto[0m ind = width * j + i;                                           
[33m 94 [0m            [32mauto[0m x = (i - width / [31m2[0m) * dx;                                      
[33m 95 [0m            [32mauto[0m y = (j - height / [31m2[0m) * dx;                                     
[33m 96 [0m            data[ind] = coeff * expf(- (x - x0) * (y - y0) / ([31m2.f[0m * sigma * sigm
[33m    [0ma));                                                                            
[33m 97 [0m        }                                                                       
[33m 98 [0m    }                                                                           
[33m 99 [0m}                                                                               
[33m100 [0m                                                                                
[33m101 [0m[32m__global__[0m [32mvoid[0m toComplex([32mconst[0m [32mfloat[0m* data, cufftComplex* out, [32mint[0m N) {        
[33m102 [0m    [32mint[0m i = [36mblockIdx[0m.x * [36mblockDim[0m.x + [36mthreadIdx[0m.x;                              
[33m103 [0m    [33mif[0m (i < N) {                                                                
[33m104 [0m        out[i].x = data[i];                                                     
[33m105 [0m        out[i].y = [31m0.f[0m;                                                         
[33m106 [0m    }                                                                           
                                                                                    [33m107 [0m}[54;69H[1m[7m2[48;6H[34h[?25h[?25l[0m[55;159Hk[48;6H[55;159H [47;6H[54;69H[1m[7m1[47;6H[34h[?25h[?25l[54;69H0,0-1[46;5H[34h[?25h[?25l[0m[55;159Hk[46;5H[55;159H [45;5H[35;5H[46m{[45;5H}[0m[54;67H[1m[7m99,1   [45;5H[34h[?25h[?25l[0m[55;159Hk[45;5H[55;159H [44;6H[35;5H{[45;5H}[54;68H[1m[7m8,2[44;6H[34h[?25h[?25l[0m[55;159Hk[44;6H[55;159H [43;6H[54;68H[1m[7m7[43;6H[34h[?25h[?25l[0m[55;159Hk[43;6H[55;159H [41;6H[54;68H[1m[7m6[41;6H[34h[?25h[?25l[0m[55;159Hk[41;6H[55;159H [40;6H[54;68H[1m[7m5[40;6H[34h[?25h[?25l[0m[55;159Hk[40;6H[55;159H [39;6H[54;68H[1m[7m4[39;6H[34h[?25h[?25l[0m[55;159Hk[39;6H[55;159H [38;6H[54;68H[1m[7m3[38;6H[34h[?25h[?25l[0m[55;159Hk[38;6H[55;159H [37;6H[54;68H[1m[7m2[37;6H[34h[?25h[?25l[0m[55;159Hk[37;6H[55;159H [36;6H[54;68H[1m[7m1[36;6H[34h[?25h[?25l[0m[55;159Hk[36;6H[55;159H [35;5H[46m{[45;5H}[0m[54;68H[1m[7m0,1[35;5H[34h[?25h[?25l[0m[55;159Hk[35;5H[55;159H [34;6H[35;5H{[45;5H}[54;67H[1m[7m89,2[34;6H[34h[?25h[?25l[0m[55;159Hk[34;6H[55;159H [33;6H[54;68H[1m[7m8[33;6H[34h[?25h[?25l[0m[55;159Hk[33;6H[55;159H [32;6H[54;68H[1m[7m7[32;6H[34h[?25h[?25l[0m[55;159Hk[32;6H[55;159H [31;6H[54;68H[1m[7m6[31;6H[34h[?25h[?25l[0m[55;159Hk[31;6H[55;159H [30;6H[54;68H[1m[7m5[30;6H[34h[?25h[?25l[0m[55;159Hk[30;6H[55;159H [29;6H[54;68H[1m[7m4[29;6H[34h[?25h[?25l[0m[55;159Hk[29;6H[55;159H [28;6H[54;68H[1m[7m3[28;6H[34h[?25h[?25l[0m[55;159Hk[28;6H[55;159H [27;6H[54;68H[1m[7m2[27;6H[34h[?25h[?25l[0m[55;159Hk[27;6H[55;159H [26;6H[54;68H[1m[7m1[26;6H[34h[?25h[?25l[0m[55;159Hk[26;6H[55;159H [25;6H[54;68H[1m[7m0[25;6H[34h[?25h1m[7m2[27;6H[34h[?25h[?25l[0m[55;159Hk[27;6H[55;159H [26;6H[54;68H[1m[7m1[26;6H[34h[?25h[?25l[0m[55;159Hk[26;6H[55;159H [25;6H[54;68H[1m[7m0[25;6H[34h[?25h[?25l[0m[55;159HV[25;6H[55;159H1[25;6H[7m[32mv[1Cid[0m[7m generateGaussian( [0m[55;1H[1m-- VISUAL LINE --[0m[55;159H[K[55;159H1[25;6H[34h[?25h[?25l[55;159Hj[25;6H[55;159H2[26;6H[25;6H[7m[32mo[0m[26;5H[7m [0m [7m  std::vector<[0m[7m[32mfloat[0m[7m>& data [0m[54;68H[1m[7m1[26;6H[34h[?25h[?25l[0m[55;159Hj[26;6H[55;159H3[27;6H[26;6H[7m [27;5H [0m [7m  , [0m[7m[32mint[0m[7m width [0m[54;68H[1m[7m2[27;6H[34h[?25h[?25l[0m[55;159Hj[27;6H[55;159H4[28;6H[27;6H[7m [28;5H [0m [7m  , [0m[7m[32mint[0m[7m height [0m[54;68H[1m[7m3[28;6H[34h[?25h[?25l[0m[55;159Hj[28;6H[55;159H5[29;6H[28;6H[7m [29;5H [0m [7m  , [0m[7m[32mfloat[0m[7m x0 [0m[7m[34m// shift[0m[7m [0m[54;68H[1m[7m4[29;6H[34h[?25h[?25l[0m[55;159Hj[29;6H[55;159H6[30;6H[29;6H[7m [30;5H [0m [7m  , [0m[7m[32mfloat[0m[7m y0 [0m[7m[34m// shift[0m[7m [0m[54;68H[1m[7m5[30;6H[34h[?25h[?25l[0m[55;159Hj[30;6H[55;159H7[31;6H[30;6H[7m [31;5H [0m [7m  , [0m[7m[32mfloat[0m[7m dx [0m[54;68H[1m[7m6[31;6H[34h[?25h[?25l[0m[55;159Hj[31;6H[55;159H8[32;6H[31;6H[7m [32;5H [0m [7m  , [0m[7m[32mfloat[0m[7m sigma [0m[54;68H[1m[7m7[32;6H[34h[?25h[?25l[0m[55;159Hj[32;6H[55;159H9[33;6H[32;6H[7m [33;5H [0m [7m  , [0m[7m[32mfloat[0m[7m coeff = [0m[7m[31m1.f[0m[7m [0m[54;68H[1m[7m8[33;6H[34h[?25h[?25l[0m[55;159Hj[33;6H[55;159H10[34;6H[33;6H[7m [34;5H [0m [7m  )  [0m[54;68H[1m[7m9[34;6H[34h[?25h[?25l[0m[55;159Hj [34;6H[55;159H11[35;6H[34;6H[7m [35;5H{[0m[54;67H[1m[7m90[35;6H[34h[?25h[?25l[0m[55;159Hj [35;6H[55;159H12[36;6H[35;6H[7m [36;5H [0m [7m  [0m[7m[33mfor[0m[7m ([0m[7m[32mauto[0m[7m j = [0m[7m[31m0[0m[7m; j < height; ++j) { [0m[54;68H[1m[7m1[36;6H[34h[?25h[?25l[0m[55;159Hj [36;6H[55;159H13[37;6H[36;6H[7m [37;5H [0m [7m      [0m[7m[33mfor[0m[7m ([0m[7m[32mauto[0m[7m i = [0m[7m[31m0[0m[7m; i < width; ++i) { [0m[54;68H[1m[7m2[37;6H[34h[?25h[?25l[0m[55;159Hj [37;6H[55;159H14[38;6H[37;6H[7m [38;5H [0m [7m          [0m[7m[32mauto[0m[7m ind = width * j + i; [0m[54;68H[1m[7m3[38;6H[34h[?25h[?25l[0m[55;159Hj [38;6H[55;159H15[39;6H[38;6H[7m [39;5H [0m [7m          [0m[7m[32mauto[0m[7m x = (i - width / [0m[7m[31m2[0m[7m) * dx; [0m[54;68H[1m[7m4[39;6H[34h[?25h[?25l[0m[55;159Hj [39;6H[55;159H16[40;6H[39;6H[7m [40;5H [0m [7m          [0m[7m[32mauto[0m[7m y = (j - height / [0m[7m[31m2[0m[7m) * dx; [0m[54;68H[1m[7m5[40;6H[34h[?25h[?25l[0m[55;159Hj [40;6H[55;159H17[41;6H[40;6H[7m [41;5H [0m [7m          data[ind] = coeff * expf(- (x - x0) * (y - y0) / ([0m[7m[31m2.f[0m[7m * sigma * sigm[42;5Ha)); [0m[54;68H[1m[7m6[41;6H[34h[?25h[?25l[0m[55;159Hj [41;6H[55;159H18[43;6H[41;6H[7m [43;5H [0m [7m      } [0m[54;68H[1m[7m7[43;6H[34h[?25h[?25l[0m[55;159Hj [43;6H[55;159H19[44;6H[43;6H[7m [44;5H [0m [7m  } [0m[54;68H[1m[7m8[44;6H[34h[?25h[?25l[0m[55;159Hj [44;6H[55;159H20[45;6H[44;6H[7m [45;5H}[0m[54;68H[1m[7m9[45;6H[34h[?25h[?25l[0m[55;159Hy [45;6H[25;5H[32mvoid[0m generateGaussian( [26;5H    std::vector<[32mfloat[0m>& data [27;5H    , [32mint[0m width [28;5H    , [32mint[0m height [29;5H    , [32mfloat[0m x0 [34m// shift[0m [30;5H    , [32mfloat[0m y0 [34m// shift[0m [31;5H    , [32mfloat[0m dx [32;5H    , [32mfloat[0m sigma [33;5H    , [32mfloat[0m coeff = [31m1.f[0m [34;5H    )  [35;5H{ [36;5H    [33mfor[0m ([32mauto[0m j = [31m0[0m; j < height; ++j) { [37;5H        [33mfor[0m ([32mauto[0m i = [31m0[0m; i < width; ++i) { [38;5H            [32mauto[0m ind = width * j + i; [39;5H            [32mauto[0m x = (i - width / [